```{r}
csoriginal = read.csv('D:/github/R/marketing_analytics/Clothing_Store')
```

```{r}
drops = c("HHKEY","ZIP_CODE","REC","PC_CALC20","STORELOY")
cs = csoriginal[,!(colnames(csoriginal) %in% drops)]   # 46 variables
cs$VALPHON = as.integer(cs$VALPHON == "Y")  # transform character into binary 
cs
```

```{r}
summary(cs$AVRG)
hist(cs$AVRG,col="lightblue", main="Average Spending per Visit",xlab="Spending", breaks=20)
```

```{r}
summary(cs$GMP)
hist(cs$GMP,col="lightblue", main="Gross Margin Percentage",xlab="Gross Margin", breaks=20)
```

```{r}
par(mfrow=c(2,1)) 
hist(cs$AVRG,col="lightblue", main="Average Spending ver Visit",xlab="Spending", breaks=20)
hist(cs$GMP,col="lightblue", main="Gross Margin Percentage",xlab="Gross Margin", breaks=20)
```

```{r}
contmar = cs$AVRG*cs$GMP*0.5
summary(contmar)
hist(contmar,col="lightblue", main="Contribution Margin per Visit",xlab="Contribution Margin", breaks=20)
```

```{r}

#
#  split data into training set and test set
#

tratio = 0.7   # portion of training data set

nobs = dim(cs)[1]
set.seed(0,kind=NULL)
tryes = runif(nobs)
tryes = (tryes < tratio)

cstrain = cs[tryes==T,]
cstest = cs[tryes==F,]
```

```{r}
table(cstest$RESP)
```

```{r}
## a function that computes the hit ratio of prediction

pred.hit= function(predval,testdata,noprint=FALSE){
  
  r <- predval # variable1 in rows
  c <- testdata # variable2 in columns
  
  # run cross tabulation
  ctab <- xtabs(~r+c)
  
  # frequency table
  table_f <- ctab
  Total1 <- rowSums(ctab); table_f <- cbind(table_f, Total1)
  Total2 <- colSums(table_f); table_f <- rbind(table_f, Total2)
  
  # percentage table
  table_p <- prop.table(ctab)*100
  Total1 <- rowSums(table_p); table_p <- cbind(table_p, Total1)
  Total2 <- colSums(table_p); table_p <- rbind(table_p, Total2)
  
  # row percentage table
  table_r <- prop.table(ctab, 1)*100; sum <- rowSums(table_r);
  table_r <- cbind(table_r, sum)
  
  # col percentage table
  table_c <- prop.table(ctab, 2)*100; sum <- colSums(table_c);
  table_c <- rbind(table_c, sum);
  
  # print results
  if(!noprint){
    cat("Prediction (row) vs. Data (column) ", "\n")
    cat("* Frequency", "\n"); print(table_f); cat("\n")
    cat("* Percentage", "\n"); print(table_p, digits=3);cat("\n")
    cat("* Row Percentage: Distribution of Data for each value of Prediction", "\n"); print(table_r, digits=3); cat("\n")
    cat("* Column Percentage: Distribution of Prediction for each value of Data", "\n"); print(table_c, digits=3); cat("\n")
  }
  cat("Hit Ratio:", (sum(diag(table_p))-100), "\n")

  precision = ctab[2,2] /(ctab[2,1]+ctab[2,2])
  recall = ctab[2,2] /(ctab[2,2]+ctab[1,2])
  f1measure = 2*precision*recall/(precision+recall)
  cat("F1 measure", f1measure, "\n")

  if(!noprint) return(table_f)
}
```

```{r}

##############################################
#
# linear discriminant analysis
#
##############################################

library(MASS)
ldafit = lda(RESP~.,cstrain)
ldatrainfit = predict(ldafit)
ldapred = predict(ldafit, cstest)
cat("LDA prediction \n")
ldaout=pred.hit(ldapred$class,cstest$RESP)
cutoff=0.5
boxplot(ldapred$posterior[,2]~cstest$RESP, col="lightblue", main="Prediction by Linear Discriminant", xlab="Response Data (Test)", ylab="Probability of Response")
abline(h=cutoff)
```

```{r}
##############################################
#
# linear logistic regression 
#
##############################################

nf = table(cstrain$RESP)[1]  # number of False
lgfit = glm(RESP~., cstrain, family=binomial())
tmpx = sort(lgfit$fitted.values,F)
cutoff = (tmpx[nf]+tmpx[nf+1])/2
# table(cstrain$RESP)
# 컷오프 설정 주의
### 상기 컷오프는 logistic 상에서 0과 1을 구분짓는 %
### 또 다른 방법으로는 test set 상에서 0과 1의 비율로 구분

plot(lgfit$fitted.value~cstrain$RESP, col="lightblue", main="Logistic Regression Results", xlab = 'Response Data(Training)', ylab = 'Plobability of Response')
lgtrainfit = (lgfit$fitted.value>cutoff)*1+1

lgpred = predict(lgfit, cstest, type="response")
boxplot(lgpred~cstest$RESP,col="lightblue", main="Prediction by Logistic Regression", xlab="Response Data (Test)", ylab="Probability of Response")
abline(h=cutoff)
lgpredfit = lgpred>cutoff

cat("Logistic Regresion prediction  \n")
pred.hit(lgpredfit,cstest$RESP)

```

```{r}
##############################################
#
# CART
#
##############################################


library(tree)
cartfit = tree(factor(RESP)~.,cstrain)
plot(cartfit)
text(cartfit,all=T, cex=0.7)
title("CART Model Output")
cartfitval = predict(cartfit)
#plot(cartfitval[,2]~cstrain$RESP)  # need to check
cartpred = predict(cartfit,cstest)
cartpredclass = predict(cartfit,cstest,type="class")

cat("CART prediction  \n")
pred.hit(cartpredclass,cstest$RESP)

cutoff=0.5
boxplot(cartpred[,2]~cstest$RESP, col="lightblue", main="Prediction by CART", xlab="Response Data (Test)", ylab="Probability of RESPONSE")
abline(h=cutoff)
```

```{r}
##############################################
#
# Neural Nets 
#
##############################################

library(nnet)



iniwts = c(-14.189,0.5721987,-2.46693,14.14996,1.069072,8.218367,25.60334,-50.83269,-3.258344,8.945041,-3.098779,
           19.76139,-22.62285,-6.572795,25.81599,-37.81675,13.36847,-16.45772,-31.48462,-4.786692,1.823418,2.197832, 
           2.201936,2.290179,-0.5338952,0.2985827,-6.298235e-05,-0.06599293,6.012408,-1.560587,-0.4468282,0.03615387,
           26.36656,-0.5999836,0.7629284,3.073052,3.338232,-36.68484,-10.15628,-2.186835,12.91674,0.003426986,
           -0.1233861,6.175161,1.048332,-17.70584,-0.5017287,-2.380504)

nnfit1 = nnet(factor(RESP)~.,cstrain, size=1, decay =5e-4,maxit=1000, Wts=iniwts)
nnpred1 = factor(predict(nnfit1,cstest,type="class"))
## weight = 48 = 45(변수) + 1 (상수항) + 1 (레이어) + 1(레이어 상수)

iniwts2=c(nnfit1$wts[1:46],nnfit1$wts[1:46],nnfit1$wts[47:48],nnfit1$wts[48])
nnfit2 = nnet(factor(RESP)~.,cstrain, size=2, decay =5e-4,maxit=1000, Wts=iniwts2)
nnpred2 = factor(predict(nnfit2,cstest,type="class"))
## size = 2 : 레이어 2개
## weight = 95 = (45(변수) + (상수항)) * 2 (레이어 수) + 2 (레이어 수) + 1(레이어 상수)


iniwts3=c(nnfit2$wts[1:92],nnfit2$wts[1:46],nnfit2$wts[93:95],nnfit2$wts[94])
nnfit3 = nnet(factor(RESP)~.,cstrain, size=3, decay =5e-4,maxit=1000, Wts=iniwts3)
nnpred3 = factor(predict(nnfit3,cstest,type="class"))
## weight = 142 = 46*3 + 3 + 1

iniwts4=c(nnfit3$wts[1:138],nnfit3$wts[1:46],nnfit3$wts[139:142],nnfit3$wts[140])
nnfit4 = nnet(factor(RESP)~.,cstrain, size=4, decay =5e-4,maxit=1000, Wts=iniwts4)
nnpred4 = factor(predict(nnfit4,cstest,type="class"))

iniwts5=c(nnfit4$wts[1:184],nnfit4$wts[1:46],nnfit4$wts[185:189],nnfit4$wts[186])
nnfit5 = nnet(factor(RESP)~.,cstrain, size=5, decay =5e-4,maxit=1000, Wts=iniwts5)
nnpred5 = factor(predict(nnfit5,cstest,type="class"))

iniwts6=c(nnfit5$wts[1:230],nnfit5$wts[1:46],nnfit5$wts[231:236],nnfit5$wts[232]/100)
nnfit6 = nnet(factor(RESP)~.,cstrain, size=5, decay =5e-4,maxit=1000, Wts=iniwts5)
nnpred6 = factor(predict(nnfit6,cstest,type="class"))

# find the best model in terms of hit ratio

# compute hit result for the best model again, and to some boxplot for the results

pred.hit(nnpred1, cstest$RESP, noprint = T)
pred.hit(nnpred2, cstest$RESP, noprint = T)
pred.hit(nnpred3, cstest$RESP, noprint = T)
pred.hit(nnpred4, cstest$RESP, noprint = T)
pred.hit(nnpred5, cstest$RESP, noprint = T)
pred.hit(nnpred6, cstest$RESP, noprint = T)

```

```{r}
##############################################
#
# Support Vector Machine ; takes too much time when nobs>10000
#
##############################################

library(e1071)

svmfit = svm(RESP~.,data=cstrain, cost=1, probability=T)
#plot(svmfit,data=cstrain,aclength~daymin)

svmfitval = predict(svmfit)
svmpred = predict(svmfit,cstest,type="raw", probability=T)
svmpredpr = attributes(svmpred)$probabilities

cat("SVM prediction \n")
pred.hit(svmpred,cstest$RESP)
plot(svmpredpr[,2]~cstest$RESP, col="lightblue", main="Prediction  by SVM", xlab="Response Data (Test)", ylab="Probability of RESPONSE")
abline(h=cutoff)
```

