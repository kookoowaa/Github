data1 <- data %>% mutate(month=ifelse(APV_TS_D>20140700, 7, 6),
month=ifelse(APV_TS_D<20140601, 5, month)) %>% select(-APV_TS_D)
#user정보 추출. 60879명
user <- data1 %>% select(CLNN, SEX_CCD, CLN_AGE, AVG_Y_INA) %>%
distinct(CLNN, .keep_all=TRUE)
#나이, 성별 더미
user <- user %>% mutate(age2 =ifelse( (CLN_AGE>=40 & CLN_AGE <60), 1, 0),
age3 =ifelse(CLN_AGE >=60, 1, 0)) %>% select(-CLN_AGE)
user$SEX_CCD <- ifelse(user$SEX_CCD =="F", 1, 0)
library(tidyr)
#5,6월 자료로 설명변수 만듦
input <- data1 %>% filter(month !=7) %>% group_by(CLNN, MCT_RY_NM) %>%
summarise(count=n()) %>% spread(MCT_RY_NM, count) %>% ungroup()
input <- input %>% inner_join(user, by="CLNN")
input[is.na(input)]=0
head(input)
#7월 자료로 종속변수 만듦
label <- data1 %>% filter(month==7) %>% group_by(CLNN, MCT_RY_NM) %>% summarise(label=1)%>% ungroup()
label <- label %>% group_by(CLNN) %>% spread(MCT_RY_NM, label) %>% ungroup()
label[is.na(label)]=0
head(label)
#고객 순서 똑같은지 check
sum(input$CLNN != label$CLNN)
#30% 는 평가자료로 사용하자.
set.seed(1001)
idx.ts = sample(1:nrow(input), round(nrow(input)*0.3))
idx.ts = sort(idx.ts)
train=input[-idx.ts,]; label.tr = label[-idx.ts,]
test=input[idx.ts,]; label.ts = label[idx.ts,]
#user index는 따로 저장
user.tr = train$CLNN; user.ts = test$CLNN
train = train[,-1]; test = test[,-1]
label.tr = label.tr[,-1]; label.ts = label.ts[,-1]
#구매횟수 많거나 적은 품목 추천
item.count=apply(train[,1:30], 2, sum)
item.count=sort(item.count, decreasing = T)
head(item.count)
#---------- 모형 1: 추천횟수 많은 품목 추천------------------------------------------------#
real.item=colSums(label.ts)
real.item #29: 할/슈, 28: 한식, 27: 편의점
real.item[29]/length(user.ts) #할인점/슈퍼마켓 추천
sum(real.item[c(29,28)])/(2*length(user.ts)) #할인점/슈퍼마켓, 한식 추천
sum(real.item[c(29,28,27)])/(3*length(user.ts)) #할인점/슈퍼마켓,한식, 편의점 추천
sum(real.item[c(25,9,21)])/(3*length(user.ts)) #커피전문점, 백화점, 제과점. 강의노트 틀림
L
#---------- 모형 2: 로지스틱 모형 ------------------------------------------------#
p.logis = label.ts #확률 저장할 table
library(glmnet)
for(i in 1:30){
lm=glmnet(x=as.matrix(train), y=as.matrix(label.tr[,i]), family="binomial", alpha=0, lambda = 0.02)
p.logis[,i]=predict(lm, as.matrix(test), type="response")
rm(lm); gc()
}
#user별 첫번째, 두번째, 세번째 확률 높은 아이템 인덱스 추출
index1=apply(p.logis, 1, function(x) sort.int(t(x), index.return=TRUE, decreasing = T)$ix[1])
index2=apply(p.logis, 1, function(x) sort.int(t(x), index.return=TRUE, decreasing = T)$ix[2])
index3=apply(p.logis, 1, function(x) sort.int(t(x), index.return=TRUE, decreasing = T)$ix[3])
#Hit ratio (Precision)
sum(as.matrix(label.ts)[cbind(1:nrow(label.ts),index1)])/length(user.ts)
(sum(as.matrix(label.ts)[cbind(1:nrow(label.ts),index1)]) + sum(as.matrix(label.ts)[cbind(1:nrow(label.ts),index2)]))/
(2*length(user.ts))
(sum(as.matrix(label.ts)[cbind(1:nrow(label.ts),index1)]) + sum(as.matrix(label.ts)[cbind(1:nrow(label.ts),index2)])+
sum(as.matrix(label.ts)[cbind(1:nrow(label.ts),index3)]))/
(3*length(user.ts))
head(p.logis)
index1
#추천 품목수
length(unique(index1))
length(unique(c(index1, index2)))
length(unique(c(index1, index2, index3)))
#추천 품목수
length(unique(index1))
length(unique(c(index1,index2)))
length(unique(c(index1,index2,index3)))
#품목별로 구매가능성 높은 일부 고객에게 추천
#커피전문점, 백화점, 제과점
colnames(p.logis)[25]; colnames(p.logis)[9]; colnames(p.logis)[21]
sum(label.ts[,25]); sum(label.ts[,9]); sum(label.ts[,21])
dim(labels.ts)
dim(label.ts)
#품목별 구매가능성 높은 고객700명에게 추천
(sum(label.ts[sort.int(t(p.logis[,25]), index.return=TRUE, decreasing = T)$ix[1:7000],25]) +
sum(label.ts[sort.int(t(p.logis[,9]), index.return=TRUE, decreasing = T)$ix[1:7000],9]) +
sum(label.ts[sort.int(t(p.logis[,21]), index.return=TRUE, decreasing = T)$ix[1:7000],21])) / (7000*3)
#---------- 모형 3: boosting 모형 ------------------------------------------------#
p.boost = label.ts #확률 저장할 table
library(xgboost)
for(i in 1:30){
X=xgb.DMatrix(as.matrix(train), label=as.matrix(label.tr)[,i])
model <- xgboost(X, max_depth=3, eta=0.1, nrounds = 200, objective="binary:logistic", verbose = F)
p.boost[,i]=predict(model, as.matrix(test), type="response")
rm(model);gc()
}
#user별 첫번째, 두번째, 세번째 확률 높은 아이템 인덱스 추출
ind1=apply(p.boost, 1, function(x) sort.int(t(x), index.return=TRUE, decreasing = T)$ix[1])
ind2=apply(p.boost, 1, function(x) sort.int(t(x), index.return=TRUE, decreasing = T)$ix[2])
ind3=apply(p.boost, 1, function(x) sort.int(t(x), index.return=TRUE, decreasing = T)$ix[3])
sum(as.matrix(label.ts)[cbind(1:nrow(label.ts),ind1)])/length(user.ts)
(sum(as.matrix(label.ts)[cbind(1:nrow(label.ts),ind1)]) + sum(as.matrix(label.ts)[cbind(1:nrow(label.ts),index2)]))/
(2*length(user.ts))
(sum(as.matrix(label.ts)[cbind(1:nrow(label.ts),ind1)]) + sum(as.matrix(label.ts)[cbind(1:nrow(label.ts),index2)])+
sum(as.matrix(label.ts)[cbind(1:nrow(label.ts),ind3)]))/
(3*length(user.ts))
length(unique(ind1))
length(unique(c(ind1, ind2)))
length(unique(c(ind1, ind2, ind3)))
#품목별로 구매가능성 높은 일부 고객에게 추천
#커피전문점, 백화점, 제과점
(sum(label.ts[sort.int(t(p.boost[,25]), index.return=TRUE, decreasing = T)$ix[1:7000],25]) +
sum(label.ts[sort.int(t(p.boost[,9]), index.return=TRUE, decreasing = T)$ix[1:7000],9]) +
sum(label.ts[sort.int(t(p.boost[,21]), index.return=TRUE, decreasing = T)$ix[1:7000],21])) / (7000*3)
#실제 구매 고객수
sum(label.ts[,25], na.rm=T)
sum(label.ts[,9], na.rm=T)
sum(label.ts[,21], na.rm=T)
rm(list=ls())
gc()
data = read.csv('d:/recommendation 실습자료/instacart.csv')
data = read.csv('d:/recommendation 실습자료/instacart.csv')
data
unique(data$user_id)
length(unique(data$user_id))
ind = unique(data$user_id)
sample(ind, 0.7*length(ind))
ind==train_ind
set.seed(1)
train_ind = sample(ind, 0.7*length(ind))
valid_ind = ind==train_ind
which(ind == train_ind)
insta = read.csv('d:/recommendation 실습자료/instacart.csv')
set.seed(1)
idx.ts = sample (1:length(unique(insta$user_id)), length(unique(insta$user_id))*0.3)
idx.ts = sort (idx.ts)
idx.ts
user.tr = as.data.frame(unique(insta$user_id)[-idx.ts])
user.ts = as.data.frame(unique(insta$user_id)[idx.ts])
user.ts
user.tr = as.data.frame(unique(insta$user_id)[-idx.ts])
user.ts = as.data.frame(unique(insta$user_id)[idx.ts])
colnames(user.tr) = 'user_id'
colnames(user.ts) = 'user_id'
user.ts
ind = unique(data$user_id)
ind = unique(insta$user_id)
set.seed(1)
train_ind = sample(ind, 0.7*length(ind))
ind
ind in train_ind
train_ind in ind
rm(c(ind, train_ind))
rm(c('ind', 'train_ind'))
rm(list = c('ind', 'train_ind'))
library(dplyr)
tr.mat = insta %>% inner_join(user.tr, by = 'user_id')
tr.mat
ts.mat = insta %>% inner_join(user.ts, by = 'user_id')
ts.mat
library(xgboost)
colnames(tr.mat)
```{r}
X = xgb.DMatrix(as.matrix(tr.mat[,-c(1,2,37)]), label = tr.mat$reordered)
model = xgboost(data = X, max_depth = 5, eta = 0.1, nrounds = 200, objective = 'binary:logistic')
```
tr.mat$reordered
tr.mat[,-c(1,2,37)]
X
model
model
importance = xgb.importance(colnames(X), model = model)
xgb.ggplot.importance(importance)
insta = read.csv('d:/recommendation 실습자료/instacart.csv')
insta
set.seed(1)
idx.ts = sample (1:length(unique(insta$user_id)), length(unique(insta$user_id))*0.3)
idx.ts = sort (idx.ts)
user.tr = as.data.frame(unique(insta$user_id)[-idx.ts])
user.ts = as.data.frame(unique(insta$user_id)[idx.ts])
colnames(user.tr) = 'user_id'
colnames(user.ts) = 'user_id'
library(dplyr)
tr.mat = insta %>% inner_join(user.tr, by = 'user_id')
ts.mat = insta %>% inner_join(user.ts, by = 'user_id')
library(xgboost)
colnames(tr.mat)
X = xgb.DMatrix(as.matrix(tr.mat[,-c(1,2,37)]), label = tr.mat$reordered)
model = xgboost(data = X, max_depth = 5, eta = 0.1, nrounds = 200, objective = 'binary:logistic')
# Apply
test.mat = xgb.DMatrix(ts.mat[,-c(1,2,37)])
importance = xgb.importance(colnames(X), model = model)
xgb.ggplot.importance(importance)
# Apply
test.mat = xgb.DMatrix(ts.mat[,-c(1,2,37)])
ts.mat[,-c(1,2,37)]
xgb.DMatrix(ts.mat[,-c(1,2,37)])
X = xgb.DMatrix(as.matrix(tr.mat[,-c(1,2,37)]), label = tr.mat$reordered)
model = xgboost(data = X, max_depth = 5, eta = 0.1, nrounds = 200, objective = 'binary:logistic')
model
test.mat = xgb.DMatrix(as.matrix(ts.mat[,-c(1,2,37)]))
ts.mat$fitted = predict(model, test.mat)
as.matrix(ts.mat[,-c(1,2,37)])
rm(list=ls())
gc()
library(recommenderlab)
install.packages("recommenderlab")
library(recommenderlab)
data(Jester5k)
head(getRatingMatrix(Jester5k))
image(Jester5k[1:80,])
head(getRatingMatrix(Jester5k))
image(Jester5k[1:80,])
n.user=1000; n.item=ncol(Jester5k)
a <- as(Jester5k[1:n.user],"matrix") #1000 명의 user만 사용
set.seed(123)
subset=sample.int(sum(!is.na(a)),sum(!is.na(a))*.3) #rating 된 점수중 30%만 추출
subset=sort(subset)
train = a; test =a
train[!is.na(train)][subset] = NA; test[!is.na(test)][-subset] = NA #rating 된 점수들 중 train, test 나눔
subset
train
train_head()
train(head)
head(train)
#User based Collaborative filtering
rtrain = as(train,"realRatingMatrix") #rating matrix 만 함수의 input으로 가능
r=Recommender(rtrain, method="UBCF")  #method를 IBCF로하면 item-based CF
getModel(r)$method
getModel(r)$nn
pr=predict(r, rtrain[1:2,], type="ratings")
as(pr, "matrix") #이미 평가한 자료의 경우 예측값을 주지 않는다
#n을 통해 추천품목수를 조정 가능
ptype=predict(r, rtrain[1:2,], n=5)
as(ptype, "list")
#RMSE
pr=predict(r, rtrain, type="ratings")
pr=as(pr, "matrix") #이미 평가한 자료의 경우 예측값을 주지 않는다
pr[pr>10]=10 #10보다 크면 10, -10보다 작으면 -10으로 예측하겠다.
pr[pr<(-10)]=-10
RMSE(test, pr)
#----- (4-2) 스케일 보정  ----------#
dgmat=cbind(train[1:(n.user*n.item)], as.data.frame(cbind(rep(rownames(train), n.item),
rep(colnames(train), each=n.user))))
head(dgmat)
#각 행은 하나의 평점에 대한 정보를 가짐
colnames(dgmat) <- c("rating","user","item")
user = unique(dgmat$user); item = unique(dgmat$item)
head(dgmat)
dgmat = dgmat[is.na(dgmat$rating)==F,]        #평점이 있는 정보만 사용
#making dummy variable
dummy = model.matrix(rating~user+item, dgmat) #user와 item에 대해 dummy variable 생성, 변수 1개 기준으로 999개
dummy = dummy[,-1] # coefficient term은 제외하자
library(glmnet)
set.seed(100)
cv.lm = cv.glmnet(dummy, dgmat$rating, type.measure = "deviance", alpha=0) # squared-error for gaussian madel
cv.lm$lambda.min
#나머지 점수로 neighborhood method
dgmat$rating = dgmat$rating - (lm$a0 + dummy %*% lm$beta)
dgmat$rating
(lm$a0 + dummy %*% lm$beta)
(lm$a0
)
lm$a0
lm= glmnet(dummy, dgmat$rating, family="gaussian", lambda = cv.lm$lambda.min, alpha=0)
#lm= glmnet(dummy, dgmat$rating, family="gaussian", lambda = 0.2, alpha=0)
head(coef(lm))
#나머지 점수로 neighborhood method
dgmat$rating = dgmat$rating - (lm$a0 + dummy %*% lm$beta)
#각 row의 user(item)가 몇번째 user(item) 인지 확인
user.index = match(dgmat$user, user); item.index = match(dgmat$item, item)
mat=sparseMatrix(i=user.index, j=item.index, x=dgmat$rating)
#dgmat$rating 값 중 0이 없음을 확인하고 NA를 넣음
mat=as.matrix(mat) ; sum(dgmat$rating==0,na.rm=T) ; mat[mat==0]=NA
colnames(mat)=item; rownames(mat)=user
#Recommender 함수를 위해 다시 rating matrix로..
mat= as(mat, "realRatingMatrix")
r1= Recommender(mat, method="UBCF")
pr1=predict(r1, mat, type="ratings")
pr1 = as(pr1, "matrix")
#user를 ""로바꾸는 function
rownames(lm$beta) = gsub('user','', rownames(lm$beta)); rownames(lm$beta) = gsub('item', '', rownames(lm$beta))
item=as.character(item); user=as.character(user)
# 추정된 값들을 따로 저장
tmp.cf=data.frame(as.matrix(rownames(lm$beta)), as.matrix(lm$beta))
mu.0=lm$a0
mu.u=data.frame(user)
mu.i=data.frame(item)
head(tmp.cf)
library(dplyr)
colnames(tmp.cf) =c("user", "coef"); mu.u <- mu.u %>% left_join(tmp.cf, by="user")
colnames(tmp.cf) =c("item", "coef"); mu.i <- mu.i %>% left_join(tmp.cf, by="item")
mu.u[is.na(mu.u$coef),]$coef = 0 ; mu.i[is.na(mu.i$coef),]$coef = 0
rownames(lm$beta) = gsub('user','', rownames(lm$beta)); rownames(lm$beta) = gsub('item', '', rownames(lm$beta))
item=as.character(item); user=as.character(user)
head(tmp.cf)
r1= Recommender(mat, method="UBCF")
pr1=predict(r1, mat, type="ratings")
pr1 = as(pr1, "matrix")
#user를 ""로바꾸는 function
rownames(lm$beta) = gsub('user','', rownames(lm$beta)); rownames(lm$beta) = gsub('item', '', rownames(lm$beta))
item=as.character(item); user=as.character(user)
# 추정된 값들을 따로 저장
tmp.cf=data.frame(as.matrix(rownames(lm$beta)), as.matrix(lm$beta))
mu.0=lm$a0
mu.u=data.frame(user)
mu.i=data.frame(item)
head(tmp.cf)
rownames(lm$beta) = gsub('user','', rownames(lm$beta))
head(tmp.cf)
colnames(tmp.cf) =c("user", "coef"); mu.u <- mu.u %>% left_join(tmp.cf, by="user")
colnames(tmp.cf) =c("item", "coef"); mu.i <- mu.i %>% left_join(tmp.cf, by="item")
mu.u[is.na(mu.u$coef),]$coef = 0 ; mu.i[is.na(mu.i$coef),]$coef = 0
mu.u
mu.i
scale.value=matrix(mu.0, nrow = length(user), ncol = length(item))
scale.value = apply(scale.value, 2, function(x) x+mu.u$coef )
scale.value = t(apply(scale.value, 1, function(x) x+mu.i$coef))
pr1.final =scale.value + pr1
pr1.final
scale.value
pr1.final
#pr1.final
sort(pr1.final[1,], decreasing=T) #user1에 대해 예상평점이 높은 순으로 정렬
sort(pr1.final[2,], decreasing=T)
#RMSE
pr1.final[pr1.final>10]=10 #10보다 크면 10, -10보다 작으면 -10으로 예측하겠다.
pr1.final[pr1.final<(-10)]=-10
RMSE(test,pr1.final)
pr1.final[:1.5]
pr1.final[,1:5]
pr1.final[1:10,1:5]
install.packages("D:/Recommendation 실습자료/matfact.zip", repos = NULL, type = "win.binary")
data("MovieLense")
library(matfact)
mf = matfact(train)
#mf = matfact(as(Jester5k,"matrix")[1:1000,], 0.2, 5, 100)
pred = mf$P %*% t(mf$Q)
colnames(pred)=item ; rownames(pred)=user
head(pred[,1:10])
#prediction
i1 = is.na(train[1,])     #사용자 1,2가 평점을 내리지 않은 상품들을 추출
i2 = is.na(train[2,])
pred[1,i1==T] ; pred[2,i2==T]         #평점을 내리지 않은 상품들에 대해서만 예상평점을 관찰
sort(pred[1,i1==T],decreasing=T)
sort(pred[2,i2==T],decreasing=T)
pred[pred>10]=10 #10보다 크면 10, -10보다 작으면 -10으로 예측하겠다.
pred[pred<(-10)]=-10
RMSE(test,pred)
RMSE(train,pred)
#----- (5-2) scale 보정 ----------#
mat1=as(mat,"matrix")           #앞에서 scale 보정에 사용했던 mat 변수를 그대로 사용하면 된다.
mf1 = matfact(mat1)
pred1 = mf1$P %*% t(mf1$Q)
pred1
colnames(pred1)=item ; rownames(pred1)=user
pred1.final=pred1+scale.value       #앞에서와 같은 보정값을 더해주면 된다
pred1.final[1,i1==T] ; pred1.final[2,i2==T]       #matrix factorization을 통한 user1,2의 예상별점
#별점 순으로 정렬
sort(pred1.final[1,i1==T],decreasing=T); sort(pred1.final[2,i2==T],decreasing=T)
pred1.final[pred1.final>10]=10 #10보다 크면 10, -10보다 작으면 -10으로 예측하겠다.
pred1.final[pred1.final<(-10)]=-10
RMSE(test,pred1.final)
RMSE(train, pred1.final)
RMSE(test,pred)
RMSE(test,pred)
RMSE(train, pred)
rm(list=ls())
data("MovieLense")
library(matfact)
mf = matfact(train)
n.user=1000; n.item=ncol(Jester5k)
rm(list=ls())
ml = data("MovieLense")
ml
rm(list=ls())
data("MovieLense")
dim(MovieLense)
head(getRatingMatrix(MovieLense))
imange(MovieLense[1:100, 1:100])
image(MovieLense[1:100, 1:100])
MovieLense = as(MovieLense, 'matrix')
csoriginal = read.csv('D:/github/R/marketing_analytics/Clothing_Store')
drops = c("HHKEY","ZIP_CODE","REC","PC_CALC20","STORELOY")
cs = csoriginal[,!(colnames(csoriginal) %in% drops)]   # 46 variables
cs$VALPHON = as.integer(cs$VALPHON == "Y")  # transform character into binary
cs
#
#  split data into training set and test set
#
tratio = 0.7   # portion of training data set
nobs = dim(cs)[1]
set.seed(0,kind=NULL)
tryes = runif(nobs)
tryes = (tryes < tratio)
cstrain = cs[tryes==T,]
cstest = cs[tryes==F,]
##############################################
#
# Support Vector Machine ; takes too much time when nobs>10000
#
##############################################
library(e1071)
svmfit = svm(RESP~.,data=cstrain, cost=1, probability=T)
#plot(svmfit,data=chdata,aclength~daymin)
svmfitval = predict(svmfit)
svmpred = predict(svmfit,cstest,type="raw", probability=T)
svmpredpr = attributes(svmpred)$probabilities
cat("SVM prediction \n")
pred.hit(svmpred,cstest$RESP)
?pred.hit
pred(svmpred,cstest$RESP)
library(MASS)
## a function that computes the hit ratio of prediction
pred.hit= function(predval,testdata,noprint=FALSE){
r <- predval # variable1 in rows
c <- testdata # variable2 in columns
# run cross tabulation
ctab <- xtabs(~r+c)
# frequency table
table_f <- ctab
Total1 <- rowSums(ctab); table_f <- cbind(table_f, Total1)
Total2 <- colSums(table_f); table_f <- rbind(table_f, Total2)
# percentage table
table_p <- prop.table(ctab)*100
Total1 <- rowSums(table_p); table_p <- cbind(table_p, Total1)
Total2 <- colSums(table_p); table_p <- rbind(table_p, Total2)
# row percentage table
table_r <- prop.table(ctab, 1)*100; sum <- rowSums(table_r);
table_r <- cbind(table_r, sum)
# col percentage table
table_c <- prop.table(ctab, 2)*100; sum <- colSums(table_c);
table_c <- rbind(table_c, sum);
# print results
if(!noprint){
cat("Prediction (row) vs. Data (column) ", "\n")
cat("* Frequency", "\n"); print(table_f); cat("\n")
cat("* Percentage", "\n"); print(table_p, digits=3);cat("\n")
cat("* Row Percentage: Distribution of Data for each value of Prediction", "\n"); print(table_r, digits=3); cat("\n")
cat("* Column Percentage: Distribution of Prediction for each value of Data", "\n"); print(table_c, digits=3); cat("\n")
}
cat("Hit Ratio:", (sum(diag(table_p))-100), "\n")
precision = ctab[2,2] /(ctab[2,1]+ctab[2,2])
recall = ctab[2,2] /(ctab[2,2]+ctab[1,2])
f1measure = 2*precision*recall/(precision+recall)
cat("F1 measure", f1measure, "\n")
if(!noprint) return(table_f)
}
pred.hit(svmpred,cstest$RESP)
plot(svmpredpr[,2]~cstest$RESP, col="lightblue", main="Prediction  by SVM", xlab="Response Data (Test)", ylab="Probability of RESPONSE")
svmpredpr[,2]
svmpredpr
attributes(svmpred)
attributes(svmpred)$probabilities
svmpred
plot(svmfit,data=cstrain,aclength~daymin)
svmfitval = predict(svmfit)
svmpred = predict(svmfit,cstest,type="raw", probability=T)
svmpredpr = attributes(svmpred)$probabilities
cat("SVM prediction \n")
pred.hit(svmpred,cstest$RESP)
plot(svmpredpr[,2]~cstest$RESP, col="lightblue", main="Prediction  by SVM", xlab="Response Data (Test)", ylab="Probability of RESPONSE")
'+'(2,3)
'-'(2,3)
-(2,3)
'+'(2,3,5)
'(+,-)'(2,3,5)
t.test(12,3)
x = c(1,2,3)
y = c(3,6,9)
t.test(x,y)
chisq.test(x,y)
anova(x,y)
ANOVA(x,y)
anova(x)
library(igraph)
set.seed(2020)
ex1 = graph.empty(() + vertices(letters[1:13], color = 'green'))
set.seed(2020)
ex1 = graph.empty() + vertices(letters[1:13], color = 'green'))
set.seed(2020)
ex1 = graph.empty() + vertices(letters[1:13], color = 'green')
ex1 = ex1 + vertices(letters[14:26], color = 'yellow')
ex1 = ex1 + edges(sample(V(ex1), 80, replace=T), color = 'blue')
head(E(ex1))
sample(V(ex1), 80, replace =T)
plot(ex1)
V(ex1)$color = sample(c('skyblue', 'pink'), vcount(ex1), rep = T)
E(ex1)$color = 'grey'
plot(ex1)
pk = V(ex1)[color =='pink']
E(ex1)[pk %--% pk]$color = 'pink'
skbl = V(ex1)[color=='skyblue']
E(ex1)[skbl %--% skbl]$color = 'skyblue'
plot(ex1)
??pineplot()
??pineplot()
??pineplot
library(scatterplot3d)
attach(mtcars)
scatterplot3d(wt, disp, mpg, main="3D Scatterplot")
library(rgl)
attach(mtcars)
plot3d(wt, disp, mpg, col="red", size=3)
mypal = c('blue', 'red', 'green')
class(mtcars$cyl)
factor(mtcars$cyl)
mypal[factor(mtcars$cyl)]
plot3d(wt, disp, mpg, col= a[factor(mtcars$cyl)], size=10)
library(scatterplot3d)
attach(mtcars)
scatterplot3d(wt, disp, mpg, main="3D Scatterplot")
library(rgl)
attach(mtcars)
plot3d(wt, disp, mpg, col="red", size=3)
