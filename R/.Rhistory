plot(tmp,freq=F,main="Distribution of Mean Price Elasticity", xlab="Mean Price Elasticity", ylab="Probability",col="lightblue")
tmp = hist(tempdelta2,breaks=30,plot=F)
tmp$density = tmp$counts/sum(tmp$counts)
plot(tmp,freq=F,main="Distribution of Mean Promotion Sensitivity", xlab="Mean Promotion Sensitivity", ylab="Probability",col="lightblue")
buytable = read.table('./business_analytics/buytest.txt', header = T)
getwd()
buytable = read.table('buytest.txt', header = T)
buytable
summaty(buytable)
buytable = read.table('buytest.txt', header = T)
buytable
summary(buytable)
buytest[buytest$SEX == "", 'SEX'] = NA
buytest = read.table('buytest.txt', header = T)
buytest
summary(buytest)
buytest[buytest$SEX == "", 'SEX'] = NA
buytest$SEX
levels(buytest$SEX)[1] = NA
buytest$SEX
buytest[buytest$ORGSRC == "", 'ORGSRC'] = NA
levels(buytest$ORGSRC)[1] = NA
buydata = buytest[,-c(1, 10,19:26)] # 사용되지 않는 변수 제거
complete.cases(buydata)
buydata[,complete.cases(buydata)]
buydata = buydata[complete.cases(buydata),] # 결측치 제거
buydata
model.matrix(~., buydata)
model.matrix(~., buydata)[,-1]
buydata = model.matrix(~., buydata)[,-1] # 가변수 생성
buydata
boxplot(buydata$AGE, col = c(1,2))
buydata$AGE
boxplot(buydata[2], col = c(1,2))
boxplot(buydata[2,], col = c(1,2))
buydata
boxplot(buydata[1,],buydata[2,], col = c(1,2))
boxplot(buydata[1,]~buydata[2,], col = c(1,2))
boxplot(buydata[1:2,] col = c(1,2))
boxplot(buydata[1:2,], col = c(1,2))
boxplot(buydata[,1:2], col = c(1,2))
?boxplot
boxplot(formula = (buydata[,1]~buydata[,2]), col = c(1,2))
boxplot(formula = buydata[,1]~buydata[,2], col = c(1,2))
buydata = as.data.frame(buydata)
boxplot(formula = buydata[,1]~buydata[,2], col = c(1,2))
boxplot(formula = buydata$RESPOND~buydata$AGE, col = c(1,2))
logit = glm(RESPOND~., data = buydata, family = 'binomial')
logit
buydata
library(ggplot2)
ggplot(data = buydata) +
geom_boxplot(aes(x = AGE, y = RESPOND), fill = c(1,2))
ggplot(data = buydata, aes(y = RESPOND)) +
geom_boxplot(aes(x = AGE, y = RESPOND), fill = c(1,2))
ggplot(data = buydata) +
geom_boxplot(aes(x = AGE, y = RESPOND))
ggplot(data = buydata[,1:2]) +
geom_boxplot(aes(x = AGE, y = RESPOND))
ggplot(data = buydata, aes(x = AGE, y = RESPOND)) +
geom_boxplot(aes(x = AGE, y = RESPOND))
ggplot(data = buydata, aes(x = RESPOND, y = AGE)) +
geom_boxplot()
ggplot(data = buydata, aes(x = RESPOND, y = AGE)) +
geom_boxplot(mapping = RESPONDE)
ggplot(data = buydata, aes(x = RESPOND, y = AGE)) +
geom_boxplot(aes(group = RESPOND))
ggplot(data = buydata, aes(x = RESPOND, y = AGE)) +
geom_boxplot(aes(group = RESPOND), col = c(1,2))
ggplot(data = buydata, aes(x = RESPOND, y = AGE)) +
geom_boxplot(aes(group = RESPOND), fill = c(1,2))
ggplot(data = buydata, aes(x = RESPOND, y = AGE)) +
geom_boxplot(aes(group = RESPOND), fill = RESPOND)
ggplot(data = buydata, aes(x = RESPOND, y = AGE)) +
geom_boxplot(aes(group = RESPOND), fill = buydata$RESPOND)
ggplot(data = buydata, aes(x = RESPOND, y = AGE)) +
geom_boxplot(aes(group = RESPOND), fill = unique(buydata$RESPOND))
buytest = read.table('buytest.txt', header = T)
buytest
summary(buytest)
buytest[buytest$SEX == "", 'SEX'] = NA
levels(buytest$SEX)[1] = NA
buytest[buytest$ORGSRC == "", 'ORGSRC'] = NA
levels(buytest$ORGSRC)[1] = NA
buydata = buytest[,-c(1, 10,19:26)] # 사용되지 않는 변수 제거
buydata = buydata[complete.cases(buydata),] # 결측치 제거
buydata = model.matrix(~., buydata)[,-1] # 가변수 생성
buydata = as.data.frame(buydata)
logit = glm(RESPOND~., data = buydata, family = 'binomial')
logit
buydata
ggplot(data = buydata, aes(x = RESPOND, y = AGE)) +
geom_boxplot(aes(group = RESPOND), fill = unique(buydata$RESPOND))
unique(buydata$RESPOND)
buydata
set.seed(1)
?sample
set.seed(1)
train_ind = sample(1:nrow(buydata), nrow(buydata)*0.7)
train = as.data.frame(buydata[train_ind,])
test = as.data.frame(buydata[!train_ind, ])
X_train = buydata[train_ind, -1]
y_train = buydata[train_ind, 1]
X_test = buydata[-train_ind, -1]
y_test = buydata[-train_ind, 1]
View(X_test)
par(mfrow = c(1,2))
barplot(table(y_train),
main = paste("The distribution of RESPOND in Training set
\n # of (Y=0): # of (Y=1)=",
paste(round(table(y_train)/sum(y_train == 1),2),
collapse = ":")))
barplot(table(y_test),
main = paste("The distribution of RESPOND in Testset
\n # of (Y=0): # of (Y=1)=",
paste(round(table(y_test)/sum(y_test == 1),2),
collapse = ":")))
logit = glm(RESPOND~., data = train, family = "binomial")
prob = predict(logit, test, type = 'response')
logit = glm(RESPOND~., data = train, family = "binomial")
prob = predict(logit, test, type = 'response')
set.seed(1)
train_ind = sample(1:nrow(buydata), nrow(buydata)*0.7)
train = as.data.frame(buydata[train_ind,])
test = as.data.frame(buydata[!train_ind,])
X_train = buydata[train_ind, -1]
y_train = buydata[train_ind, 1]
X_test = buydata[-train_ind, -1]
y_test = buydata[-train_ind, 1]
logit = glm(RESPOND~., data = train, family = "binomial")
prob = predict(logit, test, type = 'response')
View(test)
test = as.data.frame(buydata[-train_ind,])
prob = predict(logit, test, type = 'response')
prob[1:10]
cutoff = 0.5
ifelse(prob[1:10] > cutoff,1,0)
cutoff = 1/6
classification = function(model, newdata, cutoff){
prob = predict(model,newdata,'response')
ifelse(prob > cutoff,1,0)
}
table(test$RESPOND, classification(logit, test, 0.5))
crosstable = function(model, newdata, cutoff){
table(test$RESPOND,classification(model,newdata,cutoff))
}
crosstable(logit, test, 1/4)
table(test$RESPOND, classification(logit, test, 0.3))
cutoff_res = function(beta_hat = NULL, newx, response, cutoff, pred_prob = NULL){
if (!is.null(beta_hat)) {
X = cbind(1,as.matrix(newx))
pred_prob = 1/(1+exp(-X%*%beta_hat))}
pred = ifelse(pred_prob> cutoff, 1, 0)
error_rate = mean(response != pred)
sensitivity = sum(response == 1 & pred == 1)/sum(response == 1)
specificity = sum(response == 0 & pred == 0)/sum(response == 0)
precision = sum(response == 1 & pred == 1)/sum(pred == 1)
recall = sensitivity
if (sum(response == 1 & pred == 1) == 0) {f1 = 0}
else {f1 = 2*(precision*recall)/(precision+recall)}
cross_table = table(response, pred)
return(list(res = c(cutoff, round(error_rate,4),
round(sensitivity,4), round(specificity,4), round(f1, 4)),
cross_table = cross_table))
}
cutoff_res(logit$coefficients, X_test, y_test, 1/4)
library(ROCR)
prob = predict(logit, train, type = 'response')
library(ROCR)
AUC = performance(prediction(prob , train[,'RESPOND']) , "auc")
AUC@y.values # area under the curve
ROC = performance(prediction(prob ,y_train) , "tpr","fpr")
plot(ROC , main = paste("ROC curve for Train data\n AUC:",
round(as.numeric(AUC@y.values),4)),
col = "blue", lwd = 2.5)
abline(c(0,0), c(1,1), lty = 2, lwd = 2)
cutoff_can = seq(0.01, 0.99, by = 0.01)
cutoff_out = t(sapply(1:length(cutoff_can),
function(i) cutoff_res(logit$coefficients, X_train,
y_train, cutoff_can[i])[[1]]))
colnames(cutoff_out) = c("cutoff","error rate","sensitivity","specificity","f1 score")
cutoff_out
head(cutoff_can)
head(cutoff_out)
plot(cutoff_out$error_rate)
plot(cutoff_out$cutoff, cutoff_out$error_rate)
which.min(curoff_out$error rate)
which.min(curoff_out[,2])
which.min(cutoff_out[,2])
cutoff_out[which.min(cutoff_out[,2]),]
mean(y_train ==1)
cutoff_res(logit$coefficients, X_train, y_train,
cutoff_out[which.min(cutoff_out[,2]), 1])[[2]]
cutoff_out[tail(which(cutoff_out[,3] >= 0.5), n = 1),]
cutoff_sel = cutoff_out[tail(which(cutoff_out[,3] >= 0.5), n = 1), 1]
cutoff_res(logit$coefficients, X_train, y_train, cutoff_sel)[[2]]
which(cutoff_out[,3] >= 0.5
)
cutoff_out[which.max(cutoff_out[,5]),]
cutoff_res(logit$coefficients, X_train, y_train,
cutoff_out[which.max(cutoff_out[,5]), 1])[[2]]
cutoff = c()
cutoff[1] = cutoff_out[which.min(cutoff_out[,2]), 1]
cutoff[2] = cutoff_out[tail(which(cutoff_out[,3] >= 0.5), n = 1), 1]
cutoff[3] = cutoff_out[which.max(cutoff_out[,5]), 1]
as.data.frame(sapply(cutoff, function(cut) cutoff_res(logit$coefficients, X_test,
y_test, cut)[[1]]),
row.names = colnames(cutoff_out))
cutoff_out[which.min(cutoff_out[,2]), 1]
cutoff_out[tail(which(cutoff_out[,3] >= 0.5), n = 1), 1]
as.data.frame(sapply(cutoff, function(cut) cutoff_res(logit$coefficients, X_test, y_test, cut)[[1]]), row.names = colnames(cutoff_out))
rm(list=ls())
gc()
null = glm(RESPOND~1, data = train, family = 'binomial')
buytest = read.table('buytest.txt', header = T)
buytest[buytest$SEX == "", 'SEX'] = NA
levels(buytest$SEX)[1] = NA
buytest[buytest$ORGSRC == "", 'ORGSRC'] = NA
levels(buytest$ORGSRC)[1] = NA
buydata = buytest[,-c(1, 10,19:26)] # 사용되지 않는 변수 제거
buydata = buydata[complete.cases(buydata),] # 결측치 제거
buydata = model.matrix(~., buydata)[,-1] # 가변수 생성
set.seed(1)
train_ind = sample(1:nrow(buydata), nrow(buydata)*0.7)
train = as.data.frame(buydata[train_ind,])
test = as.data.frame(buydata[-train_ind,])
null = glm(RESPOND~1, data = train, family = 'binomial')
full = glm(RESPOND~., data = train, family = 'binomial')
forward = step(null, scope = list(lower = null, upper = full),
data = train, direction = "forward")
summary(forward)
null
library(glmnet)
ridge.fit = glmnet(X_train, as.factor(y_train), alpha = 0,
family="binomial" )
X_train = buydata[train_ind, -1]
y_train = buydata[train_ind, 1]
X_test = buydata[-train_ind, -1]
y_test = buydata[-train_ind, 1]
ridge.fit = glmnet(X_train, as.factor(y_train), alpha = 0,
family="binomial" )
ridge.fit$lambda[c(1, 10, 100)]
ridge.fit$beta[,c(1, 10, 100)]
ridge.fit$lambda
ridge.fit$beta[,c(1, 10, 100)]
set.seed(1)
cv.ridge = cv.glmnet(X_train, as.factor(y_train), alpha = 0, family = "binomial")
bestlam = cv.ridge$lambda.min
ridge.fit = glmnet(X_train,as.factor(y_train), alpha = 0, lambda = bestlam, family = "binomial")
ridge.fit$beta
ridge.fit = glmnet(X_train, as.factor(y_train), alpha = 0, lambda = bestlam, family = "binomial")
ridge.fit$beta
lasso.fit = glmnet(X_train, as.factor(y_train), alpha = 1, family="binomial")
lasso.fit$lambda[c(1, 5, 10)]
lasso.fit$beta[,c(1, 5, 10)]
set.seed(1)
cv.lasso = cv.glmnet(X_train, as.factor(y_train), alpha = 1,
family="binomial")
bestlam = cv.lasso$lambda.min
lasso.fit = glmnet(X_train, as.factor(y_train), alpha = 1,
lambda = bestlam, family="binomial")
lasso.fit$beta
beta_hat = logit$coefficients
#beta_hat = logit$coefficients
#------------------------------------
# forward selection
#------------------------------------
tmp = forward$coefficients
beta_forward = c()
for (i in 1:length(beta_hat)){
if (names(beta_hat)[i] %in% names(tmp)) beta_forward[i] = tmp[names(beta_hat)[i]]
else beta_forward[i] = 0
}
beta_hat = none$coefficients
beta_hat = null$coefficients
#------------------------------------
# forward selection
#------------------------------------
tmp = forward$coefficients
beta_forward = c()
for (i in 1:length(beta_hat)){
if (names(beta_hat)[i] %in% names(tmp)) beta_forward[i] = tmp[names(beta_hat)[i]]
else beta_forward[i] = 0
}
beta_hat = cbind(beta_hat, beta_forward)
#------------------------------------
# Ridge & LASSO
#------------------------------------
beta_hat = cbind(beta_hat, c(ridge.fit$a0, as.vector(ridge.fit$beta)),
c(lasso.fit$a0, as.vector(lasso.fit$beta)))
logit = glm(RESPOND~., data = train, family = "binomial")
beta_hat = logit$coefficients
#------------------------------------
# forward selection
#------------------------------------
tmp = forward$coefficients
beta_forward = c()
for (i in 1:length(beta_hat)){
if (names(beta_hat)[i] %in% names(tmp)) beta_forward[i] = tmp[names(beta_hat)[i]]
else beta_forward[i] = 0
}
beta_hat = cbind(beta_hat, beta_forward)
#------------------------------------
# Ridge & LASSO
#------------------------------------
beta_hat = cbind(beta_hat, c(ridge.fit$a0, as.vector(ridge.fit$beta)),
c(lasso.fit$a0, as.vector(lasso.fit$beta)))
library(ROCR)
auc_res = function(beta = NULL, newx, newy, pred_prob = NULL){
if (!is.null(beta)){
X = cbind(1,as.matrix(newx))
pred_prob = 1/(1+exp(-X%*%beta)) }
AUC = performance(prediction(pred_prob , newy) , "auc")
return(AUC@y.values[[1]])
}
auc_table = rbind(sapply(1:4, function(i) auc_res(beta_hat[,i], X_train, y_train)),
sapply(1:4, function(i) auc_res(beta_hat[,i], X_test, y_test)))
rownames(auc_table) = c('Training set', 'Test set')
colnames(auc_table) = model_names = c('Logistic','Logistic+AIC', 'Ridge', 'LASSO')
auc_table
set.seed(5)
cv.ridge = cv.glmnet(X_train, as.factor(y_train), alpha = 0, family = "binomial")
bestlam = cv.ridge$lambda.min
ridge.fit = glmnet(X_train, as.factor(y_train), alpha = 0, lambda = bestlam, family = "binomial")
ridge.fit$beta
set.seed(5)
cv.lasso = cv.glmnet(X_train, as.factor(y_train), alpha = 1, family="binomial")
bestlam = cv.lasso$lambda.min
lasso.fit = glmnet(X_train, as.factor(y_train), alpha = 1, lambda = bestlam, family="binomial")
lasso.fit$beta
library(ROCR)
auc_res = function(beta = NULL, newx, newy, pred_prob = NULL){
if (!is.null(beta)){
X = cbind(1,as.matrix(newx))
pred_prob = 1/(1+exp(-X%*%beta)) }
AUC = performance(prediction(pred_prob , newy) , "auc")
return(AUC@y.values[[1]])
}
auc_table = rbind(sapply(1:4, function(i) auc_res(beta_hat[,i], X_train, y_train)),
sapply(1:4, function(i) auc_res(beta_hat[,i], X_test, y_test)))
rownames(auc_table) = c('Training set', 'Test set')
colnames(auc_table) = model_names = c('Logistic','Logistic+AIC', 'Ridge', 'LASSO')
auc_table
set.seed(1)
cv.lasso = cv.glmnet(X_train, as.factor(y_train), alpha = 1, family="binomial")
bestlam = cv.lasso$lambda.min
lasso.fit = glmnet(X_train, as.factor(y_train), alpha = 1, lambda = bestlam, family="binomial")
lasso.fit$beta
set.seed(1)
cv.ridge = cv.glmnet(X_train, as.factor(y_train), alpha = 0, family = "binomial")
bestlam = cv.ridge$lambda.min
ridge.fit = glmnet(X_train, as.factor(y_train), alpha = 0, lambda = bestlam, family = "binomial")
ridge.fit$beta
library(ROCR)
auc_res = function(beta = NULL, newx, newy, pred_prob = NULL){
if (!is.null(beta)){
X = cbind(1,as.matrix(newx))
pred_prob = 1/(1+exp(-X%*%beta)) }
AUC = performance(prediction(pred_prob , newy) , "auc")
return(AUC@y.values[[1]])
}
auc_table = rbind(sapply(1:4, function(i) auc_res(beta_hat[,i], X_train, y_train)),
sapply(1:4, function(i) auc_res(beta_hat[,i], X_test, y_test)))
rownames(auc_table) = c('Training set', 'Test set')
colnames(auc_table) = model_names = c('Logistic','Logistic+AIC', 'Ridge', 'LASSO')
auc_table
cut_sel = matrix(0, nrow = 4, ncol = 3)
for (i in 1:4){
cutoff_out = t(sapply(1:length(cutoff_can),
function(j) cutoff_res(beta_hat[,i], X_train,
y_train, cutoff_can[j])[[1]]))
cut_sel[i, 1] = cutoff_out[which.min(cutoff_out[,2]), 1]
cut_sel[i, 2] = cutoff_out[tail(which(cutoff_out[,3] >= 0.5), n = 1), 1]
cut_sel[i, 3] = cutoff_out[which.max(cutoff_out[,5]), 1]
}
cut_sel = matrix(0, nrow = 4, ncol = 3)
matrix(t(sapply(1:3, function(i) cutoff_res(beta_hat[,i], X_test,
y_test, cut_sel[i, 1])[[1]])),
nrow = 3,
dimnames =list(model_names, c("error rate","sensitivity",
"specificity","f1 score")))
cut_sel = matrix(0, nrow = 3, ncol = 3)
matrix(t(sapply(1:3, function(i) cutoff_res(beta_hat[,i], X_test,
y_test, cut_sel[i, 1])[[1]])),
nrow = 3,
dimnames =list(model_names, c("error rate","sensitivity",
"specificity","f1 score")))
cut_sel = matrix(0, nrow = 3, ncol = 3)
matrix(t(sapply(1:3, function(i) , X_test, y_test, cut_sel[i, 1])[[1]])),
cut_sel = matrix(0, nrow = 3, ncol = 3)
matrix(t(sapply(1:3, function(i) X_test, y_test, cut_sel[i, 1])[[1]])),
cut_sel = matrix(0, nrow = 3, ncol = 3)
matrix(t(sapply(1:3, function(i) X_test, y_test, cut_sel[i, 1])[[1]]),
nrow = 3,
dimnames =list(model_names, c("error rate","sensitivity",
"specificity","f1 score")))
cut_sel = matrix(0, nrow = 3, ncol = 3)
matrix(t(sapply(1:3, function(i) X_test, y_test, cut_sel[i, 1])[[1]]),
nrow = 3,
dimnames =list(model_names, c("error rate","sensitivity",
"specificity","f1 score")))
cut_sel = matrix(0, nrow = 3, ncol = 3)
matrix(t(sapply(1:3, function(i) cutoff_res(beta_hat[,i], X_test,
y_test, cut_sel[i, 1])[[1]])),
nrow = 3,
dimnames =list(model_names, c("error rate","sensitivity",
"specificity","f1 score")))
cutoff_res = function(beta_hat = NULL, newx, response, cutoff, pred_prob = NULL){
if (!is.null(beta_hat)) {
X = cbind(1,as.matrix(newx))
pred_prob = 1/(1+exp(-X%*%beta_hat))}
cut_sel = matrix(0, nrow = 3, ncol = 3)
matrix(t(sapply(1:3, function(i) cutoff_res(X_test,
y_test, cut_sel[i, 1])[[1]])),
nrow = 3,
dimnames =list(model_names, c("error rate","sensitivity",
"specificity","f1 score")))
cutoff_res = function(beta_hat = NULL, newx, response, cutoff, pred_prob = NULL){
if (!is.null(beta_hat)) {
X = cbind(1,as.matrix(newx))
pred_prob = 1/(1+exp(-X%*%beta_hat))}
pred = ifelse(pred_prob> cutoff, 1, 0)
error_rate = mean(response != pred)
sensitivity = sum(response == 1 & pred == 1)/sum(response == 1)
specificity = sum(response == 0 & pred == 0)/sum(response == 0)
precision = sum(response == 1 & pred == 1)/sum(pred == 1)
recall = sensitivity
if (sum(response == 1 & pred == 1) == 0) {f1 = 0}
else {f1 = 2*(precision*recall)/(precision+recall)}
cross_table = table(response, pred)
return(list(res = c(cutoff, round(error_rate,4),
round(sensitivity,4), round(specificity,4), round(f1, 4)),
cross_table = cross_table))
}
cut_sel = matrix(0, nrow = 3, ncol = 3)
matrix(t(sapply(1:3, function(i) cutoff_res(X_test,
y_test, cut_sel[i, 1])[[1]])),
nrow = 3,
dimnames =list(model_names, c("error rate","sensitivity",
"specificity","f1 score")))
cut_sel = matrix(0, nrow = 3, ncol = 3)
matrix(t(sapply(1:3, function(i) cutoff_res(X_test,
y_test, cut_sel[i, 1])[[1]])),
nrow = 3,
dimnames =list(model_names, c("error rate","sensitivity",
"specificity","f1 score")))
matrix(t(sapply(1:3, function(i) cutoff_res(X_test,
y_test, cut_sel[i, 1])[[1]])),
nrow = 3,
dimnames =list(model_names, c("error rate","sensitivity",
"specificity","f1 score")))
cutoff_res = function(beta_hat = NULL, newx, response, cutoff, pred_prob = NULL){
if (!is.null(beta_hat)) {
X = cbind(1,as.matrix(newx))
pred_prob = 1/(1+exp(-X%*%beta_hat))}
pred = ifelse(pred_prob> cutoff, 1, 0)
error_rate = mean(response != pred)
sensitivity = sum(response == 1 & pred == 1)/sum(response == 1)
specificity = sum(response == 0 & pred == 0)/sum(response == 0)
precision = sum(response == 1 & pred == 1)/sum(pred == 1)
recall = sensitivity
if (sum(response == 1 & pred == 1) == 0) {f1 = 0}
else {f1 = 2*(precision*recall)/(precision+recall)}
cross_table = table(response, pred)
return(list(res = c(cutoff, round(error_rate,4),
round(sensitivity,4), round(specificity,4), round(f1, 4)),
cross_table = cross_table))
}
cut_sel = matrix(0, nrow = 3, ncol = 3)
matrix(t(sapply(1:3, function(i) cutoff_res(X_test,
y_test, cut_sel[i, 1])[[1]])),
nrow = 3,
dimnames =list(model_names, c("error rate","sensitivity",
"specificity","f1 score")))
cut_sel = matrix(0, nrow = 3, ncol = 3)
matrix(t(sapply(1:4, function(i) cutoff_res(X_test,
y_test, cut_sel[i, 1])[[1]])),
nrow = 4,
dimnames =list(model_names, c("cutoff","error rate","sensitivity", "specificity","f1 score")))
cutoff_res(X_test,
y_test, cut_sel[i, 1])
cut_sel = matrix(0, nrow = 4, ncol = 3)
for (i in 1:4){
cutoff_out = t(sapply(1:length(cutoff_can),
function(j) cutoff_res(beta_hat[,i], X_train,
y_train, cutoff_can[j])[[1]]))
cut_sel[i, 1] = cutoff_out[which.min(cutoff_out[,2]), 1]
cut_sel[i, 2] = cutoff_out[tail(which(cutoff_out[,3] >= 0.5), n = 1), 1]
cut_sel[i, 3] = cutoff_out[which.max(cutoff_out[,5]), 1]
}
cutoff_can = seq(0.01, 0.99, by = 0.01)
for (i in 1:4){
cutoff_out = t(sapply(1:length(cutoff_can),
function(j) cutoff_res(beta_hat[,i], X_train,
y_train, cutoff_can[j])[[1]]))
cut_sel[i, 1] = cutoff_out[which.min(cutoff_out[,2]), 1]
cut_sel[i, 2] = cutoff_out[tail(which(cutoff_out[,3] >= 0.5), n = 1), 1]
cut_sel[i, 3] = cutoff_out[which.max(cutoff_out[,5]), 1]
}
for (i in 1:4){
cutoff_out = t(sapply(1:length(cutoff_can),
function(j) cutoff_res(beta_hat[,i], X_train,
y_train, cutoff_can[j])[[1]]))
cut_sel[i, 1] = cutoff_out[which.min(cutoff_out[,2]), 1]
cut_sel[i, 2] = cutoff_out[tail(which(cutoff_out[,3] >= 0.5), n = 1), 1]
cut_sel[i, 3] = cutoff_out[which.max(cutoff_out[,5]), 1]
}
matrix(t(sapply(1:4, function(i) cutoff_res(beta_hat[,i], X_test,
y_test, cut_sel[i, 1])[[1]])),
nrow = 4,
dimnames =list(model_names, c("cutoff","error rate","sensitivity",
"specificity","f1 score")))
