X
model
model
importance = xgb.importance(colnames(X), model = model)
xgb.ggplot.importance(importance)
insta = read.csv('d:/recommendation 실습자료/instacart.csv')
insta
set.seed(1)
idx.ts = sample (1:length(unique(insta$user_id)), length(unique(insta$user_id))*0.3)
idx.ts = sort (idx.ts)
user.tr = as.data.frame(unique(insta$user_id)[-idx.ts])
user.ts = as.data.frame(unique(insta$user_id)[idx.ts])
colnames(user.tr) = 'user_id'
colnames(user.ts) = 'user_id'
library(dplyr)
tr.mat = insta %>% inner_join(user.tr, by = 'user_id')
ts.mat = insta %>% inner_join(user.ts, by = 'user_id')
library(xgboost)
colnames(tr.mat)
X = xgb.DMatrix(as.matrix(tr.mat[,-c(1,2,37)]), label = tr.mat$reordered)
model = xgboost(data = X, max_depth = 5, eta = 0.1, nrounds = 200, objective = 'binary:logistic')
# Apply
test.mat = xgb.DMatrix(ts.mat[,-c(1,2,37)])
importance = xgb.importance(colnames(X), model = model)
xgb.ggplot.importance(importance)
# Apply
test.mat = xgb.DMatrix(ts.mat[,-c(1,2,37)])
ts.mat[,-c(1,2,37)]
xgb.DMatrix(ts.mat[,-c(1,2,37)])
X = xgb.DMatrix(as.matrix(tr.mat[,-c(1,2,37)]), label = tr.mat$reordered)
model = xgboost(data = X, max_depth = 5, eta = 0.1, nrounds = 200, objective = 'binary:logistic')
model
test.mat = xgb.DMatrix(as.matrix(ts.mat[,-c(1,2,37)]))
ts.mat$fitted = predict(model, test.mat)
as.matrix(ts.mat[,-c(1,2,37)])
rm(list=ls())
gc()
library(recommenderlab)
install.packages("recommenderlab")
library(recommenderlab)
data(Jester5k)
head(getRatingMatrix(Jester5k))
image(Jester5k[1:80,])
head(getRatingMatrix(Jester5k))
image(Jester5k[1:80,])
n.user=1000; n.item=ncol(Jester5k)
a <- as(Jester5k[1:n.user],"matrix") #1000 명의 user만 사용
set.seed(123)
subset=sample.int(sum(!is.na(a)),sum(!is.na(a))*.3) #rating 된 점수중 30%만 추출
subset=sort(subset)
train = a; test =a
train[!is.na(train)][subset] = NA; test[!is.na(test)][-subset] = NA #rating 된 점수들 중 train, test 나눔
subset
train
train_head()
train(head)
head(train)
#User based Collaborative filtering
rtrain = as(train,"realRatingMatrix") #rating matrix 만 함수의 input으로 가능
r=Recommender(rtrain, method="UBCF")  #method를 IBCF로하면 item-based CF
getModel(r)$method
getModel(r)$nn
pr=predict(r, rtrain[1:2,], type="ratings")
as(pr, "matrix") #이미 평가한 자료의 경우 예측값을 주지 않는다
#n을 통해 추천품목수를 조정 가능
ptype=predict(r, rtrain[1:2,], n=5)
as(ptype, "list")
#RMSE
pr=predict(r, rtrain, type="ratings")
pr=as(pr, "matrix") #이미 평가한 자료의 경우 예측값을 주지 않는다
pr[pr>10]=10 #10보다 크면 10, -10보다 작으면 -10으로 예측하겠다.
pr[pr<(-10)]=-10
RMSE(test, pr)
#----- (4-2) 스케일 보정  ----------#
dgmat=cbind(train[1:(n.user*n.item)], as.data.frame(cbind(rep(rownames(train), n.item),
rep(colnames(train), each=n.user))))
head(dgmat)
#각 행은 하나의 평점에 대한 정보를 가짐
colnames(dgmat) <- c("rating","user","item")
user = unique(dgmat$user); item = unique(dgmat$item)
head(dgmat)
dgmat = dgmat[is.na(dgmat$rating)==F,]        #평점이 있는 정보만 사용
#making dummy variable
dummy = model.matrix(rating~user+item, dgmat) #user와 item에 대해 dummy variable 생성, 변수 1개 기준으로 999개
dummy = dummy[,-1] # coefficient term은 제외하자
library(glmnet)
set.seed(100)
cv.lm = cv.glmnet(dummy, dgmat$rating, type.measure = "deviance", alpha=0) # squared-error for gaussian madel
cv.lm$lambda.min
#나머지 점수로 neighborhood method
dgmat$rating = dgmat$rating - (lm$a0 + dummy %*% lm$beta)
dgmat$rating
(lm$a0 + dummy %*% lm$beta)
(lm$a0
)
lm$a0
lm= glmnet(dummy, dgmat$rating, family="gaussian", lambda = cv.lm$lambda.min, alpha=0)
#lm= glmnet(dummy, dgmat$rating, family="gaussian", lambda = 0.2, alpha=0)
head(coef(lm))
#나머지 점수로 neighborhood method
dgmat$rating = dgmat$rating - (lm$a0 + dummy %*% lm$beta)
#각 row의 user(item)가 몇번째 user(item) 인지 확인
user.index = match(dgmat$user, user); item.index = match(dgmat$item, item)
mat=sparseMatrix(i=user.index, j=item.index, x=dgmat$rating)
#dgmat$rating 값 중 0이 없음을 확인하고 NA를 넣음
mat=as.matrix(mat) ; sum(dgmat$rating==0,na.rm=T) ; mat[mat==0]=NA
colnames(mat)=item; rownames(mat)=user
#Recommender 함수를 위해 다시 rating matrix로..
mat= as(mat, "realRatingMatrix")
r1= Recommender(mat, method="UBCF")
pr1=predict(r1, mat, type="ratings")
pr1 = as(pr1, "matrix")
#user를 ""로바꾸는 function
rownames(lm$beta) = gsub('user','', rownames(lm$beta)); rownames(lm$beta) = gsub('item', '', rownames(lm$beta))
item=as.character(item); user=as.character(user)
# 추정된 값들을 따로 저장
tmp.cf=data.frame(as.matrix(rownames(lm$beta)), as.matrix(lm$beta))
mu.0=lm$a0
mu.u=data.frame(user)
mu.i=data.frame(item)
head(tmp.cf)
library(dplyr)
colnames(tmp.cf) =c("user", "coef"); mu.u <- mu.u %>% left_join(tmp.cf, by="user")
colnames(tmp.cf) =c("item", "coef"); mu.i <- mu.i %>% left_join(tmp.cf, by="item")
mu.u[is.na(mu.u$coef),]$coef = 0 ; mu.i[is.na(mu.i$coef),]$coef = 0
rownames(lm$beta) = gsub('user','', rownames(lm$beta)); rownames(lm$beta) = gsub('item', '', rownames(lm$beta))
item=as.character(item); user=as.character(user)
head(tmp.cf)
r1= Recommender(mat, method="UBCF")
pr1=predict(r1, mat, type="ratings")
pr1 = as(pr1, "matrix")
#user를 ""로바꾸는 function
rownames(lm$beta) = gsub('user','', rownames(lm$beta)); rownames(lm$beta) = gsub('item', '', rownames(lm$beta))
item=as.character(item); user=as.character(user)
# 추정된 값들을 따로 저장
tmp.cf=data.frame(as.matrix(rownames(lm$beta)), as.matrix(lm$beta))
mu.0=lm$a0
mu.u=data.frame(user)
mu.i=data.frame(item)
head(tmp.cf)
rownames(lm$beta) = gsub('user','', rownames(lm$beta))
head(tmp.cf)
colnames(tmp.cf) =c("user", "coef"); mu.u <- mu.u %>% left_join(tmp.cf, by="user")
colnames(tmp.cf) =c("item", "coef"); mu.i <- mu.i %>% left_join(tmp.cf, by="item")
mu.u[is.na(mu.u$coef),]$coef = 0 ; mu.i[is.na(mu.i$coef),]$coef = 0
mu.u
mu.i
scale.value=matrix(mu.0, nrow = length(user), ncol = length(item))
scale.value = apply(scale.value, 2, function(x) x+mu.u$coef )
scale.value = t(apply(scale.value, 1, function(x) x+mu.i$coef))
pr1.final =scale.value + pr1
pr1.final
scale.value
pr1.final
#pr1.final
sort(pr1.final[1,], decreasing=T) #user1에 대해 예상평점이 높은 순으로 정렬
sort(pr1.final[2,], decreasing=T)
#RMSE
pr1.final[pr1.final>10]=10 #10보다 크면 10, -10보다 작으면 -10으로 예측하겠다.
pr1.final[pr1.final<(-10)]=-10
RMSE(test,pr1.final)
pr1.final[:1.5]
pr1.final[,1:5]
pr1.final[1:10,1:5]
install.packages("D:/Recommendation 실습자료/matfact.zip", repos = NULL, type = "win.binary")
data("MovieLense")
library(matfact)
mf = matfact(train)
#mf = matfact(as(Jester5k,"matrix")[1:1000,], 0.2, 5, 100)
pred = mf$P %*% t(mf$Q)
colnames(pred)=item ; rownames(pred)=user
head(pred[,1:10])
#prediction
i1 = is.na(train[1,])     #사용자 1,2가 평점을 내리지 않은 상품들을 추출
i2 = is.na(train[2,])
pred[1,i1==T] ; pred[2,i2==T]         #평점을 내리지 않은 상품들에 대해서만 예상평점을 관찰
sort(pred[1,i1==T],decreasing=T)
sort(pred[2,i2==T],decreasing=T)
pred[pred>10]=10 #10보다 크면 10, -10보다 작으면 -10으로 예측하겠다.
pred[pred<(-10)]=-10
RMSE(test,pred)
RMSE(train,pred)
#----- (5-2) scale 보정 ----------#
mat1=as(mat,"matrix")           #앞에서 scale 보정에 사용했던 mat 변수를 그대로 사용하면 된다.
mf1 = matfact(mat1)
pred1 = mf1$P %*% t(mf1$Q)
pred1
colnames(pred1)=item ; rownames(pred1)=user
pred1.final=pred1+scale.value       #앞에서와 같은 보정값을 더해주면 된다
pred1.final[1,i1==T] ; pred1.final[2,i2==T]       #matrix factorization을 통한 user1,2의 예상별점
#별점 순으로 정렬
sort(pred1.final[1,i1==T],decreasing=T); sort(pred1.final[2,i2==T],decreasing=T)
pred1.final[pred1.final>10]=10 #10보다 크면 10, -10보다 작으면 -10으로 예측하겠다.
pred1.final[pred1.final<(-10)]=-10
RMSE(test,pred1.final)
RMSE(train, pred1.final)
RMSE(test,pred)
RMSE(test,pred)
RMSE(train, pred)
rm(list=ls())
data("MovieLense")
library(matfact)
mf = matfact(train)
n.user=1000; n.item=ncol(Jester5k)
rm(list=ls())
ml = data("MovieLense")
ml
rm(list=ls())
data("MovieLense")
dim(MovieLense)
head(getRatingMatrix(MovieLense))
imange(MovieLense[1:100, 1:100])
image(MovieLense[1:100, 1:100])
MovieLense = as(MovieLense, 'matrix')
csoriginal = read.csv('D:/github/R/marketing_analytics/Clothing_Store')
drops = c("HHKEY","ZIP_CODE","REC","PC_CALC20","STORELOY")
cs = csoriginal[,!(colnames(csoriginal) %in% drops)]   # 46 variables
cs$VALPHON = as.integer(cs$VALPHON == "Y")  # transform character into binary
cs
#
#  split data into training set and test set
#
tratio = 0.7   # portion of training data set
nobs = dim(cs)[1]
set.seed(0,kind=NULL)
tryes = runif(nobs)
tryes = (tryes < tratio)
cstrain = cs[tryes==T,]
cstest = cs[tryes==F,]
##############################################
#
# Support Vector Machine ; takes too much time when nobs>10000
#
##############################################
library(e1071)
svmfit = svm(RESP~.,data=cstrain, cost=1, probability=T)
#plot(svmfit,data=chdata,aclength~daymin)
svmfitval = predict(svmfit)
svmpred = predict(svmfit,cstest,type="raw", probability=T)
svmpredpr = attributes(svmpred)$probabilities
cat("SVM prediction \n")
pred.hit(svmpred,cstest$RESP)
?pred.hit
pred(svmpred,cstest$RESP)
library(MASS)
## a function that computes the hit ratio of prediction
pred.hit= function(predval,testdata,noprint=FALSE){
r <- predval # variable1 in rows
c <- testdata # variable2 in columns
# run cross tabulation
ctab <- xtabs(~r+c)
# frequency table
table_f <- ctab
Total1 <- rowSums(ctab); table_f <- cbind(table_f, Total1)
Total2 <- colSums(table_f); table_f <- rbind(table_f, Total2)
# percentage table
table_p <- prop.table(ctab)*100
Total1 <- rowSums(table_p); table_p <- cbind(table_p, Total1)
Total2 <- colSums(table_p); table_p <- rbind(table_p, Total2)
# row percentage table
table_r <- prop.table(ctab, 1)*100; sum <- rowSums(table_r);
table_r <- cbind(table_r, sum)
# col percentage table
table_c <- prop.table(ctab, 2)*100; sum <- colSums(table_c);
table_c <- rbind(table_c, sum);
# print results
if(!noprint){
cat("Prediction (row) vs. Data (column) ", "\n")
cat("* Frequency", "\n"); print(table_f); cat("\n")
cat("* Percentage", "\n"); print(table_p, digits=3);cat("\n")
cat("* Row Percentage: Distribution of Data for each value of Prediction", "\n"); print(table_r, digits=3); cat("\n")
cat("* Column Percentage: Distribution of Prediction for each value of Data", "\n"); print(table_c, digits=3); cat("\n")
}
cat("Hit Ratio:", (sum(diag(table_p))-100), "\n")
precision = ctab[2,2] /(ctab[2,1]+ctab[2,2])
recall = ctab[2,2] /(ctab[2,2]+ctab[1,2])
f1measure = 2*precision*recall/(precision+recall)
cat("F1 measure", f1measure, "\n")
if(!noprint) return(table_f)
}
pred.hit(svmpred,cstest$RESP)
plot(svmpredpr[,2]~cstest$RESP, col="lightblue", main="Prediction  by SVM", xlab="Response Data (Test)", ylab="Probability of RESPONSE")
svmpredpr[,2]
svmpredpr
attributes(svmpred)
attributes(svmpred)$probabilities
svmpred
plot(svmfit,data=cstrain,aclength~daymin)
svmfitval = predict(svmfit)
svmpred = predict(svmfit,cstest,type="raw", probability=T)
svmpredpr = attributes(svmpred)$probabilities
cat("SVM prediction \n")
pred.hit(svmpred,cstest$RESP)
plot(svmpredpr[,2]~cstest$RESP, col="lightblue", main="Prediction  by SVM", xlab="Response Data (Test)", ylab="Probability of RESPONSE")
'+'(2,3)
'-'(2,3)
-(2,3)
'+'(2,3,5)
'(+,-)'(2,3,5)
t.test(12,3)
x = c(1,2,3)
y = c(3,6,9)
t.test(x,y)
chisq.test(x,y)
anova(x,y)
ANOVA(x,y)
anova(x)
library(igraph)
set.seed(2020)
ex1 = graph.empty(() + vertices(letters[1:13], color = 'green'))
set.seed(2020)
ex1 = graph.empty() + vertices(letters[1:13], color = 'green'))
set.seed(2020)
ex1 = graph.empty() + vertices(letters[1:13], color = 'green')
ex1 = ex1 + vertices(letters[14:26], color = 'yellow')
ex1 = ex1 + edges(sample(V(ex1), 80, replace=T), color = 'blue')
head(E(ex1))
sample(V(ex1), 80, replace =T)
plot(ex1)
V(ex1)$color = sample(c('skyblue', 'pink'), vcount(ex1), rep = T)
E(ex1)$color = 'grey'
plot(ex1)
pk = V(ex1)[color =='pink']
E(ex1)[pk %--% pk]$color = 'pink'
skbl = V(ex1)[color=='skyblue']
E(ex1)[skbl %--% skbl]$color = 'skyblue'
plot(ex1)
??pineplot()
??pineplot()
??pineplot
library(scatterplot3d)
attach(mtcars)
scatterplot3d(wt, disp, mpg, main="3D Scatterplot")
library(rgl)
attach(mtcars)
plot3d(wt, disp, mpg, col="red", size=3)
mypal = c('blue', 'red', 'green')
class(mtcars$cyl)
factor(mtcars$cyl)
mypal[factor(mtcars$cyl)]
plot3d(wt, disp, mpg, col= a[factor(mtcars$cyl)], size=10)
library(scatterplot3d)
attach(mtcars)
scatterplot3d(wt, disp, mpg, main="3D Scatterplot")
library(rgl)
attach(mtcars)
plot3d(wt, disp, mpg, col="red", size=3)
movies = read.csv('D:\\data_mining_DB\\movies.csv', stringsAsFactors = F)
str(movies)
ratings = read.csv('D:\\data_mining_db\\ratings.csv')
str(ratings)
library(reshape2)
ratingmat = dcast(ratings, userId~movieId, value.var = 'rating', na.rm = F)
ratingmat
ratingmat[1:10,1:10]
ratingmat = as.matrix(ratingmat[,-1])
library(ggplot2)
image(ratingmat, main = "Raw Ratings")
library(recommenderlab)
#Coerce the rating matrix into a recommenderlab sparse matrix
ratingmat <- as(ratingmat, "realRatingMatrix")
#Normalize the data
ratingmat_norm <- normalize(ratingmat)
#Create Recommender Model.
#        UBCF: User-based collaborative filtering
#        IBCF: Item-based collaborative filtering
#        Parameter 'method' decides similarity measure: Cosine or Jaccard
# Here we use UBCF, cosine similarity, nearest neighbors = 30
recommender_model = Recommender(ratingmat_norm, method = "UBCF", param=list(method="Cosine",nn=30))
# other methods
#recommender_model = Recommender(ratingmat_norm, method = "UBCF", param=list(method="Jaccard",nn=30))
#recommender_model = Recommender(ratingmat_norm, method = "IBCF", param=list(method="Jaccard",nn=30))
#recommender_model = Recommender(ratingmat_norm, method = "POPULAR")
recom = predict(recommender_model, ratingmat[1], n=10) #Obtain top 10 recommendations for 1st user in dataset
recom_list = as(recom, "list") #convert recommenderlab object to readable list
# Obtain recommendations
recom_num = NULL
recom_title = NULL
recom_genre = NULL
for (i in c(1:10)){
recom_num = c(recom_num,  movies[as.integer(recom_list[[1]][i]),1])
recom_title = c(recom_title,  movies[as.integer(recom_list[[1]][i]),2])
recom_genre = c(recom_genre,  movies[as.integer(recom_list[[1]][i]),3])
}
myrecom = data.frame(recom_num,recom_title, recom_genre)
cat('Recommendation for the user \n')
print(myrecom)
nrow(ratings)
ratings[1:10,1:10]
ratingmat[1:10,1:10]
table(ratings$userId)
summary(ratings$userId)
unique(ratings$userId)
library(arules)
library(datasets)
# Load the data set
data(Groceries)
rm(list=ls())
library(arules)
library(datasets)
# Load the data set
data(Groceries)
Groceries
as.matric(Groceries)[1:10,1:10]
as.matrix(Groceries)[1:10,1:10]
str(Groceries)
Groceries@itemInfo
Groceries@data[,1:100]  # the first 100 baskets
image(Groceries@data[,1:100])
# Create an item frequency plot for the top 20 items
itemFrequencyPlot(Groceries,topN=20,type="absolute")
# Get the rules
rules = apriori(Groceries, parameter = list(supp = 0.001, conf = 0.8))
plot(rules)
library(arulesViz)
library(arulesViz)
plot(rules)
#plot(rules,method="graph",interactive=TRUE,shading=NA)
plot(rules[1:10],method="graph",interactive=TRUE,shading=NA)
#plot(rules,method="graph",interactive=TRUE,shading=NA)
plot(rules[1:10],method="graph",engine = 'interactive',shading=NA)
#plot(rules,method="graph",interactive=TRUE,shading=NA)
plot(rules[1:10],method="graph",engine = 'interactive',shading=NA)
rules[1]
# Show the top 5 rules, but only 2 digits
options(digits=2)
inspect(rules[1:5])
rules<-sort(rules, by="confidence", decreasing=TRUE)
inspect(rules[1:5])
# Show the top 5 rules, but only 2 digits
options(digits=3)
inspect(rules[1:5])
rules<-sort(rules, by="confidence", decreasing=TRUE)
inspect(rules[1:5])
# Show the top 5 rules, but only 2 digits
options(digits=2)
inspect(rules[1:5])
rules<-sort(rules, by="confidence", decreasing=TRUE)
inspect(rules[1:5])
rules<-apriori(data=Groceries, parameter=list(supp=0.001,conf = 0.08),
appearance = list(default="lhs",rhs="whole milk"),
control = list(verbose=F))
rules<-sort(rules, decreasing=TRUE,by="confidence")
inspect(rules[1:5])
rules<-apriori(data=Groceries, parameter=list(supp=0.001,conf = 0.08),
appearance = list(default="lhs",rhs="whole milk"),
control = list(verbose=F))
#rules<-sort(rules, decreasing=TRUE,by="confidence")
inspect(rules[1:5])
rules<-apriori(data=Groceries, parameter=list(supp=0.001,conf = 0.08),
appearance = list(default="lhs",rhs="whole milk"),
control = list(verbose=F))
rules<-sort(rules, decreasing=TRUE,by="lift")
inspect(rules[1:5])
rules = apriori(Groceries, parameter = list(supp = 0.001, conf = 0.8))
plot(rules)
# Show the top 5 rules, but only 2 digits
options(digits=2)
inspect(rules[1:5])
rules<-sort(rules, by="confidence", decreasing=TRUE)
inspect(rules[1:5])
rules<-apriori(data=Groceries, parameter=list(supp=0.001,conf = 0.08),
appearance = list(default="lhs",rhs="bottled beer"),
control = list(verbose=F))
rules<-sort(rules, decreasing=TRUE,by="confidence")
inspect(rules[1:5])
url  = 'http://tv.naver.com/cjenm.reply1988'
main = read_html(url)
library(rvest)
main = read_html(url)
temp = main
temp
install.packages('stringr')
install.packages("stringr")
library(rvest)
library(stingr)
library(stringr)
temp = main %>% html_nodes('class.title') %>% html_table(header = F, fill = T) %>% data.frame()
temp
temp = main %>% html_nodes('cds.info') %>% html_table(header = F, fill = T) %>% data.frame()
temp
temp = main %>% html_nodes('cds_info') %>% html_table(header = F, fill = T) %>% data.frame()
temp = main %>% html_nodes('.cds_info') %>% html_table(header = F, fill = T) %>% data.frame()
main = read_html(url)
temp = main %>% html_nodes('.cds_info') %>% html_table(header = F, fill = T) %>% data.frame()
temp = main %>% html_nodes('.cds_info') %>% html_nodes('.title') %>% data.frame()
temp = main %>% html_nodes('.cds_info') %>% html_nodes('.title')
temp
temp = main %>% html_nodes('.cds_info') %>% html_nodes('.title') %>% html_text()
temp
titles = str_to_title(temp)
titles
titles = str_trim(temp)
titles
library(xlsx)
data = read.xlsx('./data_set.xlsx', encoding = 'UTF-8', sheetIndex = 1)
coord = read.csv('./road_coord.csv')
coord = coord[,-1]
data = merge(data,coord, by = 'road_name')
library(ggmap)
library(RColorBrewer)
# 서울 기준 open street map 호출
SeoulMap = qmap("seoul", zoom = 11, scale = 4, maptype = "toner-lite", source= 'stamen', legend = "topleft", extent = 'device', size = c(1280,1280))
data
kdata = kmeans(data[,2:23], centers = 4)
data = cbind(data,kdata$cluster)
map1 = SeoulMap +
geom_point(aes(x = x, y = y, col = kdata$cluster), data = data, size =4) +
coord_equal()
map1
mds = cmdscale(dist(data[-19,2:23]), k = 2)
plot(mds, col = kdata$cluster)
