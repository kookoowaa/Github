b
rd1 %>% group_by(cat_tf) %>% summarize(v = sum(volume), s = sum(sales))
c = rd1 %>% group_by(cat_tf) %>% summarize(v = sum(volume), s = sum(sales))
merge(b,c)
d = merge(b,c)
d
library(ggplot2)
ggplot(data = d) +
geom_histogram()
ggplot(data = d) +
geom_histogram(aes(x = n()))
ggplot(data = d) +
geom_histogram(aes(x = v
))
ggplot(data = d) +
geom_histogram(aes(x = cat_tf, y = v))
ggplot(data = d) +
geom_histogram(aes(x = cat_tf))
ggplot(data = d) +
geom_histogram(aes(x = s))
ggplot(data = d) +
geom_bar(aes(x = cat_tf, y = s))
ggplot(data = d) +
geom_bar(aes(factor(cat_tf)))
ggplot(data = rd1) +
geom_bar(aes(factor(cat_tf)))
ggplot(data = rd1) +
geom_bar(aes(x = factor(cat_tf), y = sales))
ggplot(data = d) +
geom_bar(aes(x = cat_tf, y = s))
ggplot(data = d) +
geom_bar(aes(x = factor(cat_tf), y = s))
rd
rd1
ggplot(data = rd1) +
geom_point(aes(x = date, y = sales)) +
facet_grid(~cat_tf)
ggplot(data = rd1) +
geom_point(aes(x = date, y = sales))
rd1$date = as.Date(as.character(rd1$date), format = '%Y%m%d', origin='1974-01-01')
ggplot(data = rd1) +
geom_point(aes(x = date, y = sales))
ggplot(data = rd1) +
geom_point(aes(x = date, y = sales))+
facet_grid(~cat_tf)
ggplot(data = rd1) +
geom_point(aes(x = date, y = volume))+
facet_grid(~cat_tf)
rd
rd1
rd
rd1 = rd1[,-1]
rd1
rd1 = rd1[,-c(6:10)]
rd1
rd1 = rd1[,c(1:7,12,13)]
rd1
rd1 = read.csv('d:/cns_data/(170111)raw_data1_박찬우.csv', stringsAsFactors  = F)
rd1 = rd1[,-1]
rd1 = rd1[,-c(6:10)]
rd1 = rd1[,c(1:7,13,14)]
rd1
rd1 = read.csv('d:/cns_data/(170111)raw_data1_박찬우.csv', stringsAsFactors  = F)
rd1 = read.csv('d:/cns_data/(180115)raw_data1_박찬우.csv', stringsAsFactors  = F)
rd1
rd1 = rd1[,-1]
rd1 = rd1[,-c(6:10)]
rd1 = rd1[,c(1:7,13,14)]
rd1
rd1$date = as.Date(as.character(rd1$date), format = '%Y%m%d', origin='1974-01-01')
rd2 = read.csv('d:/cns_data/(180115)rd_dummy_박찬우.csv', stringsAsFactors  = F)
rd2
rd2[,69:70]
rd2[,69:70] %>% distinct(promo_cat1)
rd1
rd2
rd2[,c(9,69:70)]
rd2[,c(10,69:70)] %>% distinct(promo_cat1)
rd2[,c(10,69:70)]
rd2[,c(19,69:70)]
rd2[,c(20,69:70)]
rd2
rd2[,c(29,69:70)]
rdz=cbind(rd1,rd2[,c(29,69:70)])
rd2
rdz=cbind(rd1,rd2[,c(29,69:70)])
rdz=left_join(rd1,rd2[,c(29,69:70)], by = c(UPC,date))
rdz=left_join(rd1,rd2[,c(29,69:70)], by = c('UPC','date'))
rdz=left_join(rd1,rd2[,c(1,2,29,69:70)], by = c('UPC','date'))
rd2$date = as.Date(as.character(rd2$date), format = '%Y%m%d', origin='1974-01-01')
rdz=left_join(rd1,rd2[,c(1,2,29,69:70)], by = c('UPC','date'))
rdz
write.csv(rdz, 'd:/cns_data/final_file_with_factor.csv')
ggplot((rdz %>% filter(cat_tf=='마스크/팩'))) +
geom_point(aes(x = date, y = volume))+
facet_grid(~brand)
rdz %>% filter(brand =='갈아만든')
ggplot(rdz) +
geom_point(aes(x = cat_tf, y = volume))+
facet_grid(~brand)
ggplot(rdz) +
geom_point(aes(x = date, y = volume))+
facet_grid(~promo_cat2)
ggplot(rdz) +
geom_line(aes(x = date, y = volume))
ggplot(rdz) +
geom_histogram(aes(date))
ggplot(rdz) +
geom_line(aes(x = date, y = volume)) +
facet_grid(~cat_tf)
rdz
ggplot((rdz%>%filter(기타_잡화)) +
geom_line(aes(x = date, y = volume)) +
facet_grid(~promo_cat1)
ggplot((rdz%>%filter(기타_잡화))) +
ggplot((rdz%>%filter(기타_잡화))) +
geom_line(aes(x = date, y = volume)) +
facet_grid(~promo_cat1)
ggplot((rdz%>%filter(cat_tf=='기타_잡화'))) +
geom_line(aes(x = date, y = volume)) +
facet_grid(~promo_cat1)
ggplot(rdz) +
geom_point(aes(x = date, y = volume))+
facet_grid(~cat_tf)
glm.fit0 <- glm(log(volume)~.-UPC-date-sales-year_16-sale_event-promo1-promo3_10.-promo3_20.-promo3_30.-promo3_50.-promo3_70.
-promo3_discount-promo3_soldier-promo3_subs_discount-promo3_gift-promo3_royalty-promo3_addition, family = gaussian, data = rd)
rd
rd$date
rd$date/3
as.integer(rd$date)/3
as.integer(rd$date)%%3==0
rd
rd_train = rd[as.integer(rd$date)%%3==0,]
rd_test = rd[as.integer(rd$date)%%3!=0,]
rd_train
rd_test
rd[rd$date>2016-06-09,]
rd[rd$date>as.Date('2016-06-09'),]
rd_test = rd[rd$date>as.Date('2016-06-09'),]
rd_test
rd_train = rd_train[,-c(1,2,4)]
rd_train
rd_lm = lm(volume~., data = rd_train)
rd_test1 = rd_test[,-c(1,2,4)]
predict(rd_lm, newdata = rd_test1)
mutate(rd_test1, predict(rd_lm, newdata = rd_test1))
rd_lm = lm(log(volume)~., data = rd_train)
mutate(rd_test1, prediction = predict(rd_lm, newdata = rd_test1))
mutate(rd_test1, prediction = predict(rd_lm, newdata = rd_test1)) %>% select(volume, prediction)
rd_train[rd_train$volume==0, ]
rd_train[rd_train$volume==1, ]
rd_train[rd_train$volume==0, ]
rd_train[rd_train$volume==0, ]$volume
rd_train[rd_train$volume==0, ]$volume = 0.1
rd_lm = lm(log(volume)~., data = rd_train)
mutate(rd_test1, prediction = predict(rd_lm, newdata = rd_test1)) %>% select(volume, prediction)
mutate(rd_test1, prediction = exp(predict(rd_lm, newdata = rd_test1))) %>% select(volume, prediction)
mutate(rd_test1, prediction = exp(predict(rd_lm, newdata = rd_test1))) %>% select(volume, prediction) %>% summarize(sum(volume), sum(prediction))
rd_train[rd_train$volume>=0, ]
rd_train[rd_train$volume<=0, ]
rd_train[rd_train$volume<=0, ]$volume = 0.1
rd_lm = lm(log(volume)~., data = rd_train)
mutate(rd_test1, prediction = exp(predict(rd_lm, newdata = rd_test1))) %>% select(volume, prediction) %>% summarize(sum(volume), sum(prediction))
rd_train
rd_test
rd_train = rd[as.integer(rd$date)%%3==0,]
#rd_test = rd[as.integer(rd$date)%%3!=0,]
rd_test = rd[rd$date>as.Date('2016-06-09'),]
rd_train
write.csv(rdz, 'd:/cns_data/modeling_data_without_dummy.csv')
rd
nh = read.csv('d:/cns_data/national_holiday.csv')
nh
as.Date(nh[,1])
nh[,1]=as.Date(nh[,1])
nh
mutate(rd,holiday = apply(rd, 1,function(x){ifelse(x[2]%in%national.holiday,1,0)})
)
mutate(rd[1:100],holiday = apply(rd[1:100,], 1,function(x){ifelse(x[2]%in%national.holiday,1,0)}))
mutate(rd[1:100,],holiday = apply(rd[1:100,], 1,function(x){ifelse(x[2]%in%national.holiday,1,0)}))
mutate(rd[1:100,],holiday = apply(rd[1:100,], 1,function(x){ifelse(x[2]%in%nh$national.holiday,1,0)}))
nh$national.holiday
mutate(rd[1:100,],holiday = apply(rd[1:100,], 1,function(x){ifelse(x[2]%in%nh$national.holiday,1,0)})) %>% select(date, holiday)
mutate(rd[1:100,],holiday = apply(rd[1:100,], 1,function(x){ifelse(as.character(x[2]) %in% as.character(nh$national.holiday),1,0)})) %>% select(date, holiday)
rd = mutate(rd,holiday = apply(rd, 1,function(x){ifelse(as.character(x[2]) %in% as.character(nh$national.holiday),1,0)}))
rd
rdz = cbind(rdz, rd$holiday)
rdd = read.csv('d:/cns_data/(180117)modeling_data_without_dummy.csv', stringsAsFactors = F)
rdd = read.csv('d:/cns_data/modeling_data_without_dummy.csv', stringsAsFactors = F)
rdd = read.csv('d:/cns_data/modeling_data_without_dummy.csv', stringsAsFactors = F)
rd
rd = read.csv('d:/cns_data/(180117)modeling_data.csv', stringsAsFactors = F)
rd = read.csv('d:/cns_data/(180117)modeling_data.csv', stringsAsFactors = F)
rdd = read.csv('d:/cns_data/modeling_data_without_dummy.csv', stringsAsFactors = F)
nh = read.csv('d:/cns_data/national_holiday.csv', stringsAsFactors = F)
rd
rd_train = rd[as.integer(as.Date.character(rd$date))%%3==0,]
rd_test = rd[as.Date.character(rd$date)>as.Date('2016-06-09'),]
rd_train1 = rd_train[,-c(1,2,4, )]
rd_train
rd_train1 = rd_train[,-c(1,2,4, 7:9)]
rd_train1[rd_train1$volume<=0, ]$volume = 0.1
rd_lm = lm(log(volume)~., data = rd_train)
rd_lm = lm(log(volume)~., data = rd_train1)
rd_test
mutate(rd_test1, prediction = exp(predict(rd_lm, newdata = rd_test1))) %>% select(volume, prediction) %>% summarize(sum(volume), sum(prediction))
library(dplyr)
mutate(rd_test1, prediction = exp(predict(rd_lm, newdata = rd_test1))) %>% select(volume, prediction) %>% summarize(sum(volume), sum(prediction))
rd_test1 = rd_test[,-c(1,2,4)]
mutate(rd_test1, prediction = exp(predict(rd_lm, newdata = rd_test1))) %>% select(volume, prediction) %>% summarize(sum(volume), sum(prediction))
rd_test
rd_test %>% filter(UPC==32900539)
single_test = rd_test %>% filter(UPC==32900539)
single_train = rd_train %>% filter(UPC==32900539)
single_lm = lm(log(volume)~., data = single_train)
mutate(single_test, prediction = exp(predict(single_lm, newdata = single_test))) %>% select(volume, prediction) %>% summarize(sum(volume), sum(prediction))
single_lm = lm(log(volume)~.-UPC-date, data = single_train)
mutate(single_test, prediction = exp(predict(single_lm, newdata = single_test))) %>% select(volume, prediction) %>% summarize(sum(volume), sum(prediction))
single_lm
single_lm = lm(log(volume)~.-UPC-date-sales-promo1-promo2_non_pricing-promo2_pricing, data = single_train)
mutate(single_test, prediction = exp(predict(single_lm, newdata = single_test))) %>% select(volume, prediction) %>% summarize(sum(volume), sum(prediction))
single_lm
mutate(single_test, prediction = exp(predict(single_lm, newdata = single_test[,-c(1,2,4,7:9)]))) %>% select(volume, prediction) %>% summarize(sum(volume), sum(prediction))
single_test[,-c(1,2,4,7:9)]
mutate(single_test, prediction = exp(predict(single_lm, newdata = single_test[,-c(1:4,7:9)]))) %>% select(volume, prediction) %>% summarize(sum(volume), sum(prediction))
single_lm = lm(log(volume)~., data = single_train[,-c(1:4,7:9)])
single_lm = lm(log(volume)~., data = single_train[,-c(1,2,4,7:9)])
mutate(single_test, prediction = exp(predict(single_lm, newdata = single_test[,-c(1:4,7:9)]))) %>% select(volume, prediction) %>% summarize(sum(volume), sum(prediction))
18690/17121
mutate(single_test, prediction = exp(predict(single_lm, newdata = single_test[,-c(1:4,7:9)]))) %>% select(volume, prediction)
mutate(single_test, prediction = exp(predict(single_lm, newdata = single_test[,-c(1:4,7:9)]))) %>% select(volume, prediction) %>% mutate((volume-prediction)/volume)
mutate(single_test, prediction = exp(predict(single_lm, newdata = single_test[,-c(1:4,7:9)]))) %>% select(volume, prediction) %>% mutate(mape = (volume-prediction)/volume) %>% summarize(sum(mape)/nrow()*100)
mutate(single_test, prediction = exp(predict(single_lm, newdata = single_test[,-c(1:4,7:9)]))) %>% select(volume, prediction) %>% mutate(mape = (volume-prediction)/volume) %>% summarize(sum(mape)/83*100)
mutate(single_test, prediction = exp(predict(single_lm, newdata = single_test[,-c(1:4,7:9)]))) %>% select(volume, prediction) %>% mutate(mape = (volume-prediction)/volume) %>% summarize(sum(mape))
mutate(single_test, prediction = exp(predict(single_lm, newdata = single_test[,-c(1:4,7:9)]))) %>% select(volume, prediction) %>% mutate(mape = (volume-prediction)/volume) %>% summarize(sum(mape)/83)
mutate(single_test, prediction = exp(predict(single_lm, newdata = single_test[,-c(1:4,7:9)]))) %>% select(volume, prediction) %>% mutate(mape = (volume-prediction)/volume)
mutate(single_test, prediction = exp(predict(single_lm, newdata = single_test[,-c(1:4,7:9)]))) %>% select(volume, prediction) %>% mutate(mape = abs((volume-prediction))/volume) %>% summarize(sum(mape)/83)
library(glmnet)
single_ridge = cv.glmnet(x = single_test[,-1], y = single_test[,1], alpha = 0)
single_ridge = cv.glmnet(x = matrix(single_test[,-1]), y = log(single_test[,1]), alpha = 0)
matrix(single_test[,-1])
single_test[,-1]
single_ridge = cv.glmnet(x = single_test[,-1], y = log(single_test[,1]), alpha = 0)
log(single_test[,1])
single_train[,1]
single_ridge = cv.glmnet(x = single_train[,-c(1,2,4,7:9], y = log(single_train[,2]), alpha = 0)
single_ridge = cv.glmnet(x = single_train[,-c(1,2,4,7:9)], y = log(single_train[,2]), alpha = 0)
single_ridge = cv.glmnet(x = single_train[,-c(1,2,4,7:9)], y = log(single_train[,2]), alpha = 0)
single_ridge = cv.glmnet(x = single_train[,-c(1,2,4,7:9)], y = log(single_train[,4]), alpha = 0)
single_ridge = cv.glmnet(x = model.matrix(~single_train[,-c(1,2,4,7:9)]), y = log(single_train[,4]), alpha = 0)
model.matrix(~single_train[,-c(1,2,4,7:9)])
single_train[,-c(1,2,4,7:9)]
single_train[,-c(1,2,4,7:9)]
single_train
single_ridge = cv.glmnet(x = single_train[,-c(1,2,3, 4,7:9)], y = log(single_train[,3]), alpha = 0)
single_ridge = cv.glmnet(x = as.matrix(single_train[,-c(1,2,3, 4,7:9)]), y = log(single_train[,3]), alpha = 0)
single_ridge
mutate(single_test, prediction = exp(predict(single_ridge, newdata = single_test[,-c(1:4,7:9)]))) %>% select(volume, prediction) %>% mutate(mape = abs((volume-prediction))/volume) %>% summarize(sum(mape)/83)
mutate(single_test, prediction = exp(predict(single_ridge, newdata = as.matrix(single_test[,-c(1:4,7:9)])))) %>% select(volume, prediction) %>% mutate(mape = abs((volume-prediction))/volume) %>% summarize(sum(mape)/83)
mutate(single_test, prediction = exp(predict(single_ridge, newx = as.matrix(single_test[,-c(1:4,7:9)])))) %>% select(volume, prediction) %>% mutate(mape = abs((volume-prediction))/volume) %>% summarize(sum(mape)/83)
predict(single_ridge, newx = as.matrix(single_test[,-c(1:4,7:9)])))
predict(single_ridge, newx = as.matrix(single_test[,-c(1:4,7:9)]))
mutate(single_test, prediction = exp(predict(single_ridge, newx = as.matrix(single_test[,-c(1:4,7:9)])))) %>% select(volume, prediction) %>% mutate(mape = abs((volume-prediction))/volume) %>% summarize(sum(mape)/83)
exp(predict(single_ridge, newx = as.matrix(single_test[,-c(1:4,7:9)])))
mutate(single_test, prediction = unlist(exp(predict(single_ridge, newx = as.matrix(single_test[,-c(1:4,7:9)]))))) %>% select(volume, prediction) %>% mutate(mape = abs((volume-prediction))/volume) %>% summarize(sum(mape)/83)
unlist(exp(predict(single_ridge, newx = as.matrix(single_test[,-c(1:4,7:9)])))
)
unlist(exp(predict(single_ridge, newx = as.matrix(single_test[,-c(1:4,7:9)]))))
mutate(single_test, prediction = as.data.frame(exp(predict(single_ridge, newx = as.matrix(single_test[,-c(1:4,7:9)]))))) %>% select(volume, prediction) %>% mutate(mape = abs((volume-prediction))/volume) %>% summarize(sum(mape)/83)
mutate(single_test, prediction = unlist(as.data.frame(exp(predict(single_ridge, newx = as.matrix(single_test[,-c(1:4,7:9)])))))) %>% select(volume, prediction) %>% mutate(mape = abs((volume-prediction))/volume) %>% summarize(sum(mape)/83)
library(glmnet)
grid = seq(5,1, length.out = 200)
single_ridge = cv.glmnet(x = as.matrix(single_train[,-c(1,2,3, 4,7:9)]), y = log(single_train[,3]), alpha = 0, lambda = grid)
best_lambda = single_ridge$lambda.min
single_ridge = cv.glmnet(x = as.matrix(single_train[,-c(1,2,3, 4,7:9)]), y = log(single_train[,3]), alpha = 0, lambda = best_lambda)
best_lambda
single_ridge
single_ridge = cv.glmnet(x = as.matrix(single_train[,-c(1,2,3, 4,7:9)]), y = log(single_train[,3]), alpha = 0, lambda = 1)
mutate(single_test, prediction = unlist(as.data.frame(exp(predict(single_ridge, newx = as.matrix(single_test[,-c(1:4,7:9)]))), s=single_ridge$lambda.min))) %>% select(volume, prediction) %>% mutate(mape = abs((volume-prediction))/volume) %>% summarize(sum(mape)/83)
mutate(single_test, prediction = unlist(as.data.frame(exp(predict(single_ridge, newx = as.matrix(single_test[,-c(1:4,7:9)])))))) %>% select(volume, prediction) %>% mutate(mape = abs((volume-prediction))/volume) %>% summarize(sum(mape)/83)
library(glmnet)
grid = seq(5,1, length.out = 200)
single_ridge = cv.glmnet(x = as.matrix(single_train[,-c(1,2,3, 4,7:9)]), y = log(single_train[,3]), alpha = 0, lambda = grid)
best_lambda = single_ridge$lambda.min
mutate(single_test, prediction = unlist(as.data.frame(exp(predict(single_ridge, newx = as.matrix(single_test[,-c(1:4,7:9)])))))) %>% select(volume, prediction) %>% mutate(mape = abs((volume-prediction))/volume) %>% summarize(sum(mape)/83)
library(glmnet)
grid = seq(5,1, length.out = 200)
single_ridge = cv.glmnet(x = as.matrix(single_train[,-c(1,2,3, 4,7:9)]), y = log(single_train[,3]), alpha = 0, lambda = grid)
best_lambda = single_ridge$lambda.min
mutate(single_test, prediction = unlist(as.data.frame(exp(predict(single_ridge, newx = as.matrix(single_test[,-c(1:4,7:9)])))))) %>% select(volume, prediction) %>% mutate(mape = abs((volume-prediction))/volume) %>% summarize(sum(mape)/83)
library(glmnet)
grid = seq(5,1, length.out = 200)
single_ridge = cv.glmnet(x = as.matrix(single_train[,-c(1,2,3, 4,7:9)]), y = log(single_train[,3]), alpha = 0, lambda = grid)
best_lambda = single_ridge$lambda.min
mutate(single_test, prediction = unlist(as.data.frame(exp(predict(single_ridge, newx = as.matrix(single_test[,-c(1:4,7:9)])))))) %>% select(volume, prediction) %>% mutate(mape = abs((volume-prediction))/volume) %>% summarize(sum(mape)/83)
mutate(single_test, prediction = exp(predict(single_lm, newdata = single_test[,-c(1:4,7:9)]))) %>% select(volume, prediction) %>% mutate(mape = abs((volume-prediction))/volume) %>% summarize(sum(mape)/83)
library(xgboost)
xgboost(data =data.matrix(single_train[,-c(1,2,3, 4,7:9)]),
label = log(single_train[,3],
objective = "reg:linear",
eval_metric = "rmse",
max.depth =15,
eta = 0.1,
nround = 15,
subsample = 0.5,
colsample_bytree = 0.5,
num_class = 12,
nthread = 3
)
single_boost = xgboost(data =data.matrix(single_train[,-c(1,2,3, 4,7:9)]),
data.matrix(single_train[,-c(1,2,3, 4,7:9)])
log(single_train[,3]
single_boost = xgboost(data =data.matrix(single_train[,-c(1,2,3, 4,7:9)]),
single_boost = xgboost(data =data.matrix(single_train[,-c(1,2,3, 4,7:9)]),
label = log(single_train[,3]),
objective = "reg:linear",
eval_metric = "rmse",
max.depth =15,
eta = 0.1,
nround = 15,
subsample = 0.5,
colsample_bytree = 0.5,
num_class = 12,
nthread = 3
)
single_boost = xgboost(data =data.matrix(single_train[,-c(1,2,3, 4,7:9)]),
label = as.numeric(log(single_train[,3])),
objective = "reg:linear",
eval_metric = "rmse",
max.depth =15,
eta = 0.1,
nround = 15,
subsample = 0.5,
colsample_bytree = 0.5,
num_class = 12,
nthread = 3
)
single_boost = xgboost(data =data.matrix(as.numeric(single_train[,-c(1,2,3, 4,7:9)])),
label = log(single_train[,3]),
objective = "reg:linear",
eval_metric = "rmse",
max.depth =15,
eta = 0.1,
nround = 15,
subsample = 0.5,
colsample_bytree = 0.5,
num_class = 12,
nthread = 3
)
as.numeric(single_train[,-c(1,2,3, 4,7:9)])
single_train[,-c(1,2,3, 4,7:9)])
single_train[,-c(1,2,3, 4,7:9)]
single_boost = xgboost(data =data.matrix(single_train[,-c(1,2,3, 4,7:9)]),
label = log(single_train[,3]),
objective = "reg:linear",
eval_metric = "rmse",
max.depth =15,
eta = 0.1,
nround = 15,
subsample = 0.5,
colsample_bytree = 0.5,
num_class = 12,
nthread = 3
)
single_train
for(i in 1:ncol(single_train)){single_train[,i]=as.numeric(single_train[,i])}
single_boost = xgboost(data =data.matrix(single_train[,-c(1,2,3, 4,7:9)]),
label = log(single_train[,3]),
objective = "reg:linear",
eval_metric = "rmse",
max.depth =15,
eta = 0.1,
nround = 15,
subsample = 0.5,
colsample_bytree = 0.5,
num_class = 12,
nthread = 3
)
log(single_train[,3])
single_train[,-c(1,2,3, 4,7:9)]
data.matrix(single_train[,-c(1,2,3, 4,7:9)])
single_train[,-c(1,2,3, 4,7:9)]
single_boost = xgboost(data =data.matrix(single_train[,-c(1,2,3, 4,7:9)]),
label = log(single_train[,3]),
objective = "reg:linear",
eval_metric = "rmse",
max.depth =15,
eta = 0.1,
nround = 15,
subsample = 0.5,
colsample_bytree = 0.5,
num_class = 12,
nthread = 3
)
single_boost = xgboost(data =data.matrix(single_train[,-c(1,2,3, 4,7:9)]),
label = log(single_train[,3]),
objective = "reg:linear",
eval_metric = "rmse",
max.depth =4,
eta = 0.1,
nround = 1,
subsample = 0.5,
colsample_bytree = 0.5,
num_class = 12,
nthread = 3
)
library(gbm)
install.packages('gbm')
library(gclus)
library(gclus)
library(gbm)
single_train[,-c(1,2,3, 4,7:9)]
single_train[,-c(1,2, 4,7:9)]
single_boost = gbm(log(volume)~., data =single_train[,-c(1,2, 4,7:9)], distribution="gaussian",
n.trees=1000,shrinkage=0.01, interaction.depth=3)
single_boost
mutate(single_test, prediction = unlist(as.data.frame(exp(predict(single_boost, newx = single_test[,-c(1:4,7:9)]))))) %>% select(volume, prediction) %>% mutate(mape = abs((volume-prediction))/volume) %>% summarize(sum(mape)/83)
mutate(single_test, prediction = unlist(as.data.frame(exp(predict(single_boost, n.trees=3, newdata = single_test[,-c(1:4,7:9)]))))) %>% select(volume, prediction) %>% mutate(mape = abs((volume-prediction))/volume) %>% summarize(sum(mape)/83)
mutate(single_test, prediction = unlist(as.data.frame(exp(predict(single_boost, n.trees=1000, newdata = single_test[,-c(1:4,7:9)]))))) %>% select(volume, prediction) %>% mutate(mape = abs((volume-prediction))/volume) %>% summarize(sum(mape)/83)
mutate(single_test, prediction = unlist(as.data.frame(exp(predict(single_boost, n.trees=10000, newdata = single_test[,-c(1:4,7:9)]))))) %>% select(volume, prediction) %>% mutate(mape = abs((volume-prediction))/volume) %>% summarize(sum(mape)/83)
mutate(single_test, prediction = unlist(as.data.frame(exp(predict(single_boost, n.trees=1000, newdata = single_test[,-c(1:4,7:9)]))))) %>% select(volume, prediction) %>% mutate(mape = abs((volume-prediction))/volume) %>% summarize(sum(mape)/83)
mutate(single_test, prediction = unlist(as.data.frame(exp(predict(single_boost, n.trees=1000, newdata = single_test[,-c(1:4,7:9)]))))) %>% select(volume, prediction) %>% mutate(mape = abs((volume-prediction))/volume)
summary(single_boost)
mutate(single_test, prediction = unlist(as.data.frame(exp(predict(single_boost, n.trees=1000, newdata = single_test[,-c(1:4,7:9)]))))) %>% select(volume, prediction) %>% mutate(mape = abs((volume-prediction))/volume) %>% summarize(sum(mape)/83)
library(randomForest)
single_rf = randomForest(log(volume)~., data =single_train[,-c(1,2, 4,7:9)], mtry = 3, ntree = 25)
mutate(single_test, prediction = unlist(as.data.frame(exp(predict(single_rf, newdata = single_test[,-c(1:4,7:9)]))))) %>% select(volume, prediction) %>% mutate(mape = abs((volume-prediction))/volume) %>% summarize(sum(mape)/83)
mutate(rd_test1, prediction = exp(predict(rd_lm, newdata = rd_test1))) %>% select(volume, prediction)
a = mutate(rd_test1, prediction = exp(predict(rd_lm, newdata = rd_test1))) %>% select(volume, prediction) %>% mutate(PE = volume-prediction)
mutate(rd_test1, prediction = exp(predict(rd_lm, newdata = rd_test1))) %>% select(volume, prediction) %>% mutate(PE = volume-prediction)
mutate(rd_test1, prediction = exp(predict(rd_lm, newdata = rd_test1))) %>% select(volume, prediction) %>% mutate(PE = (volume-prediction)/volume)
a = mutate(rd_test1, prediction = exp(predict(rd_lm, newdata = rd_test1))) %>% select(volume, prediction) %>% mutate(PE = (volume-prediction)/volume)
sum(a$PE)/nrow(a)
sum(a$PE)
a$PE
sum(abs(a$PE))/nrow(a)
a %>% summarize(sum(PE))
a %>% filter(volume==0)
a %>% filter(volume!=0)
a = a %>% filter(volume!=0)
sum(a$PE)/nrow(a)
a = mutate(rd_test1, prediction = exp(predict(rd_lm, newdata = rd_test1))) %>% select(volume, prediction) %>% mutate(PE = (volume-prediction)/volume*100)
a = a %>% filter(volume!=0)
sum(a$PE)/nrow(a)
sum(abs(a$PE))/nrow(a)
a
sum(abs(a$PE))/nrow(a)
sum(a$PE)/nrow(a)
library(glmnet)
grid = seq(5,1, length.out = 200)
single_ridge = cv.glmnet(x = as.matrix(single_train[,-c(1,2,3, 4,7:9)]), y = log(single_train[,3]), alpha = 0, lambda = grid)
best_lambda = single_ridge$lambda.min
mutate(single_test, prediction = unlist(as.data.frame(exp(predict(single_ridge, newx = as.matrix(single_test[,-c(1:4,7:9)])))))) %>% select(volume, prediction) %>% mutate(mape = abs((volume-prediction))/volume) %>% summarize(sum(mape)/83)
library(glmnet)
grid = seq(5,1, length.out = 200)
single_ridge = cv.glmnet(x = as.matrix(single_train[,-c(1,2,3, 4,7:9)]), y = log(single_train[,3]), alpha = 0, lambda = grid)
best_lambda = single_ridge$lambda.min
mutate(single_test, prediction = unlist(as.data.frame(exp(predict(single_ridge, newx = as.matrix(single_test[,-c(1:4,7:9)])))))) %>% select(volume, prediction) %>% mutate(mape = abs((volume-prediction))/volume) %>% summarize(sum(mape)/83)
library(xgboost)
library(gclus)
library(gbm)
for(i in 1:ncol(single_train)){single_train[,i]=as.numeric(single_train[,i])}
single_boost = gbm(log(volume)~., data =single_train[,-c(1,2, 4,7:9)], distribution="gaussian",
n.trees=1000,shrinkage=0.01, interaction.depth=3)
mutate(single_test, prediction = unlist(as.data.frame(exp(predict(single_boost, n.trees=1000, newdata = single_test[,-c(1:4,7:9)]))))) %>% select(volume, prediction) %>% mutate(mape = abs((volume-prediction))/volume) %>% summarize(sum(mape)/83)
#library(xgboost)
#library(gclus)
#library(gbm)
#for(i in 1:ncol(single_train)){single_train[,i]=as.numeric(single_train[,i])}
single_boost = gbm(log(volume)~., data =single_train[,-c(1,2, 4,7:9)], distribution="gaussian",
n.trees=1000,shrinkage=0.01, interaction.depth=3)
mutate(single_test, prediction = unlist(as.data.frame(exp(predict(single_boost, n.trees=1000, newdata = single_test[,-c(1:4,7:9)]))))) %>% select(volume, prediction) %>% mutate(mape = abs((volume-prediction))/volume) %>% summarize(sum(mape)/83)
rd_train1
X <- as.matrix(rd_train1[,-1])
Y <- as.matrix(rd_train1[,1])
beta0=rep(0,ncol(X)); Sigma0inv=diag(rep(1,ncol(X))); u=1; v=1  # prior
Gibbs_REG=function(beta0, Sigma0inv, u, v, X, Y, iter, burnin, thinning){
k=ncol(X)
n=nrow(X)
XtX=t(X)%*%X
betas=matrix(0,nrow=iter,ncol=k)
taus=numeric(iter)
beta=coefficients(lm(Y~X-1)) # initial values
for(i in 1:iter){
taus[i]=tau=rgamma(1,u+n/2,v+sum((Y-X%*%beta)*(Y-X%*%beta))/2)
Sigma=solve(Sigma0inv+tau*XtX)
betas[i,]=beta=Sigma%*%(Sigma0inv%*%beta0+tau*t(X)%*%Y)+
matrix(rnorm(k)%*%chol(Sigma),ncol=1)
}
betas=betas[-(1:burnin),] # burn-in
betas=betas[1:((iter-burnin)/thinning)*thinning,] # thinning
taus=taus[-(1:burnin)]
taus=taus[1:((iter-burnin)/thinning)*thinning]
list(beta=betas,tau=taus)
}
res=Gibbs_REG(beta0,Sigma0inv,u,v,X,Y,20000,10000,10)
beta=res$beta
tau=res$tau
par(mfrow=c(2,2))
plot(beta[,1],type="l",main="Trace plot",ylab=names(data)[3])
acf(beta[,1],main="Autocorrelation function")
plot(beta[,2],type="l",main="Trace plot",ylab=names(data)[4])
acf(beta[,2],main="Autocorrelation function")
plot(beta[,3],type="l",main="Trace plot",ylab=names(data)[5])
acf(beta[,3],main="Autocorrelation function")
plot(beta[,4],type="l",main="Trace plot",ylab=names(data)[6])
acf(beta[,4],main="Autocorrelation function")
par(mfrow=c(1,2))
plot(tau,type="l",main="Trace plot",ylab="tau")
acf(tau,main="Autocorrelation function")
res
summary(res)
