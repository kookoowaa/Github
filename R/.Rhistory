paste(round(table(y_test)/sum(y_test == 1),2),
collapse = ":")))
logit = glm(RESPOND~., data = train, family = "binomial")
prob = predict(logit, test, type = 'response')
logit = glm(RESPOND~., data = train, family = "binomial")
prob = predict(logit, test, type = 'response')
set.seed(1)
train_ind = sample(1:nrow(buydata), nrow(buydata)*0.7)
train = as.data.frame(buydata[train_ind,])
test = as.data.frame(buydata[!train_ind,])
X_train = buydata[train_ind, -1]
y_train = buydata[train_ind, 1]
X_test = buydata[-train_ind, -1]
y_test = buydata[-train_ind, 1]
logit = glm(RESPOND~., data = train, family = "binomial")
prob = predict(logit, test, type = 'response')
View(test)
test = as.data.frame(buydata[-train_ind,])
prob = predict(logit, test, type = 'response')
prob[1:10]
cutoff = 0.5
ifelse(prob[1:10] > cutoff,1,0)
cutoff = 1/6
classification = function(model, newdata, cutoff){
prob = predict(model,newdata,'response')
ifelse(prob > cutoff,1,0)
}
table(test$RESPOND, classification(logit, test, 0.5))
crosstable = function(model, newdata, cutoff){
table(test$RESPOND,classification(model,newdata,cutoff))
}
crosstable(logit, test, 1/4)
table(test$RESPOND, classification(logit, test, 0.3))
cutoff_res = function(beta_hat = NULL, newx, response, cutoff, pred_prob = NULL){
if (!is.null(beta_hat)) {
X = cbind(1,as.matrix(newx))
pred_prob = 1/(1+exp(-X%*%beta_hat))}
pred = ifelse(pred_prob> cutoff, 1, 0)
error_rate = mean(response != pred)
sensitivity = sum(response == 1 & pred == 1)/sum(response == 1)
specificity = sum(response == 0 & pred == 0)/sum(response == 0)
precision = sum(response == 1 & pred == 1)/sum(pred == 1)
recall = sensitivity
if (sum(response == 1 & pred == 1) == 0) {f1 = 0}
else {f1 = 2*(precision*recall)/(precision+recall)}
cross_table = table(response, pred)
return(list(res = c(cutoff, round(error_rate,4),
round(sensitivity,4), round(specificity,4), round(f1, 4)),
cross_table = cross_table))
}
cutoff_res(logit$coefficients, X_test, y_test, 1/4)
library(ROCR)
prob = predict(logit, train, type = 'response')
library(ROCR)
AUC = performance(prediction(prob , train[,'RESPOND']) , "auc")
AUC@y.values # area under the curve
ROC = performance(prediction(prob ,y_train) , "tpr","fpr")
plot(ROC , main = paste("ROC curve for Train data\n AUC:",
round(as.numeric(AUC@y.values),4)),
col = "blue", lwd = 2.5)
abline(c(0,0), c(1,1), lty = 2, lwd = 2)
cutoff_can = seq(0.01, 0.99, by = 0.01)
cutoff_out = t(sapply(1:length(cutoff_can),
function(i) cutoff_res(logit$coefficients, X_train,
y_train, cutoff_can[i])[[1]]))
colnames(cutoff_out) = c("cutoff","error rate","sensitivity","specificity","f1 score")
cutoff_out
head(cutoff_can)
head(cutoff_out)
plot(cutoff_out$error_rate)
plot(cutoff_out$cutoff, cutoff_out$error_rate)
which.min(curoff_out$error rate)
which.min(curoff_out[,2])
which.min(cutoff_out[,2])
cutoff_out[which.min(cutoff_out[,2]),]
mean(y_train ==1)
cutoff_res(logit$coefficients, X_train, y_train,
cutoff_out[which.min(cutoff_out[,2]), 1])[[2]]
cutoff_out[tail(which(cutoff_out[,3] >= 0.5), n = 1),]
cutoff_sel = cutoff_out[tail(which(cutoff_out[,3] >= 0.5), n = 1), 1]
cutoff_res(logit$coefficients, X_train, y_train, cutoff_sel)[[2]]
which(cutoff_out[,3] >= 0.5
)
cutoff_out[which.max(cutoff_out[,5]),]
cutoff_res(logit$coefficients, X_train, y_train,
cutoff_out[which.max(cutoff_out[,5]), 1])[[2]]
cutoff = c()
cutoff[1] = cutoff_out[which.min(cutoff_out[,2]), 1]
cutoff[2] = cutoff_out[tail(which(cutoff_out[,3] >= 0.5), n = 1), 1]
cutoff[3] = cutoff_out[which.max(cutoff_out[,5]), 1]
as.data.frame(sapply(cutoff, function(cut) cutoff_res(logit$coefficients, X_test,
y_test, cut)[[1]]),
row.names = colnames(cutoff_out))
cutoff_out[which.min(cutoff_out[,2]), 1]
cutoff_out[tail(which(cutoff_out[,3] >= 0.5), n = 1), 1]
as.data.frame(sapply(cutoff, function(cut) cutoff_res(logit$coefficients, X_test, y_test, cut)[[1]]), row.names = colnames(cutoff_out))
rm(list=ls())
gc()
null = glm(RESPOND~1, data = train, family = 'binomial')
buytest = read.table('buytest.txt', header = T)
buytest[buytest$SEX == "", 'SEX'] = NA
levels(buytest$SEX)[1] = NA
buytest[buytest$ORGSRC == "", 'ORGSRC'] = NA
levels(buytest$ORGSRC)[1] = NA
buydata = buytest[,-c(1, 10,19:26)] # 사용되지 않는 변수 제거
buydata = buydata[complete.cases(buydata),] # 결측치 제거
buydata = model.matrix(~., buydata)[,-1] # 가변수 생성
set.seed(1)
train_ind = sample(1:nrow(buydata), nrow(buydata)*0.7)
train = as.data.frame(buydata[train_ind,])
test = as.data.frame(buydata[-train_ind,])
null = glm(RESPOND~1, data = train, family = 'binomial')
full = glm(RESPOND~., data = train, family = 'binomial')
forward = step(null, scope = list(lower = null, upper = full),
data = train, direction = "forward")
summary(forward)
null
library(glmnet)
ridge.fit = glmnet(X_train, as.factor(y_train), alpha = 0,
family="binomial" )
X_train = buydata[train_ind, -1]
y_train = buydata[train_ind, 1]
X_test = buydata[-train_ind, -1]
y_test = buydata[-train_ind, 1]
ridge.fit = glmnet(X_train, as.factor(y_train), alpha = 0,
family="binomial" )
ridge.fit$lambda[c(1, 10, 100)]
ridge.fit$beta[,c(1, 10, 100)]
ridge.fit$lambda
ridge.fit$beta[,c(1, 10, 100)]
set.seed(1)
cv.ridge = cv.glmnet(X_train, as.factor(y_train), alpha = 0, family = "binomial")
bestlam = cv.ridge$lambda.min
ridge.fit = glmnet(X_train,as.factor(y_train), alpha = 0, lambda = bestlam, family = "binomial")
ridge.fit$beta
ridge.fit = glmnet(X_train, as.factor(y_train), alpha = 0, lambda = bestlam, family = "binomial")
ridge.fit$beta
lasso.fit = glmnet(X_train, as.factor(y_train), alpha = 1, family="binomial")
lasso.fit$lambda[c(1, 5, 10)]
lasso.fit$beta[,c(1, 5, 10)]
set.seed(1)
cv.lasso = cv.glmnet(X_train, as.factor(y_train), alpha = 1,
family="binomial")
bestlam = cv.lasso$lambda.min
lasso.fit = glmnet(X_train, as.factor(y_train), alpha = 1,
lambda = bestlam, family="binomial")
lasso.fit$beta
beta_hat = logit$coefficients
#beta_hat = logit$coefficients
#------------------------------------
# forward selection
#------------------------------------
tmp = forward$coefficients
beta_forward = c()
for (i in 1:length(beta_hat)){
if (names(beta_hat)[i] %in% names(tmp)) beta_forward[i] = tmp[names(beta_hat)[i]]
else beta_forward[i] = 0
}
beta_hat = none$coefficients
beta_hat = null$coefficients
#------------------------------------
# forward selection
#------------------------------------
tmp = forward$coefficients
beta_forward = c()
for (i in 1:length(beta_hat)){
if (names(beta_hat)[i] %in% names(tmp)) beta_forward[i] = tmp[names(beta_hat)[i]]
else beta_forward[i] = 0
}
beta_hat = cbind(beta_hat, beta_forward)
#------------------------------------
# Ridge & LASSO
#------------------------------------
beta_hat = cbind(beta_hat, c(ridge.fit$a0, as.vector(ridge.fit$beta)),
c(lasso.fit$a0, as.vector(lasso.fit$beta)))
logit = glm(RESPOND~., data = train, family = "binomial")
beta_hat = logit$coefficients
#------------------------------------
# forward selection
#------------------------------------
tmp = forward$coefficients
beta_forward = c()
for (i in 1:length(beta_hat)){
if (names(beta_hat)[i] %in% names(tmp)) beta_forward[i] = tmp[names(beta_hat)[i]]
else beta_forward[i] = 0
}
beta_hat = cbind(beta_hat, beta_forward)
#------------------------------------
# Ridge & LASSO
#------------------------------------
beta_hat = cbind(beta_hat, c(ridge.fit$a0, as.vector(ridge.fit$beta)),
c(lasso.fit$a0, as.vector(lasso.fit$beta)))
library(ROCR)
auc_res = function(beta = NULL, newx, newy, pred_prob = NULL){
if (!is.null(beta)){
X = cbind(1,as.matrix(newx))
pred_prob = 1/(1+exp(-X%*%beta)) }
AUC = performance(prediction(pred_prob , newy) , "auc")
return(AUC@y.values[[1]])
}
auc_table = rbind(sapply(1:4, function(i) auc_res(beta_hat[,i], X_train, y_train)),
sapply(1:4, function(i) auc_res(beta_hat[,i], X_test, y_test)))
rownames(auc_table) = c('Training set', 'Test set')
colnames(auc_table) = model_names = c('Logistic','Logistic+AIC', 'Ridge', 'LASSO')
auc_table
set.seed(5)
cv.ridge = cv.glmnet(X_train, as.factor(y_train), alpha = 0, family = "binomial")
bestlam = cv.ridge$lambda.min
ridge.fit = glmnet(X_train, as.factor(y_train), alpha = 0, lambda = bestlam, family = "binomial")
ridge.fit$beta
set.seed(5)
cv.lasso = cv.glmnet(X_train, as.factor(y_train), alpha = 1, family="binomial")
bestlam = cv.lasso$lambda.min
lasso.fit = glmnet(X_train, as.factor(y_train), alpha = 1, lambda = bestlam, family="binomial")
lasso.fit$beta
library(ROCR)
auc_res = function(beta = NULL, newx, newy, pred_prob = NULL){
if (!is.null(beta)){
X = cbind(1,as.matrix(newx))
pred_prob = 1/(1+exp(-X%*%beta)) }
AUC = performance(prediction(pred_prob , newy) , "auc")
return(AUC@y.values[[1]])
}
auc_table = rbind(sapply(1:4, function(i) auc_res(beta_hat[,i], X_train, y_train)),
sapply(1:4, function(i) auc_res(beta_hat[,i], X_test, y_test)))
rownames(auc_table) = c('Training set', 'Test set')
colnames(auc_table) = model_names = c('Logistic','Logistic+AIC', 'Ridge', 'LASSO')
auc_table
set.seed(1)
cv.lasso = cv.glmnet(X_train, as.factor(y_train), alpha = 1, family="binomial")
bestlam = cv.lasso$lambda.min
lasso.fit = glmnet(X_train, as.factor(y_train), alpha = 1, lambda = bestlam, family="binomial")
lasso.fit$beta
set.seed(1)
cv.ridge = cv.glmnet(X_train, as.factor(y_train), alpha = 0, family = "binomial")
bestlam = cv.ridge$lambda.min
ridge.fit = glmnet(X_train, as.factor(y_train), alpha = 0, lambda = bestlam, family = "binomial")
ridge.fit$beta
library(ROCR)
auc_res = function(beta = NULL, newx, newy, pred_prob = NULL){
if (!is.null(beta)){
X = cbind(1,as.matrix(newx))
pred_prob = 1/(1+exp(-X%*%beta)) }
AUC = performance(prediction(pred_prob , newy) , "auc")
return(AUC@y.values[[1]])
}
auc_table = rbind(sapply(1:4, function(i) auc_res(beta_hat[,i], X_train, y_train)),
sapply(1:4, function(i) auc_res(beta_hat[,i], X_test, y_test)))
rownames(auc_table) = c('Training set', 'Test set')
colnames(auc_table) = model_names = c('Logistic','Logistic+AIC', 'Ridge', 'LASSO')
auc_table
cut_sel = matrix(0, nrow = 4, ncol = 3)
for (i in 1:4){
cutoff_out = t(sapply(1:length(cutoff_can),
function(j) cutoff_res(beta_hat[,i], X_train,
y_train, cutoff_can[j])[[1]]))
cut_sel[i, 1] = cutoff_out[which.min(cutoff_out[,2]), 1]
cut_sel[i, 2] = cutoff_out[tail(which(cutoff_out[,3] >= 0.5), n = 1), 1]
cut_sel[i, 3] = cutoff_out[which.max(cutoff_out[,5]), 1]
}
cut_sel = matrix(0, nrow = 4, ncol = 3)
matrix(t(sapply(1:3, function(i) cutoff_res(beta_hat[,i], X_test,
y_test, cut_sel[i, 1])[[1]])),
nrow = 3,
dimnames =list(model_names, c("error rate","sensitivity",
"specificity","f1 score")))
cut_sel = matrix(0, nrow = 3, ncol = 3)
matrix(t(sapply(1:3, function(i) cutoff_res(beta_hat[,i], X_test,
y_test, cut_sel[i, 1])[[1]])),
nrow = 3,
dimnames =list(model_names, c("error rate","sensitivity",
"specificity","f1 score")))
cut_sel = matrix(0, nrow = 3, ncol = 3)
matrix(t(sapply(1:3, function(i) , X_test, y_test, cut_sel[i, 1])[[1]])),
cut_sel = matrix(0, nrow = 3, ncol = 3)
matrix(t(sapply(1:3, function(i) X_test, y_test, cut_sel[i, 1])[[1]])),
cut_sel = matrix(0, nrow = 3, ncol = 3)
matrix(t(sapply(1:3, function(i) X_test, y_test, cut_sel[i, 1])[[1]]),
nrow = 3,
dimnames =list(model_names, c("error rate","sensitivity",
"specificity","f1 score")))
cut_sel = matrix(0, nrow = 3, ncol = 3)
matrix(t(sapply(1:3, function(i) X_test, y_test, cut_sel[i, 1])[[1]]),
nrow = 3,
dimnames =list(model_names, c("error rate","sensitivity",
"specificity","f1 score")))
cut_sel = matrix(0, nrow = 3, ncol = 3)
matrix(t(sapply(1:3, function(i) cutoff_res(beta_hat[,i], X_test,
y_test, cut_sel[i, 1])[[1]])),
nrow = 3,
dimnames =list(model_names, c("error rate","sensitivity",
"specificity","f1 score")))
cutoff_res = function(beta_hat = NULL, newx, response, cutoff, pred_prob = NULL){
if (!is.null(beta_hat)) {
X = cbind(1,as.matrix(newx))
pred_prob = 1/(1+exp(-X%*%beta_hat))}
cut_sel = matrix(0, nrow = 3, ncol = 3)
matrix(t(sapply(1:3, function(i) cutoff_res(X_test,
y_test, cut_sel[i, 1])[[1]])),
nrow = 3,
dimnames =list(model_names, c("error rate","sensitivity",
"specificity","f1 score")))
cutoff_res = function(beta_hat = NULL, newx, response, cutoff, pred_prob = NULL){
if (!is.null(beta_hat)) {
X = cbind(1,as.matrix(newx))
pred_prob = 1/(1+exp(-X%*%beta_hat))}
pred = ifelse(pred_prob> cutoff, 1, 0)
error_rate = mean(response != pred)
sensitivity = sum(response == 1 & pred == 1)/sum(response == 1)
specificity = sum(response == 0 & pred == 0)/sum(response == 0)
precision = sum(response == 1 & pred == 1)/sum(pred == 1)
recall = sensitivity
if (sum(response == 1 & pred == 1) == 0) {f1 = 0}
else {f1 = 2*(precision*recall)/(precision+recall)}
cross_table = table(response, pred)
return(list(res = c(cutoff, round(error_rate,4),
round(sensitivity,4), round(specificity,4), round(f1, 4)),
cross_table = cross_table))
}
cut_sel = matrix(0, nrow = 3, ncol = 3)
matrix(t(sapply(1:3, function(i) cutoff_res(X_test,
y_test, cut_sel[i, 1])[[1]])),
nrow = 3,
dimnames =list(model_names, c("error rate","sensitivity",
"specificity","f1 score")))
cut_sel = matrix(0, nrow = 3, ncol = 3)
matrix(t(sapply(1:3, function(i) cutoff_res(X_test,
y_test, cut_sel[i, 1])[[1]])),
nrow = 3,
dimnames =list(model_names, c("error rate","sensitivity",
"specificity","f1 score")))
matrix(t(sapply(1:3, function(i) cutoff_res(X_test,
y_test, cut_sel[i, 1])[[1]])),
nrow = 3,
dimnames =list(model_names, c("error rate","sensitivity",
"specificity","f1 score")))
cutoff_res = function(beta_hat = NULL, newx, response, cutoff, pred_prob = NULL){
if (!is.null(beta_hat)) {
X = cbind(1,as.matrix(newx))
pred_prob = 1/(1+exp(-X%*%beta_hat))}
pred = ifelse(pred_prob> cutoff, 1, 0)
error_rate = mean(response != pred)
sensitivity = sum(response == 1 & pred == 1)/sum(response == 1)
specificity = sum(response == 0 & pred == 0)/sum(response == 0)
precision = sum(response == 1 & pred == 1)/sum(pred == 1)
recall = sensitivity
if (sum(response == 1 & pred == 1) == 0) {f1 = 0}
else {f1 = 2*(precision*recall)/(precision+recall)}
cross_table = table(response, pred)
return(list(res = c(cutoff, round(error_rate,4),
round(sensitivity,4), round(specificity,4), round(f1, 4)),
cross_table = cross_table))
}
cut_sel = matrix(0, nrow = 3, ncol = 3)
matrix(t(sapply(1:3, function(i) cutoff_res(X_test,
y_test, cut_sel[i, 1])[[1]])),
nrow = 3,
dimnames =list(model_names, c("error rate","sensitivity",
"specificity","f1 score")))
cut_sel = matrix(0, nrow = 3, ncol = 3)
matrix(t(sapply(1:4, function(i) cutoff_res(X_test,
y_test, cut_sel[i, 1])[[1]])),
nrow = 4,
dimnames =list(model_names, c("cutoff","error rate","sensitivity", "specificity","f1 score")))
cutoff_res(X_test,
y_test, cut_sel[i, 1])
cut_sel = matrix(0, nrow = 4, ncol = 3)
for (i in 1:4){
cutoff_out = t(sapply(1:length(cutoff_can),
function(j) cutoff_res(beta_hat[,i], X_train,
y_train, cutoff_can[j])[[1]]))
cut_sel[i, 1] = cutoff_out[which.min(cutoff_out[,2]), 1]
cut_sel[i, 2] = cutoff_out[tail(which(cutoff_out[,3] >= 0.5), n = 1), 1]
cut_sel[i, 3] = cutoff_out[which.max(cutoff_out[,5]), 1]
}
cutoff_can = seq(0.01, 0.99, by = 0.01)
for (i in 1:4){
cutoff_out = t(sapply(1:length(cutoff_can),
function(j) cutoff_res(beta_hat[,i], X_train,
y_train, cutoff_can[j])[[1]]))
cut_sel[i, 1] = cutoff_out[which.min(cutoff_out[,2]), 1]
cut_sel[i, 2] = cutoff_out[tail(which(cutoff_out[,3] >= 0.5), n = 1), 1]
cut_sel[i, 3] = cutoff_out[which.max(cutoff_out[,5]), 1]
}
for (i in 1:4){
cutoff_out = t(sapply(1:length(cutoff_can),
function(j) cutoff_res(beta_hat[,i], X_train,
y_train, cutoff_can[j])[[1]]))
cut_sel[i, 1] = cutoff_out[which.min(cutoff_out[,2]), 1]
cut_sel[i, 2] = cutoff_out[tail(which(cutoff_out[,3] >= 0.5), n = 1), 1]
cut_sel[i, 3] = cutoff_out[which.max(cutoff_out[,5]), 1]
}
matrix(t(sapply(1:4, function(i) cutoff_res(beta_hat[,i], X_test,
y_test, cut_sel[i, 1])[[1]])),
nrow = 4,
dimnames =list(model_names, c("cutoff","error rate","sensitivity",
"specificity","f1 score")))
getwd()
wsdata0 = read.csv('Wholesale.csv')
wsdata1 = wsdata0[,3:8]
mydata = scale(wsdata1)
# Determine number of clusters
wss = (nrow(mydata)-1)*sum(apply(mydata,2,var))   # total SS
bigK = 20
for (i in 2:bigK) wss[i] = sum(kmeans(mydata, centers=i)$withinss)   # for k=2:K, compute within SS
plot(1:bigK, wss, type="b", xlab="Number of Clusters (k)",   ylab="Within-Group Sum of Squares")
title("Looking for elbow....")
mydata
head(mydata)
wss
nrow(mydata)
(nrow(mydata)-1)*sum(apply(mydata,2,var))   # total SS
sum(apply(mydata,2,var)
)
sum(apply(mydata,2,var))
(nrow(mydata)-1)*sum(apply(mydata,2,var))   # total SS
wss
wss = (nrow(mydata)-1)*sum(apply(mydata,2,var))   # total SS
wss
for (i in 2:bigK) wss[i] = sum(kmeans(mydata, centers=i)$withinss)   # for k=2:K, compute within SS
wss
plot(1:bigK, wss, type="b", xlab="Number of Clusters (k)",   ylab="Within-Group Sum of Squares")
title("Looking for elbow....")
kmeans(mydata, 5)$size
# let us fix k=5
#fit = kmeans(mydata, 5) # 5 cluster solution
kmeans(mydata, 5)$size
#> fit$size
#[1]  98   1  10 279  52
#fit = kmeans(mydata, 4) # 4 cluster solution
kmeans(mydata, 4)$size
#> fit$size
#[1] 328  43  68   1
#fit = kmeans(mydata, 3) # 3 cluster solution
kmeans(mydata, 3)$size
#> fit$size
#[1]  49  44 347
fit = kmeans(mydata,3)
# let us fix k=5
#fit = kmeans(mydata, 5) # 5 cluster solution
kmeans(mydata, 5)$size
#> fit$size
#[1]  98   1  10 279  52
#fit = kmeans(mydata, 4) # 4 cluster solution
kmeans(mydata, 4)$size
#> fit$size
#[1] 328  43  68   1
#fit = kmeans(mydata, 3) # 3 cluster solution
kmeans(mydata, 3)$size
#> fit$size
#[1]  49  44 347
fit = kmeans(mydata,3)
aggregate(mydata,by=list(fit$cluster),FUN=mean)  # get cluster means
kmclust.output = data.frame(mydata, fit$cluster)  # append cluster assignment
# Cluster Plot against 1st 2 principal components
library(cluster)
clusplot(mydata, fit$cluster, color=TRUE, shade=TRUE, labels=2, lines=0)
fit$cluster
mydata
kmclust.output
aggregate(mydata,by=list(fit$cluster),FUN=mean)  # get cluster means
aggregate(mydata,by=list(fit$cluster),FUN=mean)  # get cluster means
kmclust.output = data.frame(mydata, fit$cluster)  # append cluster assignment
# Cluster Plot against 1st 2 principal components
library(cluster)
clusplot(mydata, fit$cluster, color=TRUE, shade=TRUE, labels=2, lines=0)
d <- dist(mydata, method = "euclidean") # distance matrix
fit <- hclust(d, method="complete")
plot(fit) # display dendogram
# draw dendogram with red borders around the 5 clusters
rect.hclust(fit, k=3, border="red")
groups <- cutree(fit, k=3) # cut tree into 5 clusters
table(groups)
#groups
#  1   2   3
#429  10   1
# looks weird. then let us use ward method
fit <- hclust(d, method="ward")
plot(fit) # display dendogram
# draw dendogram with red borders around the 5 clusters
rect.hclust(fit, k=3, border="red")
groups <- cutree(fit, k=3) # cut tree into 5 clusters
table(groups)
clusplot(mydata, groups, color=TRUE, shade=TRUE, labels=2, lines=0)
library(mclust)
fit = Mclust(mydata)
plot(fit) # plot results
library(mclust)
fit = Mclust(mydata)
library(mclust)
fit = Mclust(mydata)
plot(fit) # plot results
library(mclust)
fit = Mclust(mydata)
plot(fit) # plot results
library(mclust)
fit = Mclust(mydata)
plot(fit) # plot results
summary(fit)
summary(fit)
clusplot(mydata, fit$classification, color=TRUE, shade=TRUE, labels=2, lines=0)
fit3 = Mclust(mydata, G=3)
summary(fit3) # display the 3-component model
#Mclust VVV (ellipsoidal, varying volume, shape, and orientation) model with 3 components:
#
# log.likelihood   n df      BIC       ICL
#      -1526.219 440 83 -3557.64 -3593.981
#
#Clustering table:
#  1   2   3
#189 210  41
clusplot(mydata, fit3$classification, color=TRUE, shade=TRUE, labels=2, lines=0)
