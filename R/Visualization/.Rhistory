Histogram.3day3 = ggplot(data = festival.data.stack, aes( x = score, y =..density..)) + geom_histogram(binwidth = 0.4, color = 'black', fill = 'yellow') + labs( x = 'score', y = 'density')
Histogram.3day3
Histogram.3day3 + facet_grid(~gender)
Histogram.3day3 + facet_grid(gender~day)
scatterplot_fest = ggplot ( data = festival.data.stack, aes(x = gender, y = score, fill = gender))+ geom_point(position = 'jitter') + facet_grid(~day)
scatterplot_fest = ggplot ( data = festival.data.stack, aes(x = gender, y = score, fill = gender))+ geom_point(position = 'jitter') + facet_grid(~day)
scatterplot_fest = ggplot ( data = festival.data.stack, aes(x = gender, y = score, fill = gender))+ geom_point(position = 'jitter') + facet_grid(~day)
scatterplot_fest
scatterplot_fest = ggplot ( data = festival.data.stack, aes(x = gender, y = score, color = gender))+ geom_point(position = 'jitter') + facet_grid(~day)
scatterplot_fest
scatterplot_fest + scale_color_brewer()
scatterplot_fest + scale_color_manual(values = c('darkorange', 'darkorchid4))
scatterplot_fest + scale_color_manual(values = c('darkorange', 'darkorchid4'))
scatterplot_fest + geom_boxplot( alpha = 0.1, color = 'black', fill = 'orange')
scatterplot_fest + geom_boxplot( alpha = 0.9, color = 'black', fill = 'orange')
scatterplot_fest + geom_boxplot( alpha = 0.5, color = 'black', fill = 'orange')
library(maps)
load(file="storms.RData")
library(maps)
load(file="./exercise/wk6/storms.RData")
library(maps)
load(file="./data/wk6/storms.RData")
wm = map_data("world")
substorms = storms %>% filter(Season %in% 1999:2010) %>%
filter(!is.na(Season)) %>%
filter(Name!="NOT NAMED")
substorms$ID = as.factor(paste(substorms$Name,
substorms$Season, sep = "."))
substorms$Name = as.factor(substorms$Name)
map1 = ggplot(substorms,
aes(x = Longitude, y = Latitude, group = ID)) +
geom_polygon(data = wm,
aes(x = long, y = lat, group = group),
fill = "gray25", colour = "gray10", size = 0.2) +
geom_path(data = substorms,
aes(group = ID, colour = Wind.WMO.),
alpha = 0.5, size = 0.8) +
xlim(-138, -20) + ylim(3, 55) +
labs(x = "", y = "", colour = "Wind \n(knots)")
map1
wm
substrom
substorm
substorms
map_data('korea')
map_data(')?
?map_data(')
?map_data()
map1 = ggplot(substorms, aes(x = Longitude, y = Latitude, group = ID)) + geom_polygon(data = wm, aes(x = long, y = lat, group = group),
fill = "gray25", colour = "gray10", size = 0.2) +
geom_path(data = substorms,
aes(group = ID, colour = Wind.WMO.),
alpha = 0.5, size = 0.8) +
xlim(-138, -20) + ylim(3, 55) +
labs(x = "", y = "", colour = "Wind \n(knots)")
map1
wm
kr.map = map_data[region == 'South Korea',]
kr.map = map_data(region == 'South Korea')
kr.map = map_data(map, region == 'South Korea')
kr.map = map_data('South Korea')
kr.map = map_data('South Korea')
map_data('South Korea')
map_data('Korea')
map_data()
map_data('world')
kr.map = wm[region=='South Korea',]
kr.map = wm[wm$region=='South Korea',]
kr.map
ggplot(data = kr.map, x = long, y = lat)
ggplot(data = kr.map, x = long, y = lat) + geom_polygon()
ggplot(data = kr.map, x = long, y = lat) + geom_polygon(x = long, y = lat)
ggplot(data = kr.map, x = long, y = lat) + geom_polygon(x = long, y = lat, data = kr.map)
kr.map
ggplot(data = kr.map, x = long, y = lat) + geom_polygon(aes(x = long, y = lat))
kr.map = wm[wm$region=='South Korea',]
ggplot(data = kr.map, x = long, y = lat) + geom_polygon(aes(x = long, y = lat, group = group))
kr.map = wm[wm$region=='South Korea',]
ggplot(data = kr.map, x = long, y = lat) + geom_line(aes(x = long, y = lat, group = group))
map2 = ggplot(substorms,
aes(x = Longitude, y = Latitude, group = ID)) +
geom_polygon(data = wm,
aes(x = long, y = lat, group = group),
fill = "gray25", colour = "gray10", size = 0.2) +
geom_path(data = substorms,
aes(group = ID, colour = Wind.WMO.), size = 0.5) +
xlim(-138, -20) + ylim(3, 55) +
labs(x = "", y = "", colour = "Wind \n(knots)") +
facet_wrap(~Month)
map2 = ggplot(substorms,
aes(x = Longitude, y = Latitude, group = ID)) +
geom_polygon(data = wm,
aes(x = long, y = lat, group = group),
fill = "gray25", colour = "gray10", size = 0.2) +
geom_path(data = substorms,
aes(group = ID, colour = Wind.WMO.), size = 0.5) +
xlim(-138, -20) + ylim(3, 55) +
labs(x = "", y = "", colour = "Wind \n(knots)") +
facet_wrap(~Month)
map2
map1 + facet_wrap(~Month)
par(bg="white")
set.seed(1)
a=seq(1:100) + 0.1*seq(1:100)*sample(c(1:10) , 100 , replace=T)
b=seq(1:100) + 0.2*seq(1:100)*sample(c(1:10) , 100 , replace=T)
size = 3 +(a/30) + rnorm(length(a))
d = (b/300) + rnorm(length(a),0, 0.1)
d[d<0] = 0
rdata<- data.frame(x = a, y = b, size = size, temp = d)
myplot <- ggplot(data = rdata, aes ( x = x, y = y)) +
geom_point(aes(x,y, colour = temp), size = size) +
scale_color_gradient2(midpoint = 0.5, low="#EF5500",
mid="#FFFF77", high="blue")
myplot
map1 = ggplot(substorms, aes(x = Longitude, y = Latitude, group = ID)) + geom_polygon(data = wm, aes(x = long, y = lat, group = group),
fill = "gray25", colour = "gray10", size = 0.2) +
geom_path(data = substorms, aes(group = ID, colour = Wind.WMO.),alpha = 0.5, size = 0.8) +
xlim(-138, -20) + ylim(3, 55) +
labs(x = "", y = "", colour = "Wind \n(knots)")
map1
gc()
rm(list=ls())
dev.off()
library(map)
library(maps)
library(mapdata)
install.packages('mapdata')
install.packages('mapdata')
install.packages('mapdata')
library(maps)
library(mapdata)
map(database = 'country')
map(database = 'usa')
map(database = 'country')
map(database = 'cunty')
map(database = 'county')
map(database = 'county')
map(database = 'world', region = 'south korea')
map('world2Hires', 'South Korea')
kr.map = ggplot2::map_data('world2hires','south korea')
kr.map = ggplot2::map_data('world2hires','South Korea')
kr.map = ggplot2::map_data('World2Hires','South Korea')
map_data('World2Hires','South Korea')
kr.map = ggplot2::mapdata('World2Hires','South Korea')
data("us.cities")
head(us.cities)
ggplot2::map_data('World2Hires', 'South Korea')
map('state', 'Georgia')
map('state', 'Georgia')
map.cities(us.cities, country = GA)
map('state', 'Georgia')
map.cities(us.cities, country = 'GA')
map('world', fill = T, col = rainbow(100))
map('world', fill = T)
map('world', fill = T, color = rainbow)
map('world', fill = T, color = rainbow(7))
map('world', fill = T, col = rainbow(7))
map_data('world')
map_data('world2Hires', 'South Korea')
map_data('world')[region]
map_data('world')$region
length(unique(map_data('world')$region))
map('world', fill = T, col = rainbow(252))
length(unique(rainbow(252)))
length(unique(map_data('world')$group))
map('world', fill = T, col = rainbow(1627))
data(unemp) # unemployed rate data
data(county.fips) # county fips data
head(unemp,3)
head(county.fips)
unemp$colorBuckets <- as.numeric(cut(unemp$unemp,
c(0, 2, 4, 6, 8, 10, 100)))
colorsmatched <- unemp$colorBuckets[match(county.fips$fips, unemp$fips)]
unemp$colorBuckets
unemp
colorsmatched
unemp$colorBuckets <- as.numeric(cut(unemp$unemp, c(0, 2, 4, 6, 8, 10, 100)))
colorsmatched <- unemp$colorBuckets[match(county.fips$fips, unemp$fips)]
colors = c("#F1EEF6","#D4B9DA","#C994C7","#DF65B0","#DD1C77","#980043")
map("county", col = colors[colorsmatched], fill = TRUE, resolution = 0, lty = 0, projection = "polyconic")
unemp$colorBuckets <- as.numeric(cut(unemp$unemp, c(0, 2, 4, 6, 8, 10, 100)))
colorsmatched <- unemp$colorBuckets[match(county.fips$fips, unemp$fips)]
colors = c("#F1EEF6","#D4B9DA","#C994C7","#DF65B0","#DD1C77","#980043")
map("county", col = unemp$colorBuckets, fill = TRUE, resolution = 0, lty = 0, projection = "polyconic")
colors[colorsmat
map("county", col = colors[colorsmatched], fill = TRUE, resolution = 0, lty = 0, projection = "polyconic")
unemp$colorBuckets <- as.numeric(cut(unemp$unemp, c(0, 2, 4, 6, 8, 10, 100)))
colorsmatched <- unemp$colorBuckets[match(county.fips$fips, unemp$fips)]
colors = c("#F1EEF6","#D4B9DA","#C994C7","#DF65B0","#DD1C77","#980043")
map("county", col = colors[colorsmatched], fill = TRUE, resolution = 0, lty = 0, projection = "polyconic")
map_data('county')
colorsmatched[1]
county.fips[1]
county.fips[1,]
unemp$colorBuckets <- as.numeric(cut(unemp$unemp, c(0, 2, 4, 6, 8, 10, 100)))
colorsmatched <- unemp$colorBuckets[match(county.fips$fips, unemp$fips)]
colors = c("#F1EEF6","#D4B9DA","#C994C7","#DF65B0","#DD1C77","#980043")
map("county", col = colors[colorsmatched], fill = TRUE, resolution = 0, lty = 0)
colorsmatched[1]
county.fips[1,]
umemp$fips[1,]
unemp$colorBuckets <- as.numeric(cut(unemp$unemp, c(0, 2, 4, 6, 8, 10, 100)))
colorsmatched <- unemp$colorBuckets[match(county.fips$fips, unemp$fips)]
colors = c("#F1EEF6","#D4B9DA","#C994C7","#DF65B0","#DD1C77","#980043")
map("county", col = colors[colorsmatched], fill = TRUE, resolution = 0, lty = 0)
colorsmatched[1]
county.fips[1,]
umemp$fips[1,]
colorsmatched[1]
county.fips[1,]
unemp$fips[1,]
unemp$colorBuckets <- as.numeric(cut(unemp$unemp, c(0, 2, 4, 6, 8, 10, 100)))
colorsmatched <- unemp$colorBuckets[match(county.fips$fips, unemp$fips)]
colors = c("#F1EEF6","#D4B9DA","#C994C7","#DF65B0","#DD1C77","#980043")
map("county", col = colors[colorsmatched], fill = TRUE, resolution = 0, lty = 0)
colorsmatched[1]
county.fips$fips[1,]
unemp$colorBuckets <- as.numeric(cut(unemp$unemp, c(0, 2, 4, 6, 8, 10, 100)))
colorsmatched <- unemp$colorBuckets[match(county.fips$fips, unemp$fips)]
colors = c("#F1EEF6","#D4B9DA","#C994C7","#DF65B0","#DD1C77","#980043")
map("county", col = colors[colorsmatched], fill = TRUE, resolution = 0, lty = 0)
colorsmatched[1]
county.fips[1,]
unemp[1,]
colorsmatched[1]
county.fips[1,]
unemp[1,]
unemp$colorBuckets <- as.numeric(cut(unemp$unemp, c(0, 2, 4, 6, 8, 10, 100)))
colorsmatched <- unemp$colorBuckets[match(county.fips$fips, unemp$fips)]
library(mapproj)
colors = c("#F1EEF6","#D4B9DA","#C994C7","#DF65B0","#DD1C77","#980043")
map("county", col = colors[colorsmatched], fill = TRUE, resolution = 0, lty = 0, projection = 'polyconic')
#colorsmatched[1]
#county.fips[1,]
#unemp[1,]
wm = map_data('world2hires')
wm = map_data('World2Hires')
wm = ggplot2::map_data('World2Hires')
map_data('World2Hires')
wm = map_data('World')
wm = map_data('world')
wm
map('world2hires', regions = 'South Korea')
map('World2Hires', regions = 'South Korea')
map('WorldHires', regions = 'South Korea')
grep( "Korea", ur$region )
ur <- wm %>% dplyr::select(region)%>%unique()
grep( "Korea", ur$region )
map("world", ur$region[c(125,185)],fill = T,
col = "blue")
map("worldhires", ur$region[c(125,185)],fill = T, col = "blue")
map("world2hires", ur$region[c(125,185)],fill = T, col = "blue")
library(KoNLP)
library(httr)
library(rvest)
library(dplyr)
#Sys.getlocale()
Sys.setlocale("LC_ALL", "Korean")
client_id = 'pNeL9M2Busi7vWn4XkW6';
client_secret = 'nTY9Mj5v2K';
header = httr::add_headers(
'X-Naver-Client-Id' = client_id,
'X-Naver-Client-Secret' = client_secret)
query.n = query = '허니버터칩'
# encoding 변화
query = iconv(query, to = 'UTF-8', toRaw = T)
# iconv(query, to = "UTF-8", toRaw = F)
query = paste0('%', paste(unlist(query), collapse = '%'))
query = toupper(query)
end_num = 1000
display_num = 100
start_point = seq(1, end_num, by = display_num)
i = 1
url = paste0('https://openapi.naver.com/v1/search/blog.xml?query=',query,'&display=',display_num,'&start=',start_point[i],'&sort=sim')
url_body = read_xml(GET(url, header))
title = url_body %>% xml_nodes('item title') %>%
xml_text()
bloggername = url_body %>%
xml_nodes('item bloggername') %>% xml_text()
postdate = url_body %>% xml_nodes('postdate') %>%
xml_text()
link = url_body %>% xml_nodes('item link') %>%
xml_text()
description = url_body %>% xml_nodes('item description') %>%
html_text()
final_dat = NULL
for(i in 1:length(start_point))
{
# request xml format
url = paste0('https://openapi.naver.com/v1/search/blog.xml?query=',query,'&display=',display_num,'&start=',start_point[i],'&sort=sim')
#option header
url_body = read_xml(GET(url, header), encoding = "UTF-8")
title = url_body %>% xml_nodes('item title') %>% xml_text()
bloggername = url_body %>% xml_nodes('item bloggername') %>% xml_text()
postdate = url_body %>% xml_nodes('postdate') %>% xml_text()
link = url_body %>% xml_nodes('item link') %>% xml_text()
description = url_body %>% xml_nodes('item description') %>% html_text()
temp_dat = cbind(title, bloggername, postdate, link, description)
final_dat = rbind(final_dat, temp_dat)
cat(i, '\n')
}
final_dat = data.frame(final_dat, stringsAsFactors = F)
# 상위 블로거 검색
#library(dplyr)
tb = final_dat %>% select(bloggername) %>% table()
top_blogger = sort(tb, decreasing = T)[1:4]
tmp = final_dat %>% select(bloggername, title, link) %>% filter(bloggername %in% names(top_blogger))
#검색량 분ㅍ
library(pspline)
tb = final_dat %>% select(postdate) %>% table()
x <-as.Date(names(tb), format = "%Y%m%d")
y <- as.numeric(tb)
# as.Date("2019[02[03", format = '%Y[%m[%d')
fit <- sm.spline(x = as.integer(x), y = y, cv = TRUE)
plot(x, y, pch = 19, cex = 0.5)
lines(x=x, y=fit$ysmth, lty = 2, col = 'blue')
#기간 동안 검색량 분석 02
xx <- as.Date(min(x):max(x),origin = "1970-01-01")
yy = rep(0, length(xx))
yy[xx%in%x] = y
fit<-sm.spline(xx,yy,cv = TRUE)
plot(xx, yy, pch = 19, cex = 0.5)
points(fit$x, fit$ysmth, type = 'l', lty = 2, lwd = 1.5, col = 'blue')
#국소다항회귀 (local polynomial regression)
xint = as.integer(xx)
rdata = data.frame(y = yy, x = xint)
fit<-loess(y~x,data = rdata, span = 0.5, normalize = FALSE)
plot(fit, pch = 19, cex = 0.5)
points(fit$x,fit$fitted, type = 'l', lty = 2, lwd = 1.5, col = 'blue')
#Span 값에 따라 주변(%) 데이터를 참조 (i.e. k-means)
#fit<-loess(y~x,data = rdata, span = 0.1, normalize = FALSE)
#plot(fit, pch = 19, cex = 0.5)
#points(fit$x,fit$fitted, type = 'l', lty = 2, lwd = 1.5, col = 'blue')
# K fold cross validation
k.fold = 5
idx <-sample(1:5, length(xint), replace = TRUE)
k = 1
rdata.tr <- rdata[idx != k, ]
rdata.va <- rdata[idx == k, ]
fit<-loess(y~x,data = rdata.tr, span = 0.1, normalize = FALSE)
fit.y<-predict(fit, newdata = rdata.va)
mean((fit.y-rdata.va$y)^2, na.rm = T)
# K fold cross validation 2 - loop
k.fold = 5
idx <-sample(1:k.fold, length(xint), replace = TRUE)
span.var <- seq(0.02, 0.5, by  = 0.01)
valid.mat <- NULL
for (j in 1:length(span.var))
{
valid.err <- c()
for (k in 1:k.fold)
{
rdata.tr <- rdata[idx != k, ]
rdata.va <- rdata[idx == k, ]
fit<-loess(y~x,data = rdata.tr,
span = span.var[j], normalize = FALSE)
fit.y<-predict(fit, newdata = rdata.va)
valid.err[k] <- mean((fit.y-rdata.va$y)^2, na.rm = T)
}
valid.mat <- cbind(valid.mat, valid.err)
}
valid.mat
boxplot(valid.mat)
lines(colMeans(valid.mat), col = "blue", lty = 2)
# K fold cross validation - model selection
span.par<- span.var[which.min(colMeans(valid.mat))]
fit<-loess(y~x,data = rdata,
span = span.par, normalize = FALSE)
plot(xx,yy,  pch = 19, cex = 0.5)
points(xx,fit$fitted, type = 'l', lty = 2, lwd = 1.5, col = 'blue')
# 단어 정제
final_dat[10,5]
a = gsub(pattern = "<[/?A-Za-z]*>", replace = "", final_dat[10,5])
a
dat_tmp <- final_dat
for (i in 1:nrow(final_dat))
{
dat_tmp[i,5]<- gsub(pattern = "<[/|A-Za-z]*>",  replace = "", final_dat[i,5])
dat_tmp[i,1]<- gsub(pattern = "<[/|A-Za-z]*>",  replace = "", final_dat[i,1])
}
#library(KoNLP)
#useSejongDic()
extractNoun(a)
library(tm)
text = dat_tmp[,5]
cps = Corpus(VectorSource(text))
dtm = tm::DocumentTermMatrix(cps, control = list(tokenize = extractNoun, removeNumber = T, removePunctuation = T))
str(dtm)
# sparse matrix(i = 행, j = 열, v = 값)
# Sparse Matrix 원리 참조
library(Matrix)
T3 <- spMatrix(3,4, i=c(1,3:1), j=c(2,4:2), x=1:4)
T3
rmat <- as.matrix(dtm) # 데이터가 작은 경우 매트릭스로 변경
str(rmat)
rmat <-spMatrix(dtm$nrow,dtm$ncol, i=dtm$i, j=dtm$j, x=dtm$v) #데이터가 클 경우 sparse matrix 사
head(rmat)
#Matrix 관련 연산자 사용 가능
#library(rvest)
wcount<-colSums(rmat)
wname <- dtm$dimnames$Terms
wname <- rvest::repair_encoding(dtm$dimnames$Terms)
colnames(rmat)<- wname
str(wname)
sort.var <- sort(wcount,decreasing = T)[100]
idx <- !( grepl(query.n, wname)| (wcount<=sort.var) )
wname.rel <- wname[idx]
wcount.rel <- wcount[idx]
library(wordcloud)
pal <- brewer.pal(9, "Set1")
wordcloud(wname.rel,freq = wcount.rel, colors = pal)
#상관분석
bb <- rmat
bb.freq <- sort(colSums(bb), decreasing = T)
plot(log(bb.freq), pch = 19, type = 'l')
bb.freq <- bb.freq[bb.freq>quantile(bb.freq,0.99)]
idx <- match(names(bb.freq), colnames(bb))
bb.r <- bb[,idx]
dim(bb.r)
bb.r <- as.matrix(bb.r)
cor.mat <- cor(bb.r)
image(cor.mat, col =terrain.colors(100))
sort(cor.mat[1,], decreasing = T)[1:10]
dat_tmp[,5]
dat_tmp
library(KoNLP)
useSejongDic()
extractNoun(a)
read.table('./snack_proj/twitter_text.txt')
read('./snack_proj/twitter_text.txt')
open('./snack_proj/twitter_text.txt')
b = open('./snack_proj/twitter_text.txt')
b = readLines('./snack_proj/twitter_text.txt')
b
b = readLines('./snack_proj/twitter_text.txt', encoding = 'utf-8')
b
b = readLines('./snack_proj/twitter_text.txt', encoding = 'utf-8')
b
i (for i in b):
for (i in b):
i.decoding('utf-8')}
for (i in b){
i.decoding('utf-8')
}
i.decode('utf-8')
b = readLines('./snack_proj/twitter_text.txt', encoding = 'UTF-8')
b
extractNoun(b)
dtm
cps
cps = Corpus(VectorSource(b))
dtm = tm::DocumentTermMatrix(cps, control = list(tokenize = extractNoun, removeNumber = T, removePunctuation = T))
Sys.getlocale()
library(tm)
cps = Corpus(VectorSource(b))
dtm = tm::DocumentTermMatrix(cps, control = list(tokenize = extractNoun, removeNumber = T, removePunctuation = T))
b
b = readLines('./snack_proj/twitter_text.txt', encoding = 'UTF-8')
library(KoNLP)
library(tm)
useSejongDic()
cps = Corpus(VectorSource(b))
dtm = tm::DocumentTermMatrix(cps, control = list(tokenize = extractNoun, removeNumber = T, removePunctuation = T))
head(b)
cps = Corpus(VectorSource(b))
dtm = tm::DocumentTermMatrix(cps, control = list(tokenize = extractNoun, removeNumber = T, removePunctuation = T))
install.packages('Unicode')
library(KoNLP)
library(tm)
library(Unicode)
useSejongDic()
cps = Corpus(VectorSource(b))
dtm = tm::DocumentTermMatrix(cps, control = list(tokenize = extractNoun, removeNumber = T, removePunctuation = T))
install.packages('twitterR')
install.packages('twitteR')
library(KoNLP)
library(tm)
library(Unicode)
library(twitteR)
useSejongDic()
cps = Corpus(VectorSource(b))
dtm = tm::DocumentTermMatrix(cps, control = list(tokenize = extractNoun, removeNumber = T, removePunctuation = T))
library(KoNLP)
library(tm)
library(Unicode)
library(twitteR)
library(wordcloud)
library(plyr)
useSejongDic()
cps = Corpus(VectorSource(b))
dtm = tm::DocumentTermMatrix(cps, control = list(tokenize = extractNoun, removeNumber = T, removePunctuation = T))
result_nouns <- Map(extractNoun, result.text)
c = read('./snack_proj/twitter_text.txt', encoding = 'UTF-8')
c = open('./snack_proj/twitter_text.txt', encoding = 'UTF-8')
head(b)
readline('./snack_proj/twitter_text.txt', encoding = 'UTF-8')
readline('./snack_proj/twitter_text.txt')
b = readLines('./snack_proj/twitter_text.txt', encoding = 'UTF-8')
head(b)
library(KoNLP)
library(tm)
library(Unicode)
library(twitteR)
library(wordcloud)
library(plyr)
useSejongDic()
cps = Corpus(VectorSource(b))
dtm = tm::DocumentTermMatrix(cps, control = list(tokenize = extractNoun, removeNumber = T, removePunctuation = T))
cps
dtm = tm::DocumentTermMatrix(b, control = list(tokenize = extractNoun, removeNumber = T, removePunctuation = T))
Corpus(VectorSource(b))
VectorSource(b)
