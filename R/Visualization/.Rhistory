final_dat[,5]
grep('포카칩',final_dat[,5])
final_dat[,5](grep('포카칩',final_dat[,5]))
final_dat[,5][grep('포카칩',final_dat[,5])]
wname[wcount==17]
wname[wcount==16]
wname[wcount==15]
wname[wcount==14]
wname[wcount==13]
wname[wcount==12]
wname[wcount==11]
wname[wcount==9]
wname[wcount==7]
wname[wcount==6]
wname[wcount==5]
library(KoNLP)
library(httr)
library(rvest)
library(dplyr)
#Sys.getlocale()
Sys.setlocale("LC_ALL", "Korean")
client_id = 'pNeL9M2Busi7vWn4XkW6';
client_secret = 'nTY9Mj5v2K';
header = httr::add_headers(
'X-Naver-Client-Id' = client_id,
'X-Naver-Client-Secret' = client_secret)
query.n = query = '과자'
# encoding 변화
query = iconv(query, to = 'UTF-8', toRaw = T)
# iconv(query, to = "UTF-8", toRaw = F)
query = paste0('%', paste(unlist(query), collapse = '%'))
query = toupper(query)
end_num = 1000
display_num = 100
start_point = seq(1, end_num, by = display_num)
i = 1
url = paste0('https://openapi.naver.com/v1/search/blog.xml?query=',query,'&display=',display_num,'&start=',start_point[i],'&sort=date')
url_body = read_xml(GET(url, header))
title = url_body %>% xml_nodes('item title') %>%
xml_text()
bloggername = url_body %>%
xml_nodes('item bloggername') %>% xml_text()
postdate = url_body %>% xml_nodes('postdate') %>%
xml_text()
link = url_body %>% xml_nodes('item link') %>%
xml_text()
description = url_body %>% xml_nodes('item description') %>%
html_text()
final_dat = NULL
for(i in 1:length(start_point))
{
# request xml format
url = paste0('https://openapi.naver.com/v1/search/blog.xml?query=',query,'&display=',display_num,'&start=',start_point[i],'&sort=sim')
#option header
url_body = read_xml(GET(url, header), encoding = "UTF-8")
title = url_body %>% xml_nodes('item title') %>% xml_text()
bloggername = url_body %>% xml_nodes('item bloggername') %>% xml_text()
postdate = url_body %>% xml_nodes('postdate') %>% xml_text()
link = url_body %>% xml_nodes('item link') %>% xml_text()
description = url_body %>% xml_nodes('item description') %>% html_text()
temp_dat = cbind(title, bloggername, postdate, link, description)
final_dat = rbind(final_dat, temp_dat)
cat(i, '\n')
}
final_dat = data.frame(final_dat, stringsAsFactors = F)
# 상위 블로거 검색
#library(dplyr)
tb = final_dat %>% select(bloggername) %>% table()
top_blogger = sort(tb, decreasing = T)[1:4]
tmp = final_dat %>% select(bloggername, title, link) %>% filter(bloggername %in% names(top_blogger))
#검색량 분ㅍ
library(pspline)
tb = final_dat %>% select(postdate) %>% table()
x <-as.Date(names(tb), format = "%Y%m%d")
y <- as.numeric(tb)
# as.Date("2019[02[03", format = '%Y[%m[%d')
fit <- sm.spline(x = as.integer(x), y = y, cv = TRUE)
plot(x, y, pch = 19, cex = 0.5)
lines(x=x, y=fit$ysmth, lty = 2, col = 'blue')
#기간 동안 검색량 분석 02
xx <- as.Date(min(x):max(x),origin = "1970-01-01")
yy = rep(0, length(xx))
yy[xx%in%x] = y
fit<-sm.spline(xx,yy,cv = TRUE)
plot(xx, yy, pch = 19, cex = 0.5)
points(fit$x, fit$ysmth, type = 'l', lty = 2, lwd = 1.5, col = 'blue')
#국소다항회귀 (local polynomial regression)
xint = as.integer(xx)
rdata = data.frame(y = yy, x = xint)
fit<-loess(y~x,data = rdata, span = 0.5, normalize = FALSE)
plot(fit, pch = 19, cex = 0.5)
points(fit$x,fit$fitted, type = 'l', lty = 2, lwd = 1.5, col = 'blue')
#Span 값에 따라 주변(%) 데이터를 참조 (i.e. k-means)
#fit<-loess(y~x,data = rdata, span = 0.1, normalize = FALSE)
#plot(fit, pch = 19, cex = 0.5)
#points(fit$x,fit$fitted, type = 'l', lty = 2, lwd = 1.5, col = 'blue')
# K fold cross validation
k.fold = 5
idx <-sample(1:5, length(xint), replace = TRUE)
k = 1
rdata.tr <- rdata[idx != k, ]
rdata.va <- rdata[idx == k, ]
fit<-loess(y~x,data = rdata.tr, span = 0.1, normalize = FALSE)
fit.y<-predict(fit, newdata = rdata.va)
mean((fit.y-rdata.va$y)^2, na.rm = T)
# K fold cross validation 2 - loop
k.fold = 5
idx <-sample(1:k.fold, length(xint), replace = TRUE)
span.var <- seq(0.02, 0.5, by  = 0.01)
valid.mat <- NULL
for (j in 1:length(span.var))
{
valid.err <- c()
for (k in 1:k.fold)
{
rdata.tr <- rdata[idx != k, ]
rdata.va <- rdata[idx == k, ]
fit<-loess(y~x,data = rdata.tr,
span = span.var[j], normalize = FALSE)
fit.y<-predict(fit, newdata = rdata.va)
valid.err[k] <- mean((fit.y-rdata.va$y)^2, na.rm = T)
}
valid.mat <- cbind(valid.mat, valid.err)
}
valid.mat
boxplot(valid.mat)
lines(colMeans(valid.mat), col = "blue", lty = 2)
# K fold cross validation - model selection
span.par<- span.var[which.min(colMeans(valid.mat))]
fit<-loess(y~x,data = rdata,
span = span.par, normalize = FALSE)
plot(xx,yy,  pch = 19, cex = 0.5)
points(xx,fit$fitted, type = 'l', lty = 2, lwd = 1.5, col = 'blue')
# 단어 정제
final_dat[10,5]
a = gsub(pattern = "<[/?A-Za-z]*>", replace = "", final_dat[10,5])
a
dat_tmp <- final_dat
for (i in 1:nrow(final_dat))
{
dat_tmp[i,5]<- gsub(pattern = "<[/|A-Za-z]*>",  replace = "", final_dat[i,5])
dat_tmp[i,1]<- gsub(pattern = "<[/|A-Za-z]*>",  replace = "", final_dat[i,1])
}
#library(KoNLP)
#useSejongDic()
extractNoun(a)
library(tm)
text = dat_tmp[,5]
cps = Corpus(VectorSource(text))
dtm = tm::DocumentTermMatrix(cps, control = list(tokenize = extractNoun, removeNumber = T, removePunctuation = T))
str(dtm)
# sparse matrix(i = 행, j = 열, v = 값)
# Sparse Matrix 원리 참조
library(Matrix)
T3 <- spMatrix(3,4, i=c(1,3:1), j=c(2,4:2), x=1:4)
T3
rmat <- as.matrix(dtm) # 데이터가 작은 경우 매트릭스로 변경
str(rmat)
rmat <-spMatrix(dtm$nrow,dtm$ncol, i=dtm$i, j=dtm$j, x=dtm$v) #데이터가 클 경우 sparse matrix 사
head(rmat)
#Matrix 관련 연산자 사용 가능
#library(rvest)
wcount<-colSums(rmat)
wname <- dtm$dimnames$Terms
wname <- rvest::repair_encoding(dtm$dimnames$Terms)
colnames(rmat)<- wname
str(wname)
sort.var <- sort(wcount,decreasing = T)[100]
idx <- !( grepl(query.n, wname)| (wcount<sort.var)|grepl('좋아하는', wname)|grepl('제일', wname)|grepl('제가', wname)|grepl('과자', wname))
wname.rel <- wname[idx]
wcount.rel <- wcount[idx]
library(wordcloud)
pal <- brewer.pal(9, "Set1")
wordcloud(wname.rel,freq = wcount.rel, colors = pal)
wname[wcount==5]
final_dat[,5][grep('포카칩',final_dat[,5])]
#포카칩, 빼빼로, 새우깡,웨하스, 꼬깔콘(10)| 홈런볼, 프링글스, 콘칩,오감자, 몽쉘, 마가렛트, 프레첼, 닭다리, 구운양파, 치토스, 오사쯔, 다이제, 양파링, 꿀꽈배기, 빈츠, 카라멜콘(5)
#상관분석
bb <- rmat
bb.freq <- sort(colSums(bb), decreasing = T)
plot(log(bb.freq), pch = 19, type = 'l')
bb.freq <- bb.freq[bb.freq>quantile(bb.freq,0.99)]
idx <- match(names(bb.freq), colnames(bb))
bb.r <- bb[,idx]
dim(bb.r)
bb.r <- as.matrix(bb.r)
cor.mat <- cor(bb.r)
image(cor.mat, col =terrain.colors(100))
sort(cor.mat[1,], decreasing = T)[1:10]
final_dat[,5][grep('꼬북칩',final_dat[,5])]
final_dat[,5][grep('아이배냇',final_dat[,5])]
final_dat[,5][grep('일본',final_dat[,5])]
final_dat[,5][grep('있는',final_dat[,5])]
final_dat[,5][grep('있는',final_dat[,5])]
sort.var <- sort(wcount,decreasing = T)[100]
idx <- !( grepl(query.n, wname)| (wcount<sort.var)|grepl('좋아하는', wname)|grepl('제일', wname)|grepl('제가', wname)|grepl('과자', wname) | length(wname)==1)
wname.rel <- wname[idx]
wcount.rel <- wcount[idx]
library(wordcloud)
pal <- brewer.pal(9, "Set1")
wordcloud(wname.rel,freq = wcount.rel, colors = pal)
wname[length(wname)==1]
wname
length(wname)==1
wname[length(wname)[]==1]
length(wname)[]==1
wname%>%length()==1
wname%>%length()
for(i in wname){length(i)}
for(i in wname){print(length(i))}
wname[1]
for (i in length(wname)){length(wname[i])}
for (i in length(wname)){print(length(wname[i]))}
length(wname)
length(wname[1])
length(wname[2])
wname[2]
length('애플')
str(wname[1])
strlen(wname[1])
nchar(wname[1])
for (i in length(wname)){print(nchar(wname[i]))}
wname[nchar(wname)==1]
sort.var <- sort(wcount,decreasing = T)[100]
idx <- !( grepl(query.n, wname)| (wcount<sort.var)|grepl('좋아하는', wname)|grepl('제일', wname)|grepl('제가', wname)|grepl('과자', wname) | nchar(wname)==1)
wname.rel <- wname[idx]
wcount.rel <- wcount[idx]
library(wordcloud)
pal <- brewer.pal(9, "Set1")
wordcloud(wname.rel,freq = wcount.rel, colors = pal)
a = gsub(pattern = "<[/?A-Za-z]*>", replace = "", final_dat[1,5])
a
a = gsub(pattern = "<[/?A-Za-z]*>", replace = "", final_dat[20,5])
a
a = gsub(pattern = "<[/?A-Za-z]*>", replace = "", final_dat[1:30,5])
a
a = gsub(pattern = "<[/?A-Za-z]*>", replace = "", final_dat[30,5])
a
b = gsub(pattern = '[ㅎ]', replace = '', a)
b
a = gsub(pattern = c("<[/?A-Za-z]*>", '[ㅎ]'), replace = "", final_dat[30,5])
a
a = gsub(pattern = "<[/?A-Za-z]*>"| '[ㅎ]', replace = "", final_dat[30,5])
dat_tmp[4,1]
dat_tmp
dat_tmp <- final_dat
for (i in 1:nrow(final_dat))
{
dat_tmp[i,5]<- gsub(pattern = '[ㅋㅎ]', replace='', gsub(pattern = "<[/|A-Za-z]*>",  replace = "", final_dat[i,5]))
dat_tmp[i,1]<- gsub(pattern = '[ㅋㅎ]', replace='',  gsub(pattern = "<[/|A-Za-z]*>",  replace = "", final_dat[i,1]))
}
library(tm)
text = dat_tmp[,5]
cps = Corpus(VectorSource(text))
dtm = tm::DocumentTermMatrix(cps, control = list(tokenize = extractNoun, removeNumber = T, removePunctuation = T))
str(dtm)
# sparse matrix(i = 행, j = 열, v = 값)
# Sparse Matrix 원리 참조
library(Matrix)
T3 <- spMatrix(3,4, i=c(1,3:1), j=c(2,4:2), x=1:4)
T3
rmat <- as.matrix(dtm) # 데이터가 작은 경우 매트릭스로 변경
str(rmat)
rmat <-spMatrix(dtm$nrow,dtm$ncol, i=dtm$i, j=dtm$j, x=dtm$v) #데이터가 클 경우 sparse matrix 사
head(rmat)
#Matrix 관련 연산자 사용 가능
#library(rvest)
wcount<-colSums(rmat)
wname <- dtm$dimnames$Terms
wname <- rvest::repair_encoding(dtm$dimnames$Terms)
colnames(rmat)<- wname
str(wname)
sort.var <- sort(wcount,decreasing = T)[100]
idx <- !( grepl(query.n, wname)| (wcount<sort.var)|grepl('좋아하는', wname)|grepl('제일', wname)|grepl('제가', wname)|grepl('과자', wname) | nchar(wname)==1)
wname.rel <- wname[idx]
wcount.rel <- wcount[idx]
library(wordcloud)
pal <- brewer.pal(9, "Set1")
wordcloud(wname.rel,freq = wcount.rel, colors = pal)
unlink('snack_proj/test1_cache', recursive = TRUE)
knit_with_parameters('D:/Github/R/Visualization/snack_proj/test1.Rmd', encoding = 'UTF-8')
unlink('snack_proj/test1_cache', recursive = TRUE)
sort.var <- sort(wcount,decreasing = T)[100]
idx <- !( grepl('과자', wname) | (wcount<=sort.var) |grepl('먹고', wname) |nchar(wname)==1)
wname.rel <- wname[idx]
wcount.rel <- wcount[idx]
wname[wcount>12]
nchar(wname)==1
idx
b = readLines('./snack_proj/twitter_text_modified.txt', encoding = 'UTF-8')
b = readLines('./snack_proj/twitter_text_modified.txt', encoding = 'UTF-8')
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())
b = readLines('./snack_proj/twitter_text_modified.txt', encoding = 'UTF-8')
gc()
rm(list=ls())
dev.off()
b = readLines('./snack_proj/twitter_text_modified.txt', encoding = 'UTF-8')
getwd()
b = readLines('./twitter_text_modified.txt', encoding = 'UTF-8')
b = readLines('./twitter_text_modified.txt', encoding = 'UTF-8')
b = unique(b)
for(i in b){
i = gsub(pattern = '[ㅋㅎ]', replace='', gsub(pattern = "\\[a-z0-9]*>", replace = "", i))
}
#b[1300:1400]
library(KoNLP)
library(wordcloud)
cps = Corpus(VectorSource(b))
sort.var <- sort(wcount,decreasing = T)[100]
library(tm)
cps = Corpus(VectorSource(b))
dtm = tm::DocumentTermMatrix(cps, control = list(tokenize = extractNoun, removeNumber = T, removePunctuation = T))
str(dtm)
library(Matrix)
rmat <-spMatrix(dtm$nrow,dtm$ncol, i=dtm$i, j=dtm$j, x=dtm$v) #데이터가 클 경우 sparse matrix 사
head(rmat)
wcount<-colSums(rmat)
wname <- dtm$dimnames$Terms
wname <- rvest::repair_encoding(dtm$dimnames$Terms)
colnames(rmat)<- wname
str(wname)
sort.var <- sort(wcount,decreasing = T)[100]
idx <- !( grepl('과자', wname) | (wcount<=sort.var) |grepl('먹고', wname) |nchar(wname)==1)
wname.rel <- wname[idx]
wcount.rel <- wcount[idx]
library(wordcloud)
pal <- brewer.pal(9, "Set1")
wordcloud(wname.rel,freq = wcount.rel, colors = pal)
b = readLines('./twitter_text_modified.txt', encoding = 'UTF-8')
b = unique(b)
for(i in b){
i = gsub(pattern = '[ㅋㅎ]*', replace='', gsub(pattern = "\\[a-z0-9]*>", replace = "", i))
}
#b[1300:1400]
cps = Corpus(VectorSource(b))
dtm = tm::DocumentTermMatrix(cps, control = list(tokenize = extractNoun, removeNumber = T, removePunctuation = T))
str(dtm)
library(Matrix)
rmat <-spMatrix(dtm$nrow,dtm$ncol, i=dtm$i, j=dtm$j, x=dtm$v) #데이터가 클 경우 sparse matrix 사
head(rmat)
wcount<-colSums(rmat)
wname <- dtm$dimnames$Terms
wname <- rvest::repair_encoding(dtm$dimnames$Terms)
colnames(rmat)<- wname
str(wname)
sort.var <- sort(wcount,decreasing = T)[100]
idx <- !( grepl('과자', wname) | (wcount<=sort.var) |grepl('먹고', wname) |nchar(wname)==1)
wname.rel <- wname[idx]
wcount.rel <- wcount[idx]
library(wordcloud)
pal <- brewer.pal(9, "Set1")
wordcloud(wname.rel,freq = wcount.rel, colors = pal)
b
b[22]
gsub(pattern = '[ㅋㅎ]*', replace='', b[22])
gsub(pattern = '[ㅋㅎ]*', replace='', gsub(pattern = "\\[a-z0-9]*>", replace = "", b[22]))
b = readLines('./twitter_text_modified.txt', encoding = 'UTF-8')
b = unique(b)
for(i in b){
i = gsub(pattern = '[ㅋㅎ]*', replace='', gsub(pattern = "\\[a-z0-9]*>", replace = "", i))
}
b
b = readLines('./twitter_text_modified.txt', encoding = 'UTF-8')
b = unique(b)
for(i in b){
i = gsub(pattern = '[ㅋㅎㅠㄱㅜ]*', replace='', gsub(pattern = "\\[a-z0-9]*>", replace = "", i))
cat(i)
}
b
b = readLines('./twitter_text_modified.txt', encoding = 'UTF-8')
b = unique(b)
for(i in b){
gsub(pattern = '[ㅋㅎㅠㄱㅜ]*', replace='', gsub(pattern = "\\[a-z0-9]*>", replace = "", i))
}
b
b = readLines('./twitter_text_modified.txt', encoding = 'UTF-8')
b = unique(b)
for(i in length(b)){
b[i] = gsub(pattern = '[ㅋㅎㅠㄱㅜ]*', replace='', gsub(pattern = "\\[a-z0-9]*>", replace = "", b[i]))
}
b
b[22] = gsub(pattern = '[ㅋㅎㅠㄱㅜ]*', replace='', gsub(pattern = "\\[a-z0-9]*>", replace = "", b[22]))
b[22]
length(b)
b = readLines('./twitter_text_modified.txt', encoding = 'UTF-8')
b = unique(b)
for(i in b){
i = gsub(pattern = '[ㅋㅎㅠㄱㅜ]*', replace='', gsub(pattern = "\\[a-z0-9]*>", replace = "", i))
}
b[22]
b = readLines('./twitter_text_modified.txt', encoding = 'UTF-8')
b = unique(b)
b
b = readLines('./twitter_text_modified.txt', encoding = 'UTF-8')
b = unique(b)
for(i in b){
i = gsub(pattern = '[ㅋㅎㅠㄱㅜ]*', replace='', i)
}
b[22]
b = readLines('./snack_proj/naver_text.txt', encoding = 'UTF-8')
b = unique(b)
for(i in b){
i <- gsub(pattern = "\\[a-z0-9]*>", replace = "", i)
}
#b[1300:1400]
b = readLines('./twitter_text_modified.txt', encoding = 'UTF-8')
b = unique(b)
for(i in b){
i <- gsub(pattern = '[ㅋㅎㅠㄱㅜ]*', replace='', i)
}
b[22]
b = gsub(pattern = '[ㅋㅎㅠㄱㅜ]*', replace='', b)
b
cps = Corpus(VectorSource(b))
dtm = tm::DocumentTermMatrix(cps, control = list(tokenize = extractNoun, removeNumber = T, removePunctuation = T))
str(dtm)
library(Matrix)
rmat <-spMatrix(dtm$nrow,dtm$ncol, i=dtm$i, j=dtm$j, x=dtm$v) #데이터가 클 경우 sparse matrix 사
head(rmat)
wcount<-colSums(rmat)
wname <- dtm$dimnames$Terms
wname <- rvest::repair_encoding(dtm$dimnames$Terms)
colnames(rmat)<- wname
str(wname)
sort.var <- sort(wcount,decreasing = T)[100]
idx <- !( grepl('과자', wname) | (wcount<=sort.var) |grepl('먹고', wname) |nchar(wname)==1)
wname.rel <- wname[idx]
wcount.rel <- wcount[idx]
library(wordcloud)
pal <- brewer.pal(9, "Set1")
wordcloud(wname.rel,freq = wcount.rel, colors = pal)
sort.var <- sort(wcount,decreasing = T)[150]
idx <- !( grepl('과자', wname) | (wcount<=sort.var)  |nchar(wname)==1)
wname.rel <- wname[idx]
wcount.rel <- wcount[idx]
library(wordcloud)
pal <- brewer.pal(9, "Set1")
wordcloud(wname.rel,freq = wcount.rel, colors = pal)
sort.var <- sort(wcount,decreasing = T)[150]
idx <- !( grepl(c('과자','먹고','내가','너무'), wname) | (wcount<=sort.var)  |nchar(wname)==1)
wname.rel <- wname[idx]
wcount.rel <- wcount[idx]
sort.var <- sort(wcount,decreasing = T)[150]
idx <- !( grepl('과자', wname) | (wcount<=sort.var)  |nchar(wname)==1 | grepl('내가', wname) | grepl('먹고', wname) | grepl('너', wname) )
wname.rel <- wname[idx]
wcount.rel <- wcount[idx]
sort.var <- sort(wcount,decreasing = T)[150]
idx <- !( grepl('과자', wname) | (wcount<=sort.var)  |nchar(wname)==1 | grepl('내가', wname) | grepl('먹고', wname) | grepl('너무', wname) )
wname.rel <- wname[idx]
wcount.rel <- wcount[idx]
library(wordcloud)
pal <- brewer.pal(9, "Set1")
wordcloud(wname.rel,freq = wcount.rel, colors = pal)
cars
cars
b
knitr::opts_chunk$set(echo = TRUE)
library(KoNLP)
library(tm)
library(wordcloud)
library(Matrix)
twitter_txt = readLines('./twitter_text_modified.txt', encoding = 'UTF-8')
twitter_txt = unique(twitter_txt)
twitter_txt = gsub(pattern = '[ㅋㅎㅠㄱㅜ]*', replace='', twitter_txt)
knitr::opts_chunk$set(echo = TRUE)
twitter_txt = readLines('./twitter_text_modified.txt', encoding = 'UTF-8')
twitter_txt = unique(twitter_txt)
twitter_txt = gsub(pattern = '[ㅋㅎㅠㄱㅜ]*', replace='', twitter_txt)
cps = Corpus(VectorSource(b))
dtm = tm::DocumentTermMatrix(cps, control = list(tokenize = extractNoun, removeNumber = T, removePunctuation = T))
rmat <-spMatrix(dtm$nrow,dtm$ncol, i=dtm$i, j=dtm$j, x=dtm$v)
wcount<-colSums(rmat)
wname <- dtm$dimnames$Terms
wname <- rvest::repair_encoding(dtm$dimnames$Terms)
colnames(rmat)<- wname
# 판별 기준 부여
sort.var <- sort(wcount,decreasing = T)[150]
idx <- !( grepl('과자', wname) | (wcount<=sort.var)  |nchar(wname)==1 | grepl('내가', wname) | grepl('먹고', wname) | grepl('너무', wname) )
wname.rel <- wname[idx]
wcount.rel <- wcount[idx]
pal <- brewer.pal(9, "Set1")
wordcloud(wname.rel,freq = wcount.rel, colors = pal)
?Corpus
cps = Corpus(VectorSource(b))
dtm = DocumentTermMatrix(cps, control = list(tokenize = extractNoun, removeNumber = T, removePunctuation = T))
rmat <-spMatrix(dtm$nrow,dtm$ncol, i=dtm$i, j=dtm$j, x=dtm$v)
wcount<-colSums(rmat)
wname <- dtm$dimnames$Terms
wname <- rvest::repair_encoding(dtm$dimnames$Terms)
colnames(rmat)<- wname
cps = tm.Corpus(VectorSource(b))
cps = Corpus(VectorSource(b))
dtm = DocumentTermMatrix(cps, control = list(tokenize = extractNoun, removeNumber = T, removePunctuation = T))
rmat <-spMatrix(dtm$nrow,dtm$ncol, i=dtm$i, j=dtm$j, x=dtm$v)
wcount<-colSums(rmat)
wname <- dtm$dimnames$Terms
wname <- rvest::repair_encoding(dtm$dimnames$Terms)
colnames(rmat)<- wname
cps = Corpus(VectorSource(twitter_txt))
dtm = DocumentTermMatrix(cps, control = list(tokenize = extractNoun, removeNumber = T, removePunctuation = T))
rmat <-spMatrix(dtm$nrow,dtm$ncol, i=dtm$i, j=dtm$j, x=dtm$v)
wcount<-colSums(rmat)
wname <- dtm$dimnames$Terms
wname <- rvest::repair_encoding(dtm$dimnames$Terms)
colnames(rmat)<- wname
VectorSource(twitter_txt)
Corpus(VectorSource(twitter_txt))
library(KoNLP)
library(tm)
library(wordcloud)
library(Matrix)
