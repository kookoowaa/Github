final_dat = NULL
for(i in 1:length(start_point))
{
# request xml format
url = paste0('https://openapi.naver.com/v1/search/blog.xml?query=',query,'&display=',display_num,'&start=',start_point[i],'&sort=sim')
#option header
url_body = read_xml(GET(url, header), encoding = "UTF-8")
title = url_body %>% xml_nodes('item title') %>% xml_text()
bloggername = url_body %>% xml_nodes('item bloggername') %>% xml_text()
postdate = url_body %>% xml_nodes('postdate') %>% xml_text()
link = url_body %>% xml_nodes('item link') %>% xml_text()
description = url_body %>% xml_nodes('item description') %>% html_text()
temp_dat = cbind(title, bloggername, postdate, link, description)
final_dat = rbind(final_dat, temp_dat)
cat(i, '\n')
}
final_dat = data.frame(final_dat, stringsAsFactors = F)
# 상위 블로거 검색
library(dplyr)
tb = final_dat %>% select(bloggername) %>% table()
top_blogger = sort(tb, decreasing = T)[1:4]
tmp = final_dat %>% select(bloggername, title, link) %>% filter(bloggername %in% names(top_blogger))
#검색량 분ㅍ
library(pspline)
tb = final_dat %>% select(postdate) %>% table()
x <-as.Date(names(tb), format = "%Y%m%d")
y <- as.numeric(tb)
# as.Date("2019[02[03", format = '%Y[%m[%d')
fit <- sm.spline(x = as.integer(x), y = y, cv = TRUE)
plot(x, y, pch = 19, cex = 0.5)
lines(x=x, y=fit$ysmth, lty = 2, col = 'blue')
library(wordcloud)
pal <- brewer.pal(9, "Set1")
wordcloud(wname.rel,freq = wcount.rel, colors = pal)
library(KoNLP)
library(httr)
library(rvest)
#Sys.getlocale()
Sys.setlocale("LC_ALL", "English")
client_id = 'pNeL9M2Busi7vWn4XkW6';
client_secret = 'nTY9Mj5v2K';
header = httr::add_headers(
'X-Naver-Client-Id' = client_id,
'X-Naver-Client-Secret' = client_secret)
query.n = query = '허니버터칩'
# encoding 변화
query = iconv(query, to = 'UTF-8', toRaw = T)
# iconv(query, to = "UTF-8", toRaw = F)
query = paste0('%', paste(unlist(query), collapse = '%'))
query = toupper(query)
end_num = 1000
display_num = 100
start_point = seq(1, end_num, by = display_num)
i = 1
url = paste0('https://openapi.naver.com/v1/search/blog.xml?query=',query,'&display=',display_num,'&start=',start_point[i],'&sort=sim')
url_body = read_xml(GET(url, header))
title = url_body %>% xml_nodes('item title') %>%
xml_text()
bloggername = url_body %>%
xml_nodes('item bloggername') %>% xml_text()
postdate = url_body %>% xml_nodes('postdate') %>%
xml_text()
link = url_body %>% xml_nodes('item link') %>%
xml_text()
description = url_body %>% xml_nodes('item description') %>%
html_text()
final_dat = NULL
for(i in 1:length(start_point))
{
# request xml format
url = paste0('https://openapi.naver.com/v1/search/blog.xml?query=',query,'&display=',display_num,'&start=',start_point[i],'&sort=sim')
#option header
url_body = read_xml(GET(url, header), encoding = "UTF-8")
title = url_body %>% xml_nodes('item title') %>% xml_text()
bloggername = url_body %>% xml_nodes('item bloggername') %>% xml_text()
postdate = url_body %>% xml_nodes('postdate') %>% xml_text()
link = url_body %>% xml_nodes('item link') %>% xml_text()
description = url_body %>% xml_nodes('item description') %>% html_text()
temp_dat = cbind(title, bloggername, postdate, link, description)
final_dat = rbind(final_dat, temp_dat)
cat(i, '\n')
}
final_dat = data.frame(final_dat, stringsAsFactors = F)
# 상위 블로거 검색
library(dplyr)
tb = final_dat %>% select(bloggername) %>% table()
top_blogger = sort(tb, decreasing = T)[1:4]
tmp = final_dat %>% select(bloggername, title, link) %>% filter(bloggername %in% names(top_blogger))
#검색량 분ㅍ
library(pspline)
tb = final_dat %>% select(postdate) %>% table()
x <-as.Date(names(tb), format = "%Y%m%d")
y <- as.numeric(tb)
# as.Date("2019[02[03", format = '%Y[%m[%d')
fit <- sm.spline(x = as.integer(x), y = y, cv = TRUE)
plot(x, y, pch = 19, cex = 0.5)
lines(x=x, y=fit$ysmth, lty = 2, col = 'blue')
library(KoNLP)
library(httr)
library(rvest)
#Sys.getlocale()
Sys.setlocale("LC_ALL", "English")
client_id = 'pNeL9M2Busi7vWn4XkW6';
client_secret = 'nTY9Mj5v2K';
header = httr::add_headers(
'X-Naver-Client-Id' = client_id,
'X-Naver-Client-Secret' = client_secret)
query.n = query = '허니버터칩'
# encoding 변화
query = iconv(query, to = 'UTF-8', toRaw = T)
# iconv(query, to = "UTF-8", toRaw = F)
query = paste0('%', paste(unlist(query), collapse = '%'))
query = toupper(query)
end_num = 1000
display_num = 100
start_point = seq(1, end_num, by = display_num)
i = 1
url = paste0('https://openapi.naver.com/v1/search/blog.xml?query=',query,'&display=',display_num,'&start=',start_point[i],'&sort=sim')
url_body = read_xml(GET(url, header))
title = url_body %>% xml_nodes('item title') %>%
xml_text()
bloggername = url_body %>%
xml_nodes('item bloggername') %>% xml_text()
postdate = url_body %>% xml_nodes('postdate') %>%
xml_text()
link = url_body %>% xml_nodes('item link') %>%
xml_text()
description = url_body %>% xml_nodes('item description') %>%
html_text()
final_dat = NULL
for(i in 1:length(start_point))
{
# request xml format
url = paste0('https://openapi.naver.com/v1/search/blog.xml?query=',query,'&display=',display_num,'&start=',start_point[i],'&sort=sim')
#option header
url_body = read_xml(GET(url, header), encoding = "UTF-8")
title = url_body %>% xml_nodes('item title') %>% xml_text()
bloggername = url_body %>% xml_nodes('item bloggername') %>% xml_text()
postdate = url_body %>% xml_nodes('postdate') %>% xml_text()
link = url_body %>% xml_nodes('item link') %>% xml_text()
description = url_body %>% xml_nodes('item description') %>% html_text()
temp_dat = cbind(title, bloggername, postdate, link, description)
final_dat = rbind(final_dat, temp_dat)
cat(i, '\n')
}
final_dat = data.frame(final_dat, stringsAsFactors = F)
# 상위 블로거 검색
library(dplyr)
tb = final_dat %>% select(bloggername) %>% table()
top_blogger = sort(tb, decreasing = T)[1:4]
tmp = final_dat %>% select(bloggername, title, link) %>% filter(bloggername %in% names(top_blogger))
#검색량 분ㅍ
library(pspline)
tb = final_dat %>% select(postdate) %>% table()
x <-as.Date(names(tb), format = "%Y%m%d")
y <- as.numeric(tb)
# as.Date("2019[02[03", format = '%Y[%m[%d')
fit <- sm.spline(x = as.integer(x), y = y, cv = TRUE)
plot(x, y, pch = 19, cex = 0.5)
lines(x=x, y=fit$ysmth, lty = 2, col = 'blue')
x
library(KoNLP)
library(httr)
library(rvest)
#Sys.getlocale()
Sys.setlocale("LC_ALL", "English")
client_id = 'pNeL9M2Busi7vWn4XkW6';
client_secret = 'nTY9Mj5v2K';
header = httr::add_headers(
'X-Naver-Client-Id' = client_id,
'X-Naver-Client-Secret' = client_secret)
query.n = query = '허니버터칩'
# encoding 변화
query = iconv(query, to = 'UTF-8', toRaw = T)
# iconv(query, to = "UTF-8", toRaw = F)
query = paste0('%', paste(unlist(query), collapse = '%'))
query = toupper(query)
end_num = 1000
display_num = 100
start_point = seq(1, end_num, by = display_num)
i = 1
url = paste0('https://openapi.naver.com/v1/search/blog.xml?query=',query,'&display=',display_num,'&start=',start_point[i],'&sort=sim')
url_body = read_xml(GET(url, header))
title = url_body %>% xml_nodes('item title') %>%
xml_text()
bloggername = url_body %>%
xml_nodes('item bloggername') %>% xml_text()
postdate = url_body %>% xml_nodes('postdate') %>%
xml_text()
link = url_body %>% xml_nodes('item link') %>%
xml_text()
description = url_body %>% xml_nodes('item description') %>%
html_text()
final_dat = NULL
for(i in 1:length(start_point))
{
# request xml format
url = paste0('https://openapi.naver.com/v1/search/blog.xml?query=',query,'&display=',display_num,'&start=',start_point[i],'&sort=sim')
#option header
url_body = read_xml(GET(url, header), encoding = "UTF-8")
title = url_body %>% xml_nodes('item title') %>% xml_text()
bloggername = url_body %>% xml_nodes('item bloggername') %>% xml_text()
postdate = url_body %>% xml_nodes('postdate') %>% xml_text()
link = url_body %>% xml_nodes('item link') %>% xml_text()
description = url_body %>% xml_nodes('item description') %>% html_text()
temp_dat = cbind(title, bloggername, postdate, link, description)
final_dat = rbind(final_dat, temp_dat)
cat(i, '\n')
}
final_dat = data.frame(final_dat, stringsAsFactors = F)
# 상위 블로거 검색
library(dplyr)
tb = final_dat %>% select(bloggername) %>% table()
top_blogger = sort(tb, decreasing = T)[1:4]
tmp = final_dat %>% select(bloggername, title, link) %>% filter(bloggername %in% names(top_blogger))
#검색량 분ㅍ
library(pspline)
tb = final_dat %>% select(postdate) %>% table()
x <-as.Date(names(tb), format = "%Y%m%d")
y <- as.numeric(tb)
# as.Date("2019[02[03", format = '%Y[%m[%d')
fit <- sm.spline(x = as.integer(x), y = y, cv = TRUE)
plot(x, y, pch = 19, cex = 0.5)
lines(x=x, y=fit$ysmth, lty = 2, col = 'blue')
x
y
query.n = query = '허니버터칩'
# encoding 변화
query = iconv(query, to = 'UTF-8', toRaw = T)
# iconv(query, to = "UTF-8", toRaw = F)
query = paste0('%', paste(unlist(query), collapse = '%'))
query = toupper(query)
end_num = 1000
display_num = 100
start_point = seq(1, end_num, by = display_num)
i = 1
url = paste0('https://openapi.naver.com/v1/search/blog.xml?query=',query,'&display=',display_num,'&start=',start_point[i],'&sort=sim')
url_body = read_xml(GET(url, header))
library(KoNLP)
library(httr)
library(rvest)
library(dplyr)
#Sys.getlocale()
Sys.setlocale("LC_ALL", "Korean")
client_id = 'pNeL9M2Busi7vWn4XkW6';
client_secret = 'nTY9Mj5v2K';
header = httr::add_headers(
'X-Naver-Client-Id' = client_id,
'X-Naver-Client-Secret' = client_secret)
query.n = query = '허니버터칩'
# encoding 변화
query = iconv(query, to = 'UTF-8', toRaw = T)
# iconv(query, to = "UTF-8", toRaw = F)
query = paste0('%', paste(unlist(query), collapse = '%'))
query = toupper(query)
end_num = 1000
display_num = 100
start_point = seq(1, end_num, by = display_num)
i = 1
url = paste0('https://openapi.naver.com/v1/search/blog.xml?query=',query,'&display=',display_num,'&start=',start_point[i],'&sort=sim')
url_body = read_xml(GET(url, header))
title = url_body %>% xml_nodes('item title') %>%
xml_text()
bloggername = url_body %>%
xml_nodes('item bloggername') %>% xml_text()
postdate = url_body %>% xml_nodes('postdate') %>%
xml_text()
link = url_body %>% xml_nodes('item link') %>%
xml_text()
description = url_body %>% xml_nodes('item description') %>%
html_text()
final_dat = NULL
for(i in 1:length(start_point))
{
# request xml format
url = paste0('https://openapi.naver.com/v1/search/blog.xml?query=',query,'&display=',display_num,'&start=',start_point[i],'&sort=sim')
#option header
url_body = read_xml(GET(url, header), encoding = "UTF-8")
title = url_body %>% xml_nodes('item title') %>% xml_text()
bloggername = url_body %>% xml_nodes('item bloggername') %>% xml_text()
postdate = url_body %>% xml_nodes('postdate') %>% xml_text()
link = url_body %>% xml_nodes('item link') %>% xml_text()
description = url_body %>% xml_nodes('item description') %>% html_text()
temp_dat = cbind(title, bloggername, postdate, link, description)
final_dat = rbind(final_dat, temp_dat)
cat(i, '\n')
}
final_dat = data.frame(final_dat, stringsAsFactors = F)
# 상위 블로거 검색
#library(dplyr)
tb = final_dat %>% select(bloggername) %>% table()
top_blogger = sort(tb, decreasing = T)[1:4]
tmp = final_dat %>% select(bloggername, title, link) %>% filter(bloggername %in% names(top_blogger))
#검색량 분ㅍ
library(pspline)
tb = final_dat %>% select(postdate) %>% table()
x <-as.Date(names(tb), format = "%Y%m%d")
y <- as.numeric(tb)
# as.Date("2019[02[03", format = '%Y[%m[%d')
fit <- sm.spline(x = as.integer(x), y = y, cv = TRUE)
plot(x, y, pch = 19, cex = 0.5)
lines(x=x, y=fit$ysmth, lty = 2, col = 'blue')
#기간 동안 검색량 분석 02
xx <- as.Date(min(x):max(x),origin = "1970-01-01")
yy = rep(0, length(xx))
yy[xx%in%x] = y
fit<-sm.spline(xx,yy,cv = TRUE)
plot(xx, yy, pch = 19, cex = 0.5)
points(fit$x, fit$ysmth, type = 'l', lty = 2, lwd = 1.5, col = 'blue')
#국소다항회귀 (local polynomial regression)
xint = as.integer(xx)
rdata = data.frame(y = yy, x = xint)
fit<-loess(y~x,data = rdata, span = 0.5, normalize = FALSE)
plot(fit, pch = 19, cex = 0.5)
points(fit$x,fit$fitted, type = 'l', lty = 2, lwd = 1.5, col = 'blue')
#Span 값에 따라 주변(%) 데이터를 참조 (i.e. k-means)
#fit<-loess(y~x,data = rdata, span = 0.1, normalize = FALSE)
#plot(fit, pch = 19, cex = 0.5)
#points(fit$x,fit$fitted, type = 'l', lty = 2, lwd = 1.5, col = 'blue')
# K fold cross validation
k.fold = 5
idx <-sample(1:5, length(xint), replace = TRUE)
k = 1
rdata.tr <- rdata[idx != k, ]
rdata.va <- rdata[idx == k, ]
fit<-loess(y~x,data = rdata.tr, span = 0.1, normalize = FALSE)
fit.y<-predict(fit, newdata = rdata.va)
mean((fit.y-rdata.va$y)^2, na.rm = T)
# K fold cross validation 2 - loop
k.fold = 5
idx <-sample(1:k.fold, length(xint), replace = TRUE)
span.var <- seq(0.02, 0.5, by  = 0.01)
valid.mat <- NULL
for (j in 1:length(span.var))
{
valid.err <- c()
for (k in 1:k.fold)
{
rdata.tr <- rdata[idx != k, ]
rdata.va <- rdata[idx == k, ]
fit<-loess(y~x,data = rdata.tr,
span = span.var[j], normalize = FALSE)
fit.y<-predict(fit, newdata = rdata.va)
valid.err[k] <- mean((fit.y-rdata.va$y)^2, na.rm = T)
}
valid.mat <- cbind(valid.mat, valid.err)
}
valid.mat
boxplot(valid.mat)
lines(colMeans(valid.mat), col = "blue", lty = 2)
# K fold cross validation - model selection
span.par<- span.var[which.min(colMeans(valid.mat))]
fit<-loess(y~x,data = rdata,
span = span.par, normalize = FALSE)
plot(xx,yy,  pch = 19, cex = 0.5)
points(xx,fit$fitted, type = 'l', lty = 2, lwd = 1.5, col = 'blue')
# 단어 정제
final_dat[10,5]
a = gsub(pattern = "<[/?A-Za-z]*>", replace = "", final_dat[10,5])
a
dat_tmp <- final_dat
for (i in 1:nrow(final_dat))
{
dat_tmp[i,5]<- gsub(pattern = "<[/|A-Za-z]*>",  replace = "", final_dat[i,5])
dat_tmp[i,1]<- gsub(pattern = "<[/|A-Za-z]*>",  replace = "", final_dat[i,1])
}
#library(KoNLP)
#useSejongDic()
extractNoun(a)
library(tm)
text = dat_tmp[,5]
cps = Corpus(VectorSource(text))
dtm = tm::DocumentTermMatrix(cps, control = list(tokenize = extractNoun, removeNumber = T, removePunctuation = T))
str(dtm)
# sparse matrix(i = 행, j = 열, v = 값)
# Sparse Matrix 원리 참조
library(Matrix)
T3 <- spMatrix(3,4, i=c(1,3:1), j=c(2,4:2), x=1:4)
T3
rmat <- as.matrix(dtm) # 데이터가 작은 경우 매트릭스로 변경
str(rmat)
rmat <-spMatrix(dtm$nrow,dtm$ncol, i=dtm$i, j=dtm$j, x=dtm$v) #데이터가 클 경우 sparse matrix 사
head(rmat)
#Matrix 관련 연산자 사용 가능
#library(rvest)
wcount<-colSums(rmat)
wname <- dtm$dimnames$Terms
wname <- rvest::repair_encoding(dtm$dimnames$Terms)
colnames(rmat)<- wname
str(wname)
sort.var <- sort(wcount,decreasing = T)[100]
idx <- !( grepl(query.n, wname)| (wcount<=sort.var) )
wname.rel <- wname[idx]
wcount.rel <- wcount[idx]
library(wordcloud)
pal <- brewer.pal(9, "Set1")
wordcloud(wname.rel,freq = wcount.rel, colors = pal)
bb <- rmat
bb.freq <- sort(colSums(bb), decreasing = T)
plot(log(bb.freq), pch = 19, type = 'l')
bb.freq <- bb.freq[bb.freq>quantile(bb.freq,0.99)]
idx <- match(names(bb.freq), colnames(bb))
bb.r <- bb[,idx]
dim(bb.r)
bb.r <- as.matrix(bb.r)
bb.freq <- bb.freq[bb.freq>quantile(bb.freq,0.99)]
idx <- match(names(bb.freq), colnames(bb))
bb.r <- bb[,idx]
dim(bb.r)
bb.r <- as.matrix(bb.r)
cor.mat <- cor(bb.r)
image(cor.mat, col =terrain.colors(100))
bb.freq <- bb.freq[bb.freq>quantile(bb.freq,0.99)]
idx <- match(names(bb.freq), colnames(bb))
bb.r <- bb[,idx]
dim(bb.r)
bb.r <- as.matrix(bb.r)
cor.mat <- cor(bb.r)
image(cor.mat, col =terrain.colors(100))
sort(cor.mat[1,], decreasing = T)[1:10]
#상관분석
bb <- rmat
bb.freq <- sort(colSums(bb), decreasing = T)
plot(log(bb.freq), pch = 19, type = 'l')
bb.freq <- bb.freq[bb.freq>quantile(bb.freq,0.99)]
idx <- match(names(bb.freq), colnames(bb))
bb.r <- bb[,idx]
dim(bb.r)
bb.r <- as.matrix(bb.r)
cor.mat <- cor(bb.r)
image(cor.mat, col =terrain.colors(100))
sort(cor.mat[1,], decreasing = T)[1:10]
library(tuneR)
install.packages('tuneR')
library(tuneR)
header = add_headers('X-Naver-Client-Id' = client_id,'X-Naver-Client-Secret'
url = paste0("https://openapi.naver.com/v1/voice/tts.bin")
library(tuneR)
header = add_headers('X-Naver-Client-Id' = client_id,'X-Naver-Client-Secret' = client_secret)
url = paste0("https://openapi.naver.com/v1/voice/tts.bin")
speech = 'jinho'
encText = "�덈뀞�섏꽭��"
data = list(speaker = speech,
speed = 0,
text = encText)
url_post = POST(url,header, body = data, encode = 'form', write_disk('test.mp3',overwrite = T))
mp3 = readMP3('test.mp3')
play(mp3)
rm(list=ls())
MASS::mvrnorm()
MASS::mvrnorm(0,)
MASS::mvrnorm(mu = 0,)
MASS::mvrnorm(mu = 0, sigma())
Precision_matrix = matrix(c(1,0,.5,0,10.3,.5,.3,1), byrow = T, nrow=3)
Precision_matrix = matrix(c(1,0,.5,0,10,.3,.5,.3,1), byrow = T, nrow=3)
library(MASS)
Precision_matrix
Precision_matrix = matrix(c(1,0,.5,0,1,.3,.5,.3,1), byrow = T, nrow=3)
Cov_matrix = solve(Precision_matrix)
n = 100
pcor.vec <- rep(0,1000)
n = 100
pcor.vec <- rep(0,1000)
for ( i in 1:1000)
{
rdata<-mvrnorm(n,mu = rep(0,3), Sigma)
x = rdata[,1]
y = rdata[,2]
z = rdata[,3]
x = x-mean(x)
y = y-mean(y)
z = z-mean(z)
pcor.vec[i]<-cor(lm(x~z-1)$residual,lm(y~z-1)$residual)
}
n = 100
pcor.vec <- rep(0,1000)
for ( i in 1:1000)
{
rdata<-mvrnorm(n,mu = rep(0,3), Cov_matrix)
x = rdata[,1]
y = rdata[,2]
z = rdata[,3]
x = x-mean(x)
y = y-mean(y)
z = z-mean(z)
pcor.vec[i]<-cor(lm(x~z-1)$residual,lm(y~z-1)$residual)
}
boxplot(pcor.vec)
table(cor(lm(x~z-1)$residual,lm(y~z-1)$residual), cor(x,y))
cor(lm(x~z-1)$residual,lm(y~z-1)$residual)
cor(x,y)
table(cor(lm(x~z-1)$residual,lm(y~z-1)$residual), cor(x,y))
c(cor(lm(x~z-1)$residual,lm(y~z-1)$residual), cor(x,y))
set.seed(1)
y = runif(100)
x = matrix(runif(500,100,5))
x
set.seed(1)
y = runif(100)
x = matrix(runif(500),100,5)
x
x[,1]*1+x[,2]*0.5-x[,3]*2+x[,4]*3-x[,5]*0.1
(solve(t(x)%*%x))
y_hat = x%*%solve(t(x)%*%x)%*%t(x)%*%y
solve(t(x)%*%x)%*%t(x)%*%y
B_hat = solve(t(x)%*%x)%*%t(x)%*%y
lm(y~x[,1]+x[,2]+x[,3]+x[,4]+x[,5])
lm(y~x[,1]+x[,2]+x[,3]+x[,4]+x[,5]-1)
B_hat = solve(t(x)%*%x)%*%t(x)%*%y
lm(y~x[,1]+x[,2]+x[,3]+x[,4]+x[,5]-1)
lm(y~x[,1]+x[,2]+x[,3]+x[,4]+x[,5]-1)
B_hat
