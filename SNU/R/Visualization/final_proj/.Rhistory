}
final_dat = data.frame(final_dat, stringsAsFactors = F)
dat_tmp <- final_dat
for (i in 1:nrow(final_dat))
{
dat_tmp[i,5]<- gsub('[ㅋㅎㅠㅜ]', '', gsub( "<[/|A-Za-z]*>",  "", gsub("&[a-z]*;",  "", gsub("http[/:a-z.]*",  "", final_dat[i,5]))))
dat_tmp[i,1]<- gsub('[ㅋㅎㅠㅜ]', '',  gsub( "<[/|A-Za-z]*>",  "", gsub("&[a-z]*;",  "", final_dat[i,1])))
}
text = dat_tmp[,5]
cps = Corpus(VectorSource(text))
dtm = DocumentTermMatrix(cps, control = list(tokenize = extractNoun, removeNumber = T, removePunctuation = T))
rmat <-spMatrix(dtm$nrow,dtm$ncol, i=dtm$i, j=dtm$j, x=dtm$v)
wcount <- colSums(rmat)
wname <- repair_encoding(dtm$dimnames$Terms)
sort.var <- sort(wcount,decreasing = T)[200]
idx <- !( grepl('과자', wname) | (wcount<=sort.var)  |nchar(wname)==1 |grepl('있', wname)|grepl('역시', wname) |grepl('내', wname) |grepl('제', wname)|grepl('저', wname))
wname.rel <- wname[idx]
wcount.rel <- wcount[idx]
pal <- brewer.pal(12, "Paired")
wordcloud(wname.rel,scale=c(3.8,0.05),freq = wcount.rel, colors = pal)
text[grep('편의점',text) ]
sort.var <- sort(wcount,decreasing = T)[200]
idx <- !( grepl(query.n, wname) | (wcount<=sort.var)  |nchar(wname)==1 |grepl('과자', wname)|grepl('추천', wname)|grepl('있', wname))
wname.rel <- wname[idx]
wcount.rel <- wcount[idx]
client_id = 'pNeL9M2Busi7vWn4XkW6';
client_secret = 'nTY9Mj5v2K';
header = add_headers('X-Naver-Client-Id' = client_id, 'X-Naver-Client-Secret' = client_secret)
query.n = query = '과자 최고'
query = iconv(query, to = 'UTF-8', toRaw = T)
query = paste0('%', paste(unlist(query), collapse = '%'))
query = toupper(query)
end_num = 1000
display_num = 100
start_point = seq(1, end_num, by = display_num)
i = 1
final_dat = NULL
for(i in 1:length(start_point))
{
url = paste0('https://openapi.naver.com/v1/search/blog.xml?query=',query,'&display=',display_num,'&start=',start_point[i],'&sort=sim')
url_body = read_xml(GET(url, header), encoding = "UTF-8")
title = url_body %>% xml_nodes('item title') %>% xml_text()
bloggername = url_body %>% xml_nodes('item bloggername') %>% xml_text()
postdate = url_body %>% xml_nodes('postdate') %>% xml_text()
link = url_body %>% xml_nodes('item link') %>% xml_text()
description = url_body %>% xml_nodes('item description') %>% html_text()
temp_dat = cbind(title, bloggername, postdate, link, description)
final_dat = rbind(final_dat, temp_dat)
}
final_dat = data.frame(final_dat, stringsAsFactors = F)
dat_tmp <- final_dat
for (i in 1:nrow(final_dat))
{
dat_tmp[i,5]<- gsub('[ㅋㅎㅠㅜ]', '', gsub( "<[/|A-Za-z]*>",  "", gsub("&[a-z]*;",  "", gsub("http[/:a-z.]*",  "", final_dat[i,5]))))
dat_tmp[i,1]<- gsub('[ㅋㅎㅠㅜ]', '',  gsub( "<[/|A-Za-z]*>",  "", gsub("&[a-z]*;",  "", final_dat[i,1])))
}
text = dat_tmp[,5]
cps = Corpus(VectorSource(text))
dtm = DocumentTermMatrix(cps, control = list(tokenize = extractNoun, removeNumber = T, removePunctuation = T))
rmat <-spMatrix(dtm$nrow,dtm$ncol, i=dtm$i, j=dtm$j, x=dtm$v)
wcount <- colSums(rmat)
wname <- repair_encoding(dtm$dimnames$Terms)
sort.var <- sort(wcount,decreasing = T)[200]
idx <- !( grepl('과자', wname) | (wcount<=sort.var)  |nchar(wname)==1 |grepl('있', wname)|grepl('역시', wname) |grepl('내', wname) |grepl('제', wname)|grepl('저', wname))
wname.rel <- wname[idx]
wcount.rel <- wcount[idx]
pal <- brewer.pal(12, "Paired")
wordcloud(wname.rel,scale=c(3.8,0.05),freq = wcount.rel, colors = pal)
text[grep('꼬부',text) ]
sort.var <- sort(wcount,decreasing = T)[200]
idx <- !( grepl(query.n, wname) | (wcount<=sort.var)  |nchar(wname)==1 |grepl('과자', wname)|grepl('추천', wname)|grepl('있', wname))
wname.rel <- wname[idx]
wcount.rel <- wcount[idx]
sort.var <- sort(wcount,decreasing = T)[200]
idx <- !( grepl('과자', wname) | (wcount<=sort.var)  |nchar(wname)==1 |grepl('있', wname)|grepl('최고', wname) |grepl('내', wname) |grepl('제', wname)|grepl('저', wname))
wname.rel <- wname[idx]
wcount.rel <- wcount[idx]
pal <- brewer.pal(12, "Paired")
wordcloud(wname.rel,scale=c(3.8,0.05),freq = wcount.rel, colors = pal)
sort.var <- sort(wcount,decreasing = T)[200]
idx <- !( grepl('과자', wname) | (wcount<=sort.var)  |nchar(wname)==1 |grepl('있', wname)|grepl('최고', wname) |grepl('정말', wname) |grepl('제', wname)|grepl('저', wname))
wname.rel <- wname[idx]
wcount.rel <- wcount[idx]
pal <- brewer.pal(12, "Paired")
wordcloud(wname.rel,scale=c(3.8,0.05),freq = wcount.rel, colors = pal)
text[grep('꼬북',text) ]
client_id = 'pNeL9M2Busi7vWn4XkW6';
client_secret = 'nTY9Mj5v2K';
header = add_headers('X-Naver-Client-Id' = client_id, 'X-Naver-Client-Secret' = client_secret)
query.n = query = '과자 오리온 크라운 롯데 농심 해태 노브랜드'
query = iconv(query, to = 'UTF-8', toRaw = T)
query = paste0('%', paste(unlist(query), collapse = '%'))
query = toupper(query)
end_num = 1000
display_num = 100
start_point = seq(1, end_num, by = display_num)
i = 1
final_dat = NULL
for(i in 1:length(start_point))
{
url = paste0('https://openapi.naver.com/v1/search/blog.xml?query=',query,'&display=',display_num,'&start=',start_point[i],'&sort=sim')
url_body = read_xml(GET(url, header), encoding = "UTF-8")
title = url_body %>% xml_nodes('item title') %>% xml_text()
bloggername = url_body %>% xml_nodes('item bloggername') %>% xml_text()
postdate = url_body %>% xml_nodes('postdate') %>% xml_text()
link = url_body %>% xml_nodes('item link') %>% xml_text()
description = url_body %>% xml_nodes('item description') %>% html_text()
temp_dat = cbind(title, bloggername, postdate, link, description)
final_dat = rbind(final_dat, temp_dat)
}
final_dat = data.frame(final_dat, stringsAsFactors = F)
dat_tmp <- final_dat
for (i in 1:nrow(final_dat))
{
dat_tmp[i,5]<- gsub('[ㅋㅎㅠㅜ]', '', gsub( "<[/|A-Za-z]*>",  "", gsub("&[a-z]*;",  "", gsub("http[/:a-z.]*",  "", final_dat[i,5]))))
dat_tmp[i,1]<- gsub('[ㅋㅎㅠㅜ]', '',  gsub( "<[/|A-Za-z]*>",  "", gsub("&[a-z]*;",  "", final_dat[i,1])))
}
text = dat_tmp[,5]
cps = Corpus(VectorSource(text))
dtm = DocumentTermMatrix(cps, control = list(tokenize = extractNoun, removeNumber = T, removePunctuation = T))
rmat <-spMatrix(dtm$nrow,dtm$ncol, i=dtm$i, j=dtm$j, x=dtm$v)
wcount <- colSums(rmat)
wname <- repair_encoding(dtm$dimnames$Terms)
sort.var <- sort(wcount,decreasing = T)[200]
idx <- !( grepl('과자', wname) | (wcount<=sort.var)  |nchar(wname)==1 |grepl('있', wname)|grepl('최고', wname) |grepl('정말', wname) |grepl('제', wname)|grepl('저', wname))
wname.rel <- wname[idx]
wcount.rel <- wcount[idx]
pal <- brewer.pal(12, "Paired")
wordcloud(wname.rel,scale=c(3.8,0.05),freq = wcount.rel, colors = pal)
text[grep('꼬북',text) ]
sort.var <- sort(wcount,decreasing = T)[200]
idx <- !( grepl(query.n, wname) | (wcount<=sort.var)  |nchar(wname)==1 |grepl('과자', wname)|grepl('추천', wname)|grepl('있', wname))
wname.rel <- wname[idx]
wcount.rel <- wcount[idx]
sort.var <- sort(wcount,decreasing = T)[200]
idx <- !( grepl('과자', wname) | (wcount<=sort.var)  |nchar(wname)==1 |grepl('오리온', wname)|grepl('해태', wname) |grepl('농심', wname) |grepl('크라운', wname)|grepl('저', wname))
wname.rel <- wname[idx]
wcount.rel <- wcount[idx]
pal <- brewer.pal(12, "Paired")
wordcloud(wname.rel,scale=c(3.8,0.05),freq = wcount.rel, colors = pal)
sort.var <- sort(wcount,decreasing = T)[200]
idx <- !( grepl('과자', wname) | (wcount<=sort.var)  |nchar(wname)==1 |grepl('오리온', wname)|grepl('해태', wname) |grepl('농심', wname) |grepl('크라운', wname)|grepl('롯데', wname))
wname.rel <- wname[idx]
wcount.rel <- wcount[idx]
pal <- brewer.pal(12, "Paired")
wordcloud(wname.rel,scale=c(3.8,0.05),freq = wcount.rel, colors = pal)
sort.var <- sort(wcount,decreasing = T)[200]
idx <- !( grepl('과자', wname) | (wcount<=sort.var)  |nchar(wname)==1 |grepl('오리온', wname)|grepl('해태', wname) |grepl('농심', wname) |grepl('크라운', wname)|grepl('롯데', wname)|grepl('00', wname))
wname.rel <- wname[idx]
wcount.rel <- wcount[idx]
pal <- brewer.pal(12, "Paired")
wordcloud(wname.rel,scale=c(3.8,0.05),freq = wcount.rel, colors = pal)
text[grep('초코파이',text) ]
client_id = 'pNeL9M2Busi7vWn4XkW6';
client_secret = 'nTY9Mj5v2K';
header = add_headers('X-Naver-Client-Id' = client_id, 'X-Naver-Client-Secret' = client_secret)
query.n = query = '과자 브랜드'
query = iconv(query, to = 'UTF-8', toRaw = T)
query = paste0('%', paste(unlist(query), collapse = '%'))
query = toupper(query)
end_num = 1000
display_num = 100
start_point = seq(1, end_num, by = display_num)
i = 1
final_dat = NULL
for(i in 1:length(start_point))
{
url = paste0('https://openapi.naver.com/v1/search/blog.xml?query=',query,'&display=',display_num,'&start=',start_point[i],'&sort=sim')
url_body = read_xml(GET(url, header), encoding = "UTF-8")
title = url_body %>% xml_nodes('item title') %>% xml_text()
bloggername = url_body %>% xml_nodes('item bloggername') %>% xml_text()
postdate = url_body %>% xml_nodes('postdate') %>% xml_text()
link = url_body %>% xml_nodes('item link') %>% xml_text()
description = url_body %>% xml_nodes('item description') %>% html_text()
temp_dat = cbind(title, bloggername, postdate, link, description)
final_dat = rbind(final_dat, temp_dat)
}
final_dat = data.frame(final_dat, stringsAsFactors = F)
dat_tmp <- final_dat
for (i in 1:nrow(final_dat))
{
dat_tmp[i,5]<- gsub('[ㅋㅎㅠㅜ]', '', gsub( "<[/|A-Za-z]*>",  "", gsub("&[a-z]*;",  "", gsub("http[/:a-z.]*",  "", final_dat[i,5]))))
dat_tmp[i,1]<- gsub('[ㅋㅎㅠㅜ]', '',  gsub( "<[/|A-Za-z]*>",  "", gsub("&[a-z]*;",  "", final_dat[i,1])))
}
text = dat_tmp[,5]
cps = Corpus(VectorSource(text))
dtm = DocumentTermMatrix(cps, control = list(tokenize = extractNoun, removeNumber = T, removePunctuation = T))
rmat <-spMatrix(dtm$nrow,dtm$ncol, i=dtm$i, j=dtm$j, x=dtm$v)
wcount <- colSums(rmat)
wname <- repair_encoding(dtm$dimnames$Terms)
sort.var <- sort(wcount,decreasing = T)[200]
idx <- !( grepl('과자', wname) | (wcount<=sort.var)  |nchar(wname)==1 |grepl('오리온', wname)|grepl('해태', wname) |grepl('농심', wname) |grepl('크라운', wname)|grepl('롯데', wname)|grepl('00', wname))
wname.rel <- wname[idx]
wcount.rel <- wcount[idx]
pal <- brewer.pal(12, "Paired")
wordcloud(wname.rel,scale=c(3.8,0.05),freq = wcount.rel, colors = pal)
text[grep('초코파이',text) ]
sort.var <- sort(wcount,decreasing = T)[200]
idx <- !( grepl(query.n, wname) | (wcount<=sort.var)  |nchar(wname)==1 |grepl('과자', wname)|grepl('추천', wname)|grepl('있', wname))
wname.rel <- wname[idx]
wcount.rel <- wcount[idx]
sort.var <- sort(wcount,decreasing = T)[200]
idx <- !( grepl('과자', wname) | (wcount<=sort.var)  |nchar(wname)==1 |grepl('오리온', wname)|grepl('해태', wname) |grepl('농심', wname) |grepl('크라운', wname)|grepl('롯데', wname)|grepl('브랜드', wname))
wname.rel <- wname[idx]
wcount.rel <- wcount[idx]
pal <- brewer.pal(12, "Paired")
wordcloud(wname.rel,scale=c(3.8,0.05),freq = wcount.rel, colors = pal)
client_id = 'pNeL9M2Busi7vWn4XkW6';
client_secret = 'nTY9Mj5v2K';
header = add_headers('X-Naver-Client-Id' = client_id, 'X-Naver-Client-Secret' = client_secret)
query.n = query = '과자 좋아하는'
query = iconv(query, to = 'UTF-8', toRaw = T)
query = paste0('%', paste(unlist(query), collapse = '%'))
query = toupper(query)
end_num = 1000
display_num = 100
start_point = seq(1, end_num, by = display_num)
i = 1
final_dat = NULL
for(i in 1:length(start_point))
{
url = paste0('https://openapi.naver.com/v1/search/blog.xml?query=',query,'&display=',display_num,'&start=',start_point[i],'&sort=sim')
url_body = read_xml(GET(url, header), encoding = "UTF-8")
title = url_body %>% xml_nodes('item title') %>% xml_text()
bloggername = url_body %>% xml_nodes('item bloggername') %>% xml_text()
postdate = url_body %>% xml_nodes('postdate') %>% xml_text()
link = url_body %>% xml_nodes('item link') %>% xml_text()
description = url_body %>% xml_nodes('item description') %>% html_text()
temp_dat = cbind(title, bloggername, postdate, link, description)
final_dat = rbind(final_dat, temp_dat)
}
final_dat = data.frame(final_dat, stringsAsFactors = F)
dat_tmp <- final_dat
for (i in 1:nrow(final_dat))
{
dat_tmp[i,5]<- gsub('[ㅋㅎㅠㅜ]', '', gsub( "<[/|A-Za-z]*>",  "", gsub("&[a-z]*;",  "", gsub("http[/:a-z.]*",  "", final_dat[i,5]))))
dat_tmp[i,1]<- gsub('[ㅋㅎㅠㅜ]', '',  gsub( "<[/|A-Za-z]*>",  "", gsub("&[a-z]*;",  "", final_dat[i,1])))
}
text = dat_tmp[,5]
cps = Corpus(VectorSource(text))
dtm = DocumentTermMatrix(cps, control = list(tokenize = extractNoun, removeNumber = T, removePunctuation = T))
rmat <-spMatrix(dtm$nrow,dtm$ncol, i=dtm$i, j=dtm$j, x=dtm$v)
wcount <- colSums(rmat)
wname <- repair_encoding(dtm$dimnames$Terms)
sort.var <- sort(wcount,decreasing = T)[200]
idx <- !( grepl('과자', wname) | (wcount<=sort.var)  |nchar(wname)==1 |grepl('오리온', wname)|grepl('해태', wname) |grepl('농심', wname) |grepl('크라운', wname)|grepl('롯데', wname)|grepl('브랜드', wname))
wname.rel <- wname[idx]
wcount.rel <- wcount[idx]
pal <- brewer.pal(12, "Paired")
wordcloud(wname.rel,scale=c(3.8,0.05),freq = wcount.rel, colors = pal)
text[grep('초코파이',text) ]
sort.var <- sort(wcount,decreasing = T)[200]
idx <- !( grepl(query.n, wname) | (wcount<=sort.var)  |nchar(wname)==1 |grepl('과자', wname)|grepl('추천', wname)|grepl('있', wname))
wname.rel <- wname[idx]
wcount.rel <- wcount[idx]
sort.var <- sort(wcount,decreasing = T)[200]
idx <- !( grepl('과자', wname) | (wcount<=sort.var)  |nchar(wname)==1 |grepl('오리온', wname)|grepl('해태', wname) |grepl('농심', wname) |grepl('크라운', wname)|grepl('롯데', wname)|grepl('좋', wname))
wname.rel <- wname[idx]
wcount.rel <- wcount[idx]
pal <- brewer.pal(12, "Paired")
wordcloud(wname.rel,scale=c(3.8,0.05),freq = wcount.rel, colors = pal)
sort.var <- sort(wcount,decreasing = T)[200]
idx <- !( grepl('과자', wname) | (wcount<=sort.var)  |nchar(wname)==1 |grepl('제', wname)|grepl('내', wname) |grepl('정말', wname) |grepl('너무', wname)|grepl('롯데', wname)|grepl('좋', wname))
wname.rel <- wname[idx]
wcount.rel <- wcount[idx]
pal <- brewer.pal(12, "Paired")
wordcloud(wname.rel,scale=c(3.8,0.05),freq = wcount.rel, colors = pal)
sort.var
sort(wcount,decreasing = T)
sort.var <- sort(wcount,decreasing = T)[200]
idx <- !( grepl('과자', wname) | (wcount<10) | (wcount>50)  |nchar(wname)==1 |grepl('제', wname)|grepl('내', wname) |grepl('정말', wname) |grepl('너무', wname)|grepl('롯데', wname)|grepl('좋', wname))
wname.rel <- wname[idx]
wcount.rel <- wcount[idx]
pal <- brewer.pal(12, "Paired")
wordcloud(wname.rel,scale=c(3.8,0.05),freq = wcount.rel, colors = pal)
sort.var <- sort(wcount,decreasing = T)[200]
idx <- !( grepl('과자', wname) | (wcount<10) | (wcount>60)  |nchar(wname)==1 |grepl('제', wname)|grepl('내', wname) |grepl('정말', wname) |grepl('너무', wname)|grepl('롯데', wname)|grepl('좋', wname))
wname.rel <- wname[idx]
wcount.rel <- wcount[idx]
pal <- brewer.pal(12, "Paired")
wordcloud(wname.rel,scale=c(3.8,0.05),freq = wcount.rel, colors = pal)
sort.var <- sort(wcount,decreasing = T)[200]
idx <- !( grepl('과자', wname) | (wcount<10) | (wcount>50)  |nchar(wname)==1 |grepl('제', wname)|grepl('내', wname) |grepl('정말', wname) |grepl('너무', wname)|grepl('롯데', wname)|grepl('좋', wname))
wname.rel <- wname[idx]
wcount.rel <- wcount[idx]
pal <- brewer.pal(12, "Paired")
wordcloud(wname.rel,scale=c(3.8,0.05),freq = wcount.rel, colors = pal)
sort.var <- sort(wcount,decreasing = T)[200]
idx <- !( grepl('과자', wname) | (wcount<10) | (wcount>45)  |nchar(wname)==1 |grepl('제', wname)|grepl('내', wname) |grepl('정말', wname) |grepl('너무', wname)|grepl('롯데', wname)|grepl('좋', wname))
wname.rel <- wname[idx]
wcount.rel <- wcount[idx]
pal <- brewer.pal(12, "Paired")
wordcloud(wname.rel,scale=c(3.8,0.05),freq = wcount.rel, colors = pal)
sort.var <- sort(wcount,decreasing = T)[200]
idx <- !( grepl('과자', wname) | (wcount<10) | (wcount>40)  |nchar(wname)==1 |grepl('제', wname)|grepl('내', wname) |grepl('정말', wname) |grepl('너무', wname)|grepl('롯데', wname)|grepl('좋', wname))
wname.rel <- wname[idx]
wcount.rel <- wcount[idx]
pal <- brewer.pal(12, "Paired")
wordcloud(wname.rel,scale=c(3.8,0.05),freq = wcount.rel, colors = pal)
idx
wname.rel
sort(wcount,decreasing = T)
sort.var <- sort(wcount,decreasing = T)[200]
idx <- !( grepl('과자', wname) | (wcount<11) | (wcount>40)  |nchar(wname)==1 |grepl('제', wname)|grepl('내', wname) |grepl('정말', wname) |grepl('너무', wname)|grepl('롯데', wname)|grepl('좋', wname))
wname.rel <- wname[idx]
wcount.rel <- wcount[idx]
pal <- brewer.pal(12, "Paired")
wordcloud(wname.rel,scale=c(3.8,0.05),freq = wcount.rel, colors = pal)
sort.var <- sort(wcount,decreasing = T)[200]
idx <- !( grepl('과자', wname) | (wcount<11) | (wcount>35)  |nchar(wname)==1 |grepl('제', wname)|grepl('내', wname) |grepl('정말', wname) |grepl('너무', wname)|grepl('롯데', wname)|grepl('좋', wname))
wname.rel <- wname[idx]
wcount.rel <- wcount[idx]
pal <- brewer.pal(12, "Paired")
wordcloud(wname.rel,scale=c(3.8,0.05),freq = wcount.rel, colors = pal)
sort.var <- sort(wcount,decreasing = T)[200]
idx <- !( grepl('과자', wname) | (wcount<11) | (wcount>30)  |nchar(wname)==1 |grepl('제', wname)|grepl('내', wname) |grepl('정말', wname) |grepl('너무', wname)|grepl('롯데', wname)|grepl('좋', wname))
wname.rel <- wname[idx]
wcount.rel <- wcount[idx]
pal <- brewer.pal(12, "Paired")
wordcloud(wname.rel,scale=c(3.8,0.05),freq = wcount.rel, colors = pal)
sort.var <- sort(wcount,decreasing = T)[200]
idx <- !( (wcount<11) | (wcount>30))
wname.rel <- wname[idx]
wcount.rel <- wcount[idx]
wname.rel
sort.var <- sort(wcount,decreasing = T)[200]
idx <- ( (wcount>11) | (wcount<30))
wname.rel <- wname[idx]
wcount.rel <- wcount[idx]
pal <- brewer.pal(12, "Paired")
wordcloud(wname.rel,scale=c(3.8,0.05),freq = wcount.rel, colors = pal)
pal <- brewer.pal(12, "Paired")
wordcloud(wname.rel,scale=c(3.8,0.05),freq = wcount.rel, colors = pal)
wname.rel
sort.var <- sort(wcount,decreasing = T)[200]
idx <- ( (wcount>11) & (wcount<30))
wname.rel <- wname[idx]
wcount.rel <- wcount[idx]
wname.rel
sort.var <- sort(wcount,decreasing = T)[200]
idx <- ( (wcount>15) & (wcount<30))
wname.rel <- wname[idx]
wcount.rel <- wcount[idx]
wname.rel
sort.var <- sort(wcount,decreasing = T)[200]
idx <- ( (wcount>15) & (wcount<30) $ nchar(wname!=1))
nchar(wname!=1)
nchar(wname)!=1
sort.var <- sort(wcount,decreasing = T)[200]
idx <- ( (wcount>15) & (wcount<30) & nchar(wname)!=1)
wname.rel <- wname[idx]
wcount.rel <- wcount[idx]
wname.rel
sort.var <- sort(wcount,decreasing = T)[200]
idx <- ( (wcount>15) & (wcount<35) & nchar(wname)!=1)
wname.rel <- wname[idx]
wcount.rel <- wcount[idx]
wname.rel
text[wcount==10]
wname[wcount==10]
sort.var <- sort(wcount,decreasing = T)
sort.var <- sort(wcount,decreasing = T)
sort(wcount,decreasing = T)
wname[wcount>100]
wname[wcount<100 & wcount>90]
wname[wcount<100 & wcount>80]
wname[wcount<100 & wcount>70]
wname[wcount<100 & wcount>60]
wname[wcount<100 & wcount>50]
wname[wcount<50 & wcount>45]
wname[wcount<50 & wcount>40]
wname[wcount<50 & wcount>30]
wname[wcount<50 & wcount>20]
library(httr)
library(rvest)
url_get = GET('http://openapi.seoul.go.kr:8088/6b46504b456b6f6f313130734d624851/xml/SearchParkingInfo/1/5/')
url_xml = read_xml(url_get)
url_xml
item_list = xml_nodes(url_xml, 'items item')
item_list = xml_nodes(url_xml, 'items item')
item_list
item_list = xml_nodes(url_xml, 'PARKING_NAME')
item_list
str(item_list)
gsub('<[A-Z/_]>','',item_list)
gsub('<[A-Z/_]>','',item_list[1])
gsub('<[A-Z/_]*>','',item_list[1])
gsub('<[A-Z/_]*>','',item_list)
LENGTH(gsub('<[A-Z/_]*>','',item_list))
length(gsub('<[A-Z/_]*>','',item_list))
url_get = GET('http://openapi.seoul.go.kr:8088//xml/SearchParkingInfo/1/100/')
url_xml = read_xml(url_get)
url_xml
url_get = GET('http://openapi.seoul.go.kr:8088/6b46504b456b6f6f313130734d624851/xml/SearchParkingInfo/1/100/')
url_xml = read_xml(url_get)
url_xml
item_list = xml_nodes(url_xml, 'PARKING_NAME')
item_list
length(gsub('<[A-Z/_]*>','',item_list))
gsub('<[A-Z/_]*>','',item_list)
url_xml[3]
url_xml
url_xml$node
url_xml$doc
url_xml
url_xml<SearchParkingInfo>
url_xml
url_get = GET('http://openapi.seoul.go.kr:8088/6b46504b456b6f6f313130734d624851/xml/SearchParkingInfo/1/100/')
url_xml = read_xml(url_get)
url_xml
url_get = GET('http://openapi.seoul.go.kr:8088/6b46504b456b6f6f313130734d624851/xml/SearchParkingInfo/1/828/')
url_xml = read_xml(url_get)
url_xml
length(url_xml)
url_xml
url_xml[1]
url_xml[2]
url_xml
gsub('<[A-Z/_]*>','',url_xml)
gsub('<[A-Z\\/_]*>','',url_xml)
data.frame(url_xml)
xml_text(url_xml)
url_xml[[1]]
unique(gsub('<[A-Z/_]*>','',item_list))
url_xml
item_list = xml_nodes(url_xml, 'PARKING_NAME', 'LAT','LNG')
item_list = xml_nodes(url_xml, 'PARKING_NAME', 'LAT')
item_list = xml_nodes(url_xml, c('PARKING_NAME', 'LAT'))
item_list = xml_nodes(url_xml, 'LAT')
item_list
setwd('d:/github/R/visualization')
getwd()
library(httr)
library(rvest)
library(dplyr)
key = '6b46504b456b6f6f313130734d624851'
parking_info = data.frame()
for (i in 0:9){
start_num = i*1000+1
url = paste0('http://openapi.seoul.go.kr:8088/',
key,
'/xml/SearchParkingInfo/',
start_num,
'/',
start_num+999)
url_get = GET(url)
url_xml = read_xml(url_get)
item_list = xml_nodes(url_xml, 'row')
item_list = lapply(item_list, function(x) return(xml_text(xml_children(x))))
item_dat = do.call('rbind',item_list)
item_dat = data.frame(item_dat, stringsAsFactors = F)
tmp = xml_nodes(url_xml, 'row')
colnames_dat = html_name(xml_children(tmp[[1]]))
colnames(item_dat) = colnames_dat
parking_info = rbind(parking_info ,item_dat[!duplicated(item_dat$PARKING_CODE),])
parking_info = parking_info[!duplicated(parking_info$PARKING_CODE),]
print(i)
}
parking_info_summarized = parking_info[c(1:3, 7, 8, 21, 22, 24:27)]
parking_info_summarized = parking_info[c(1:3, 7, 8, 21, 22, 24:27)]
write.csv(parking_info_summarized, './final_proj/parking_info.csv')
setwd('D:/Github/R/Visualization')
population = read.xlsx2('./final_proj/population.xls', 1, encoding = 'utf-8')
write.csv(parking_info_summarized, './final_proj/parking_info.csv')
parking_info_summarized = parking_info[c(1:3, 7, 8, 21, 22, 24:27)]
write.csv(parking_info_summarized, './final_proj/parking_info.csv')
write.csv(parking_info_summarized, './final_proj/parking_info.csv', a)
write.csv(parking_info_summarized, './final_proj/parking_info.csv', 'a')
write.csv2(parking_info_summarized, './final_proj/parking_info.csv')
PI = read.csv('./final_proj/parking_info.csv')
load("D:/Github/R/Visualization/final_proj/vehicle.xls")
write.csv(parking_info_summarized, "./final_proj/parking_info.csv"")
parking_info_summarized = parking_info[c(1:3, 7, 8, 21, 22, 24:27)]
write.csv(parking_info_summarized, "./final_proj/parking_info.csv")
write.csv(parking_info_summarized, "./final_proj/parking_info.csv")
library(httr)
library(rvest)
library(dplyr)
key = '6b46504b456b6f6f313130734d624851'
parking_info = data.frame()
#tmp = read.csv('./final_proj/parking_info.csv')
for (i in 0:9){
start_num = i*1000+1
url = paste0('http://openapi.seoul.go.kr:8088/',
key,
'/xml/SearchParkingInfo/',
start_num,
'/',
start_num+999)
url_get = GET(url)
url_xml = read_xml(url_get)
item_list = xml_nodes(url_xml, 'row')
item_list = lapply(item_list, function(x) return(xml_text(xml_children(x))))
item_dat = do.call('rbind',item_list)
item_dat = data.frame(item_dat, stringsAsFactors = F)
tmp = xml_nodes(url_xml, 'row')
colnames_dat = html_name(xml_children(tmp[[1]]))
colnames(item_dat) = colnames_dat
parking_info = rbind(parking_info ,item_dat[!duplicated(item_dat$PARKING_CODE),])
parking_info = parking_info[!duplicated(parking_info$PARKING_CODE),]
print(i)
}
parking_info_summarized = parking_info[c(1:3, 7, 8, 21, 22, 24:27)]
parking_info_summarized = parking_info[c(1:3, 7, 8, 21, 22, 24:27)]
#rbind()
write.csv(parking_info_summarized, './final_proj/parking_info.csv')
setwd('./final_proj')
setwd('d:/github/r/visualization/final_proj')
getwd()
setwd('D:/Github/R/Visualization/final_proj')
getwd()
setwd('./finalproj')
