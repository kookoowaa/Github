{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *학습 관련 기술들*\n",
    "\n",
    "## 1.  매개변수 갱신\n",
    "\n",
    "> - Optimization(최적화)은 손실함수의 최소값을 갖는 매개변수를 찾는 문제\n",
    "- 고차원 신경망에서 이를 찾기란 쉽지 않지만 반복적으로 매개변수의 기울기(미분)를 찾아가며 근사할 수 있음\n",
    "-  매개변수를 갱신하는 최적화 기법에는 SGD, Momemtum, AdaGrad, Adam 4종류가 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) SGD (확률적 경사 하강법)\n",
    "\n",
    "- SGD는 수식으로 다음과 같이 표현 가능\n",
    "$$\n",
    "W ← W - \\eta\\frac{\\delta L}{\\delta W}\n",
    "$$\n",
    "- $W$는 갱신할 매개변수고, $\\frac{\\delta L}{\\delta W}$는 손실함수의 기울기로 여깅에 $\\eta$만큼 값을 갱신한다는 의미\n",
    "- SGD는 아래와 같이 구현 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, lr = 0.01):\n",
    "        self.lr = lr\n",
    "       \n",
    "    \n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.lr*grads[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 핵심이 되는 params[key] -= self.lr\\*grads[key]는 params[key] = params[key] - self.lr\\*grads[key]와 같은 공식임\n",
    "- 해석하자면 매개편수인 params를 기울기에 학습률을 곱한만큼 반대 방향으로 이동기킨다고 볼 수 있음\n",
    "- 실제로 사용할 때에는 아래와 같이 사용 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = TwoLayerNet(...)\n",
    "optimizer = SGD()\n",
    "\n",
    "for i in range(10000):\n",
    "    ...\n",
    "    x_batch, t_batch = get_mini_batch(...)\n",
    "    grads = network.gradient(x_batch, t_batch)\n",
    "    params = network.params\n",
    "    optimizer.update(params, grads)\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위와 같이 optimizer가 최적화를 수행할 수 있도록 '매개변수'와 '기울기' 정보만 제공하면 계산하도록 클래스 구현 ...(params, grads)\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) SGD의 단점\n",
    "\n",
    "- SGD는 문제에 따라서 비효율적일 때가 많음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
