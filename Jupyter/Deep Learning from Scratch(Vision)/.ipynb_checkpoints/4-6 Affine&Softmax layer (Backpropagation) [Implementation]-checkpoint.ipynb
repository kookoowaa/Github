{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *오차역전파법*\n",
    "\n",
    "## 6. Affine/Softmax 계층 구현하기\n",
    "\n",
    "### 1) Affine 계층\n",
    "\n",
    "- **Affine transformation(어파인변환)**은 신경망의 순전파 때 수행하는 행렬의 곱을 의미\n",
    "- '2-3 다차원 배열의 계산'을 참고하면 아래와 같은 예를 들 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,) (2, 3) (3,)\n",
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "X = np.random.rand(2)     #입력\n",
    "W = np.random.rand(2,3)  #가중치\n",
    "B = np.random.rand(3)     #편향\n",
    "\n",
    "print(X.shape, W.shape, B.shape)\n",
    "\n",
    "Y = np.dot(X,W) +B\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위 예제를 계산그래프로 그리면 아래와 같이 표기 가능\n",
    "![](image/fig 5-24.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 스칼라 값 대신 행렬을 입력값으로 제공\n",
    "- X와 Y에 대해 역전파를 수행하면 아래 같은 식을 도출 가능\n",
    "\n",
    "<br>\n",
    "$$\n",
    "\\frac{\\delta L}{\\delta X}\\quad = \\quad \\frac{\\delta L}{\\delta Y}W^T\\\\\n",
    "\\frac{\\delta L}{\\delta W}\\quad = \\quad X^T\\frac{\\delta L}{\\delta Y}\n",
    "$$\n",
    "<br>\n",
    "\n",
    "- 그림으로 표현 시 다음과 같음\n",
    "![](image/fig 5-25.PNG)\n",
    "> ### 위 예제에서 변수의 형상(차원)에 주의\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) 배치용 Affine 계층\n",
    "\n",
    "- (2, ) 입력데이터 대신 데이터 N개를 묶어 batch로 순전파 하는 Affine 계층을 계산그래프로 그리면 아래와 같이 표현 가능\n",
    "\n",
    "![](image/fig 5-27.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 달라진 점은 X의 형상이 (N,2)가 된 것 뿐\n",
    "- 편향은 N번 반복하여 데이터 각각에 더해짐\n",
    "- 예를 들면 다음과 같음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0]\n",
      " [10 10 10]]\n",
      "\n",
      "[[ 1  2  3]\n",
      " [11 12 13]]\n"
     ]
    }
   ],
   "source": [
    "X_dot_W = np.array([[0, 0, 0], [10, 10, 10]])\n",
    "B = np.array([1, 2, 3])\n",
    "\n",
    "print(X_dot_W)\n",
    "print()\n",
    "print(X_dot_W + B)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 편향의 역전파는 N개 데이터에 대한 미분을 데이터마다 더해서 구함\n",
    "- 순전파의 편향 덧셈은 각각의 데이터에 더해지므로, 역전파 때는 각 데이터의 역전파 값이 편향의 원소에 모여야 함 (차원)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "\n",
      "[5 7 9]\n"
     ]
    }
   ],
   "source": [
    "dY = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "dB = np.sum(dY, axis = 0)\n",
    "\n",
    "print(dY)\n",
    "print()\n",
    "print(dB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이상의 Affine은 아래와 같이 구현 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis = 0)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### 3) Softmax-with-Loss 계층\n",
    "\n",
    "- 출력층의 Softmax에 대해 보기 전에 전체 흐름에 대해 보면 다음과 같음\n",
    "![](image/fig 5-28.png)\n",
    "- 위와 같이 Softmax 계층은 입력값을 정규화하여(합이 1) 확률로 출력\n",
    "\n",
    "> - 신경망에서 수행하는 작업은 **학습**과 **추론**임\n",
    "> - 추론도 위에서 정규화 하지 않은 점수까지의 추론과 확률 추론으로 나눠 볼 수 있음\n",
    "> - 답을 하나만 내는 경우 Softmax 계층은 필요 없음\n",
    "> - 하지만 신경망 학습에 있어 Softmax 계층은 필수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Softmax 계층 구현에 손실 함수인 Cross-Entropy-Error까지 포함하면 Softmax-with-Loss 계층 구현 가능\n",
    "- 아래의 계산 그래프로 표현 가능하며;\n",
    "![](image/fig 5-29.png)\n",
    "- 간소화된 계산그래프는 아래처럼 표현 가능\n",
    "![](image/fig 5-30.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 그림과 같이 Softmax 계층은 입력$(a_1, a_2, a_3)$를 정규화 하여 $(y_1, y_2, y_3)$를 출력\n",
    "> - Cross Entropy Error 계층은 $(y_1, y_2, y_3)$과 정답레이블$(t_1, t_2, t_3)$를 받고, 손실$L$을 출력\n",
    "> - ## 중요한 것은 Softmax의 역전파가 $(y_1-t_1, y_2-t_2, y_3-t_3)$이란 깔끔한 결과 출력\n",
    "> - ## 신경망의 역전파에서는 '정답과 출력의 오차'가 앞 계층에 전해지는 중요한 성질 보유"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 예를 들면 정답 레이블이 (0, 1, 0)일때 Softmax가 (0.3, 0.2, 0.5)를 출력한 경우 출력의 확률은 겨우 20%에 불과\n",
    "- 해당 계층의 역전파는 (0.3, -0.8, 0.5)라는 큰 오차를 전파하고, 학습 정도도 증가\n",
    "- 반대로 Softmax가 (0.01, 0.99, 0)을 출력한 경우 역전파로 보내는 오차는 (0.01, -0.01, 0)으로 학습 정도도 작아짐\n",
    "\n",
    "<br>\n",
    "- Softmax-with-Loss를 구현하면 아래와 같음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None  #손실\n",
    "        self.y = None     #Softmax 출력\n",
    "        self.t = None     #정답 레이블(원-핫 벡터)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)    #함수 정의 필요 \"2-5-2 소프트맥스 함수 구현시 주의점\"\n",
    "        self.loss = cross_entropy_error(self.y, self.t)    #함수 정의 필요 \"3-2-4 (배치용) 교차 엔트로피 오차 구현하기\"\n",
    "        return self.loss\n",
    "    \n",
    "    \n",
    "    def backward(self, dout = 1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "        \n",
    "        return dx"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
