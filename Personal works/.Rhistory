T3 <- spMatrix(3,4, i=c(1,3:1), j=c(2,4:2), x=1:4)
T3
rmat <- as.matrix(dtm) # 데이터가 작은 경우 매트릭스로 변경
str(rmat)
rmat <-spMatrix(dtm$nrow,dtm$ncol, i=dtm$i, j=dtm$j, x=dtm$v) #데이터가 클 경우 sparse matrix 사
head(rmat)
#Matrix 관련 연산자 사용 가능
#library(rvest)
wcount<-colSums(rmat)
wname <- dtm$dimnames$Terms
wname <- rvest::repair_encoding(dtm$dimnames$Terms)
colnames(rmat)<- wname
str(wname)
sort.var <- sort(wcount,decreasing = T)[100]
idx <- !( grepl(query.n, wname)| (wcount<=sort.var) )
wname.rel <- wname[idx]
wcount.rel <- wcount[idx]
library(wordcloud)
pal <- brewer.pal(9, "Set1")
wordcloud(wname.rel,freq = wcount.rel, colors = pal)
#상관분석
bb <- rmat
bb.freq <- sort(colSums(bb), decreasing = T)
plot(log(bb.freq), pch = 19, type = 'l')
bb.freq <- bb.freq[bb.freq>quantile(bb.freq,0.99)]
idx <- match(names(bb.freq), colnames(bb))
bb.r <- bb[,idx]
dim(bb.r)
bb.r <- as.matrix(bb.r)
cor.mat <- cor(bb.r)
image(cor.mat, col =terrain.colors(100))
sort(cor.mat[1,], decreasing = T)[1:10]
wname.rel
cbind(wname.rel, wcount.rel)
as.data.frame(wname.rel, wcount.rel)
as.data.frame(cbind(wname.rel, wcount.rel))
as.data.frame(cbind(wname.rel, wcount.rel)) %>% arrange(desc(wcount.rel))
as.data.frame(cbind(wname.rel, as.numeric(as.charactor(wcount.rel)))) %>% arrange(desc(wcount.rel))
as.data.frame(cbind(wname.rel, as.numeric(as.character(wcount.rel)))) %>% arrange(desc(wcount.rel))
sort(wcount,decreasing = T)[200]
wname
idx
sort.var <- sort(wcount,decreasing = T)[200]
idx <- !( grepl(query.n, wname)| (wcount<=sort.var) )
wname.rel <- wname[idx]
wcount.rel <- wcount[idx]
as.data.frame(cbind(wname.rel, as.numeric(as.character(wcount.rel)))) %>% arrange(desc(wcount.rel))
library(KoNLP)
library(httr)
library(rvest)
library(dplyr)
#Sys.getlocale()
Sys.setlocale("LC_ALL", "Korean")
client_id = 'pNeL9M2Busi7vWn4XkW6';
client_secret = 'nTY9Mj5v2K';
header = httr::add_headers(
'X-Naver-Client-Id' = client_id,
'X-Naver-Client-Secret' = client_secret)
query.n = query = '로지텍 후원'
# encoding 변화
query = iconv(query, to = 'UTF-8', toRaw = T)
# iconv(query, to = "UTF-8", toRaw = F)
query = paste0('%', paste(unlist(query), collapse = '%'))
query = toupper(query)
end_num = 1000
display_num = 100
start_point = seq(1, end_num, by = display_num)
i = 1
url = paste0('https://openapi.naver.com/v1/search/blog.xml?query=',query,'&display=',display_num,'&start=',start_point[i],'&sort=sim')
url_body = read_xml(GET(url, header))
title = url_body %>% xml_nodes('item title') %>%
xml_text()
bloggername = url_body %>%
xml_nodes('item bloggername') %>% xml_text()
postdate = url_body %>% xml_nodes('postdate') %>%
xml_text()
link = url_body %>% xml_nodes('item link') %>%
xml_text()
description = url_body %>% xml_nodes('item description') %>%
html_text()
final_dat = NULL
for(i in 1:length(start_point))
{
# request xml format
url = paste0('https://openapi.naver.com/v1/search/blog.xml?query=',query,'&display=',display_num,'&start=',start_point[i],'&sort=sim')
#option header
url_body = read_xml(GET(url, header), encoding = "UTF-8")
title = url_body %>% xml_nodes('item title') %>% xml_text()
bloggername = url_body %>% xml_nodes('item bloggername') %>% xml_text()
postdate = url_body %>% xml_nodes('postdate') %>% xml_text()
link = url_body %>% xml_nodes('item link') %>% xml_text()
description = url_body %>% xml_nodes('item description') %>% html_text()
temp_dat = cbind(title, bloggername, postdate, link, description)
final_dat = rbind(final_dat, temp_dat)
cat(i, '\n')
}
final_dat = data.frame(final_dat, stringsAsFactors = F)
# 상위 블로거 검색
#library(dplyr)
tb = final_dat %>% select(bloggername) %>% table()
top_blogger = sort(tb, decreasing = T)[1:4]
tmp = final_dat %>% select(bloggername, title, link) %>% filter(bloggername %in% names(top_blogger))
#검색량 분ㅍ
library(pspline)
tb = final_dat %>% select(postdate) %>% table()
x <-as.Date(names(tb), format = "%Y%m%d")
y <- as.numeric(tb)
# as.Date("2019[02[03", format = '%Y[%m[%d')
fit <- sm.spline(x = as.integer(x), y = y, cv = TRUE)
plot(x, y, pch = 19, cex = 0.5)
lines(x=x, y=fit$ysmth, lty = 2, col = 'blue')
#기간 동안 검색량 분석 02
xx <- as.Date(min(x):max(x),origin = "1970-01-01")
# 단어 정제
final_dat[10,5]
a = gsub(pattern = "<[/?A-Za-z]*>", replace = "", final_dat[10,5])
a
dat_tmp <- final_dat
for (i in 1:nrow(final_dat))
{
dat_tmp[i,5]<- gsub(pattern = "<[/|A-Za-z]*>",  replace = "", final_dat[i,5])
dat_tmp[i,1]<- gsub(pattern = "<[/|A-Za-z]*>",  replace = "", final_dat[i,1])
}
#library(KoNLP)
#useSejongDic()
extractNoun(a)
library(tm)
text = dat_tmp[,5]
cps = Corpus(VectorSource(text))
dtm = tm::DocumentTermMatrix(cps, control = list(tokenize = extractNoun, removeNumber = T, removePunctuation = T))
str(dtm)
# sparse matrix(i = 행, j = 열, v = 값)
# Sparse Matrix 원리 참조
library(Matrix)
T3 <- spMatrix(3,4, i=c(1,3:1), j=c(2,4:2), x=1:4)
T3
rmat <- as.matrix(dtm) # 데이터가 작은 경우 매트릭스로 변경
str(rmat)
rmat <-spMatrix(dtm$nrow,dtm$ncol, i=dtm$i, j=dtm$j, x=dtm$v) #데이터가 클 경우 sparse matrix 사
head(rmat)
#Matrix 관련 연산자 사용 가능
#library(rvest)
wcount<-colSums(rmat)
wname <- dtm$dimnames$Terms
wname <- rvest::repair_encoding(dtm$dimnames$Terms)
colnames(rmat)<- wname
str(wname)
sort.var <- sort(wcount,decreasing = T)[100]
idx <- !( grepl(query.n, wname)| (wcount<=sort.var) )
wname.rel <- wname[idx]
wcount.rel <- wcount[idx]
library(wordcloud)
pal <- brewer.pal(9, "Set1")
wordcloud(wname.rel,freq = wcount.rel, colors = pal)
as.data.frame(cbind(wname.rel, as.numeric(as.character(wcount.rel)))) %>% arrange(desc(wcount.rel))
#상관분석
bb <- rmat
bb.freq <- sort(colSums(bb), decreasing = T)
plot(log(bb.freq), pch = 19, type = 'l')
bb.freq <- bb.freq[bb.freq>quantile(bb.freq,0.99)]
idx <- match(names(bb.freq), colnames(bb))
bb.r <- bb[,idx]
dim(bb.r)
bb.r <- as.matrix(bb.r)
cor.mat <- cor(bb.r)
image(cor.mat, col =terrain.colors(100))
sort(cor.mat[1,], decreasing = T)[1:10]
library(KoNLP)
library(httr)
library(rvest)
library(dplyr)
#Sys.getlocale()
Sys.setlocale("LC_ALL", "Korean")
client_id = 'pNeL9M2Busi7vWn4XkW6';
client_secret = 'nTY9Mj5v2K';
header = httr::add_headers(
'X-Naver-Client-Id' = client_id,
'X-Naver-Client-Secret' = client_secret)
query.n = query = '오버워치 마우스'
# encoding 변화
query = iconv(query, to = 'UTF-8', toRaw = T)
# iconv(query, to = "UTF-8", toRaw = F)
query = paste0('%', paste(unlist(query), collapse = '%'))
query = toupper(query)
end_num = 1000
display_num = 100
start_point = seq(1, end_num, by = display_num)
i = 1
url = paste0('https://openapi.naver.com/v1/search/blog.xml?query=',query,'&display=',display_num,'&start=',start_point[i],'&sort=sim')
url_body = read_xml(GET(url, header))
title = url_body %>% xml_nodes('item title') %>%
xml_text()
bloggername = url_body %>%
xml_nodes('item bloggername') %>% xml_text()
postdate = url_body %>% xml_nodes('postdate') %>%
xml_text()
link = url_body %>% xml_nodes('item link') %>%
xml_text()
description = url_body %>% xml_nodes('item description') %>%
html_text()
final_dat = NULL
for(i in 1:length(start_point))
{
# request xml format
url = paste0('https://openapi.naver.com/v1/search/blog.xml?query=',query,'&display=',display_num,'&start=',start_point[i],'&sort=sim')
#option header
url_body = read_xml(GET(url, header), encoding = "UTF-8")
title = url_body %>% xml_nodes('item title') %>% xml_text()
bloggername = url_body %>% xml_nodes('item bloggername') %>% xml_text()
postdate = url_body %>% xml_nodes('postdate') %>% xml_text()
link = url_body %>% xml_nodes('item link') %>% xml_text()
description = url_body %>% xml_nodes('item description') %>% html_text()
temp_dat = cbind(title, bloggername, postdate, link, description)
final_dat = rbind(final_dat, temp_dat)
cat(i, '\n')
}
final_dat = data.frame(final_dat, stringsAsFactors = F)
# 상위 블로거 검색
#library(dplyr)
tb = final_dat %>% select(bloggername) %>% table()
top_blogger = sort(tb, decreasing = T)[1:4]
tmp = final_dat %>% select(bloggername, title, link) %>% filter(bloggername %in% names(top_blogger))
#검색량 분ㅍ
library(pspline)
tb = final_dat %>% select(postdate) %>% table()
x <-as.Date(names(tb), format = "%Y%m%d")
y <- as.numeric(tb)
# as.Date("2019[02[03", format = '%Y[%m[%d')
fit <- sm.spline(x = as.integer(x), y = y, cv = TRUE)
plot(x, y, pch = 19, cex = 0.5)
lines(x=x, y=fit$ysmth, lty = 2, col = 'blue')
#기간 동안 검색량 분석 02
xx <- as.Date(min(x):max(x),origin = "1970-01-01")
yy = rep(0, length(xx))
yy[xx%in%x] = y
fit<-sm.spline(xx,yy,cv = TRUE)
plot(xx, yy, pch = 19, cex = 0.5)
points(fit$x, fit$ysmth, type = 'l', lty = 2, lwd = 1.5, col = 'blue')
#국소다항회귀 (local polynomial regression)
xint = as.integer(xx)
rdata = data.frame(y = yy, x = xint)
fit<-loess(y~x,data = rdata, span = 0.5, normalize = FALSE)
plot(fit, pch = 19, cex = 0.5)
points(fit$x,fit$fitted, type = 'l', lty = 2, lwd = 1.5, col = 'blue')
#Span 값에 따라 주변(%) 데이터를 참조 (i.e. k-means)
#fit<-loess(y~x,data = rdata, span = 0.1, normalize = FALSE)
#plot(fit, pch = 19, cex = 0.5)
#points(fit$x,fit$fitted, type = 'l', lty = 2, lwd = 1.5, col = 'blue')
# K fold cross validation
k.fold = 5
idx <-sample(1:5, length(xint), replace = TRUE)
k = 1
rdata.tr <- rdata[idx != k, ]
rdata.va <- rdata[idx == k, ]
fit<-loess(y~x,data = rdata.tr, span = 0.1, normalize = FALSE)
fit.y<-predict(fit, newdata = rdata.va)
mean((fit.y-rdata.va$y)^2, na.rm = T)
# K fold cross validation 2 - loop
k.fold = 5
idx <-sample(1:k.fold, length(xint), replace = TRUE)
span.var <- seq(0.02, 0.5, by  = 0.01)
valid.mat <- NULL
for (j in 1:length(span.var))
{
valid.err <- c()
for (k in 1:k.fold)
{
rdata.tr <- rdata[idx != k, ]
rdata.va <- rdata[idx == k, ]
fit<-loess(y~x,data = rdata.tr,
span = span.var[j], normalize = FALSE)
fit.y<-predict(fit, newdata = rdata.va)
valid.err[k] <- mean((fit.y-rdata.va$y)^2, na.rm = T)
}
valid.mat <- cbind(valid.mat, valid.err)
}
valid.mat
boxplot(valid.mat)
lines(colMeans(valid.mat), col = "blue", lty = 2)
# K fold cross validation - model selection
span.par<- span.var[which.min(colMeans(valid.mat))]
fit<-loess(y~x,data = rdata,
span = span.par, normalize = FALSE)
plot(xx,yy,  pch = 19, cex = 0.5)
points(xx,fit$fitted, type = 'l', lty = 2, lwd = 1.5, col = 'blue')
# 단어 정제
final_dat[10,5]
a = gsub(pattern = "<[/?A-Za-z]*>", replace = "", final_dat[10,5])
a
dat_tmp <- final_dat
for (i in 1:nrow(final_dat))
{
dat_tmp[i,5]<- gsub(pattern = "<[/|A-Za-z]*>",  replace = "", final_dat[i,5])
dat_tmp[i,1]<- gsub(pattern = "<[/|A-Za-z]*>",  replace = "", final_dat[i,1])
}
#library(KoNLP)
#useSejongDic()
extractNoun(a)
library(tm)
text = dat_tmp[,5]
cps = Corpus(VectorSource(text))
dtm = tm::DocumentTermMatrix(cps, control = list(tokenize = extractNoun, removeNumber = T, removePunctuation = T))
str(dtm)
# sparse matrix(i = 행, j = 열, v = 값)
# Sparse Matrix 원리 참조
library(Matrix)
T3 <- spMatrix(3,4, i=c(1,3:1), j=c(2,4:2), x=1:4)
T3
rmat <- as.matrix(dtm) # 데이터가 작은 경우 매트릭스로 변경
str(rmat)
rmat <-spMatrix(dtm$nrow,dtm$ncol, i=dtm$i, j=dtm$j, x=dtm$v) #데이터가 클 경우 sparse matrix 사
head(rmat)
#Matrix 관련 연산자 사용 가능
#library(rvest)
wcount<-colSums(rmat)
wname <- dtm$dimnames$Terms
wname <- rvest::repair_encoding(dtm$dimnames$Terms)
colnames(rmat)<- wname
str(wname)
sort.var <- sort(wcount,decreasing = T)[100]
idx <- !( grepl(query.n, wname)| (wcount<=sort.var) )
wname.rel <- wname[idx]
wcount.rel <- wcount[idx]
library(wordcloud)
pal <- brewer.pal(9, "Set1")
wordcloud(wname.rel,freq = wcount.rel, colors = pal)
as.data.frame(cbind(wname.rel, as.numeric(as.character(wcount.rel)))) %>% arrange(desc(wcount.rel))
#상관분석
bb <- rmat
bb.freq <- sort(colSums(bb), decreasing = T)
plot(log(bb.freq), pch = 19, type = 'l')
bb.freq <- bb.freq[bb.freq>quantile(bb.freq,0.99)]
idx <- match(names(bb.freq), colnames(bb))
bb.r <- bb[,idx]
dim(bb.r)
bb.r <- as.matrix(bb.r)
cor.mat <- cor(bb.r)
image(cor.mat, col =terrain.colors(100))
sort(cor.mat[1,], decreasing = T)[1:10]
library(lda); library(ggplot2); library(reshape2); library(cowplot)
setwd('d:/github/personal works')
install.packages('lda')
library(lda); library(ggplot2); library(reshape2); library(cowplot)
setwd("D:/Github/Personal works/R/LDA_Topic_model(180417)")
shop=read.csv("쇼핑.csv")
preproc = function(data,name){
tab=table(data[,1])
line=paste(data[,2]-1,data[,3],sep=":")
line=tapply(line,data[,1],FUN=identity)
line=Map(c, tab, line)
line=lapply(line,FUN=function(x) paste(x,collapse=" "))
line=paste(line,collapse="\n")
loc=paste0(name,".dat")
outfile=file(loc)
writeLines(line,outfile)
close(outfile)
}
preproc(shop,"쇼핑")
shop2=read.documents("쇼핑.dat"); item_name=read.vocab("품목.txt")
head(shop2,1)
set.seed(100)
n=length(shop2); K=11; W=70; alpha=1.0; beta=1.0
lda=lda.collapsed.gibbs.sampler(shop2, K, item_name, 500, alpha, beta)
# theta를 추정
theta=t(lda$document_sums)
for(i in 1:n) theta[i,]=theta[i,]/sum(theta[i,])
round(theta[1:3,1:10],2)
# phi를 추정
phi=lda$topics
for(i in 1:K)  phi[i,]=phi[i,]/sum(phi[i,])
round(phi[1:3,1:10],2)
# 상품의 비율
p=colSums(lda$topics)/sum(lda$topics)
# lift 계산
lift=matrix(0,nrow=K,ncol=W)
colnames(lift)=item_name
for(i in 1:K){
lift[i,p!=0]=phi[i,p!=0]/p[p!=0]
lift[i,p==0]=0
}
# 토픽마다 lift 상위 2개 상품으로 이름을 붙임
topic_name=c()
for(i in 1:K){
sorted=sort(lift[i,],decreasing=T)[1:2]
topic_name=c(topic_name,paste(names(sorted),collapse=" "))
}
topic_name
plot_topic=function(phi,idx,ncol,main=""){
theme_set(theme_bw())
phi.df <- melt(cbind(data.frame(phi[idx,]), char=factor(idx)), variable.name="item", id.vars = "char")
ggplot(phi.df,aes(item,value,fill=item)) + geom_bar(stat="identity") +
theme(axis.text.x = element_text(angle=90, hjust=1),legend.position="none") +
coord_flip() +
facet_wrap(~ char, ncol=ncol) +
ggtitle(main) +
theme(plot.title = element_text(hjust = 0.5))
}
par(mfrow=c(1,2))
colnames(phi)=item_name
plot_grid(plot_topic(phi,c(1,6,7),ncol=3,"Probability"),plot_topic(lift,c(1,6,7),ncol=3,"Lift"),align='h')
plot_client=function(theta,idx,ncol){
theme_set(theme_bw())
theta.df <- melt(cbind(data.frame(theta[idx,]), client=factor(idx)), variable.name="topic", id.vars = "client")
ggplot(theta.df,aes(topic,value,fill=topic)) + geom_bar(stat="identity") +
theme(axis.text.x = element_text(angle=90, hjust=1)) +
coord_flip() +
facet_wrap(~ client, ncol=ncol)
}
par(mfrow=c(1,1))
colnames(theta)=topic_name
plot_client(theta,c(5,100,500),ncol=3)
table(shop)
shop
max(shop[,1])
max(shop[,2])
ll = paste(data[,2]-1,data[,3],sep = ':')
ll = paste(shop[,2]-1,shop[,3],sep = ':')
ll
tapply(ll, data[,1], FUN = identity)
tapply(ll, shop[,1], FUN = identity)
ll = tapply(ll, shop[,1], FUN = identity)
tt = table(shope[,1])
tt = table(shop[,1])
map(c,tt,ll)
Map(c,tt,ll)
ll
ll = Map(c,tt,ll)
head(ll)
lapply(ll[1:6], fun=function(x){paste(x, collapse= ' ')})
lapply(ll[1:6], FUN =function(x){paste(x, collapse= ' ')})
lapply(ll[1:6], FUN =function(x){paste(x)})
ll = lapply(ll[1:6], FUN =function(x){paste(x, collapse = ' ')})
ll
tt = table(shop[,1])
ll = paste(shop[,2]-1,shop[,3],sep = ':')
ll = tapply(ll, shop[,1], FUN = identity)
ll = Map(c,tt,ll)
ll = lapply(ll, FUN =function(x){paste(x, collapse = ' ')})
ll
paste(ll, collapse = '\n')
shop2
read.documents("쇼핑.dat")
preproc(shop,"쇼핑")
shop2=read.documents("쇼핑.dat"); item_name=read.vocab("품목.txt")
head(shop2,1)
set.seed(100)
n=length(shop2); K=11; W=70; alpha=1.0; beta=1.0
lda=lda.collapsed.gibbs.sampler(shop2, K, item_name, 500, alpha, beta)
# theta를 추정
theta=t(lda$document_sums)
for(i in 1:n) theta[i,]=theta[i,]/sum(theta[i,])
round(theta[1:3,1:10],2)
# phi를 추정
phi=lda$topics
for(i in 1:K)  phi[i,]=phi[i,]/sum(phi[i,])
round(phi[1:3,1:10],2)
# 상품의 비율
p=colSums(lda$topics)/sum(lda$topics)
# lift 계산
lift=matrix(0,nrow=K,ncol=W)
colnames(lift)=item_name
for(i in 1:K){
lift[i,p!=0]=phi[i,p!=0]/p[p!=0]
lift[i,p==0]=0
}
# 토픽마다 lift 상위 2개 상품으로 이름을 붙임
topic_name=c()
for(i in 1:K){
sorted=sort(lift[i,],decreasing=T)[1:2]
topic_name=c(topic_name,paste(names(sorted),collapse=" "))
}
topic_name
plot_topic=function(phi,idx,ncol,main=""){
theme_set(theme_bw())
phi.df <- melt(cbind(data.frame(phi[idx,]), char=factor(idx)), variable.name="item", id.vars = "char")
ggplot(phi.df,aes(item,value,fill=item)) + geom_bar(stat="identity") +
theme(axis.text.x = element_text(angle=90, hjust=1),legend.position="none") +
coord_flip() +
facet_wrap(~ char, ncol=ncol) +
ggtitle(main) +
theme(plot.title = element_text(hjust = 0.5))
}
par(mfrow=c(1,2))
colnames(phi)=item_name
plot_grid(plot_topic(phi,c(1,6,7),ncol=3,"Probability"),plot_topic(lift,c(1,6,7),ncol=3,"Lift"),align='h')
plot_client=function(theta,idx,ncol){
theme_set(theme_bw())
theta.df <- melt(cbind(data.frame(theta[idx,]), client=factor(idx)), variable.name="topic", id.vars = "client")
ggplot(theta.df,aes(topic,value,fill=topic)) + geom_bar(stat="identity") +
theme(axis.text.x = element_text(angle=90, hjust=1)) +
coord_flip() +
facet_wrap(~ client, ncol=ncol)
}
par(mfrow=c(1,1))
colnames(theta)=topic_name
plot_client(theta,c(5,100,500),ncol=3)
plot_topic=function(phi,idx,ncol,main=""){
theme_set(theme_bw())
phi.df <- melt(cbind(data.frame(phi[idx,]), char=factor(idx)), variable.name="item", id.vars = "char")
ggplot(phi.df,aes(item,value,fill=item)) + geom_bar(stat="identity") +
theme(axis.text.x = element_text(angle=90, hjust=1),legend.position="none") +
coord_flip() +
facet_wrap(~ char, ncol=ncol) +
ggtitle(main) +
theme(plot.title = element_text(hjust = 0.5))
}
par(mfrow=c(1,2))
colnames(phi)=item_name
plot_grid(plot_topic(phi,c(1,6,7),ncol=3,"Probability"),plot_topic(lift,c(1,6,7),ncol=3,"Lift"),align='h')
topic_name
plot_client(theta,c(5,100,500),ncol=3)
plot_grid(plot_topic(phi,c(1,6,7),ncol=3,"Probability"),plot_topic(lift,c(1,6,7),ncol=3,"Lift"),align='h')
